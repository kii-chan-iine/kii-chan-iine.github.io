---
layout: post
title: 模型评价指标
date: 2019-06-26
Author: Kii. Chan
categories:
tags: [ML]
comments: true

---

# Model Evaluation method

## The Goodness of a linear model 

A model fits the data well if there is a small and unbiased difference between the obsderved values and the predicted values.

**Residual plots** can reveal unwanted residual patterns that indicate biased results effeciently. This is the first thing you need do if you want to know the whole situation of your results.

## Correlation coefficient and the coefficient of determination

### 协方差与相关系数

协方差是计算两个随机变量*X*和*Y*之间的相关性的指标，定义如下：
<img src="http://latex.codecogs.com/svg.latex?Cov(X,Y)=E[(X-EX)(Y-EY)]" border="0"/>





缺点：它的值会随着变量量纲的变化而变化（covariance is not scale invariant），因此，这才提出了相关系数的概念：
<img src="http://latex.codecogs.com/svg.latex?r=Corr(X,Y)=\frac{Cov(X,Y)}{\sigma X \sigma Y}=\frac{E[(X-EX)(Y-EY)]}{\sqrt{E[X-EX]^2}\sqrt{E[Y-EY]^2}}" border="0"/>


对于相关系数，我们需要注意：

1. 相关系数是用于描述两个变量*线性*相关程度的，如果<img src="http://latex.codecogs.com/svg.latex?r > 0" border="0"/>，呈正相关；如果<img src="http://latex.codecogs.com/svg.latex?r = 0" border="0"/>，不相关；如果<img src="http://latex.codecogs.com/svg.latex?r \textless  0" border="0"/>，呈负相关。
2. 如果我们将<img src="http://latex.codecogs.com/svg.latex?X-EX" border="0"/>和<img src="http://latex.codecogs.com/svg.latex?Y-EY" border="0"/>看成两个向量的话，那<img src="http://latex.codecogs.com/svg.latex?r" border="0"/>刚好表示的是这两个向量夹角的余弦值，这也就解释了为什么<img src="http://latex.codecogs.com/svg.latex?r" border="0"/>的值域是[-1, 1]。
3. Correlation is invariant to scaling and shift. E.g 
for all <img src="http://latex.codecogs.com/svg.latex?Corr(X,Y)=Corr(aX+b,Y)" border="0"/>.

### Attention

#### 为什么一元线性回归的判定系数等于相关系数的平方，但从各自的公式上看不存在这个关系？

其实关系是这样的：相关系数的值=判定系数的平方根，符号与x的参数相同。只是你没发现而已。他们用不同的表达式表达出来了。所以不能一眼看出来，推导有些复杂。  但是他们在概念上有明显区别，**相关系数**建立在相关分析基础之上，研究**两个变量**之间的线性相关关系。而**判定系数**建立在回归分析基础之上研究**一个随机变量对别一个随机变量的解释程度**。   

回归分析中的<img src="http://latex.codecogs.com/svg.latex?R^2" border="0"/>在数学上恰好是**Pearson积矩相关系数r**的平方。因此这极易使作者们**错误地理解为**<img src="http://latex.codecogs.com/svg.latex?R^2" border="0"/>就是“相关系数”或“相关系数的平方”。问题在于对于自变量是普通变量(即其取值有确定性的变量)、因变量为随机变量的模型(Ⅰ回归分析)，2个变量之间的“相关性”概念根本不存在，又何谈“相关系数”呢？更值得注意的是一些早期的教科书作者不是用R2来描述回归效果(拟合程度)，而是用Pearson积矩相关系数r来描述。这就更容易误导读者。

For simple linear regression (i.e., line-fits), the coefficient of determination or R2 is the square of the correlation coefficient r. It is easier to interpret than the correlation coefficient r: values of R2 close to 1 correspond to a close correlation, values close to 0 to a poor one. Note that for general models it is common to write R2, whereas for simple linear regression r2 is used.[5]


### R-Squared

**R-squared** is a statistical measure of how close the data are to the fitted regression line. It is also known as **the coefficient of determination**, or the coefficient of multiple determination for multiple regression.

**Definition: **<img src="http://latex.codecogs.com/svg.latex?f_i" border="0"/> is the predicted value and <img src="http://latex.codecogs.com/svg.latex?y_i" border="0"/> is the observed value.

1. **SSR** *(Sum of Squares forregression)* = **ESS** *(explained sum of squares)*
<img src="http://latex.codecogs.com/svg.latex?SSR=\sum_{i=1}^{n}(f_i-\overline{y})^2" border="0"/>

2. **SSE** *(Sum of Squares for Error）* =**RSS** *(residual sum of squares)*
<img src="http://latex.codecogs.com/svg.latex?SSE=\sum_{i=1}^{n}(y_i-f_i)^2" border="0"/>

3. **SST** *(Sum of Squares fortotal)* = **TSS***(total sum of squares)*
<img src="http://latex.codecogs.com/svg.latex?SST=\sum_{i=1}^{n}(y_i-\overline{y})^2" border="0"/>


<center>
<img src="http://latex.codecogs.com/svg.latex?R^2=1-\frac{SSE}{SST}=\frac{SSR}{SST}" border="0"/>
</center>

Moreover,

<center>
<img src="http://latex.codecogs.com/svg.latex?R^2=1-\frac{\sum_{i=1}^{n}(f_i-\overline{y})^2/n}{\sum_{i=1}^{n}(y_i-\overline{y})^2/n}=\frac{RMSE}{Var}" border="0"/>
</center>

which is helpful for programming implementation.

**R-square** is the percentage of the response variable variation that is explained by a linear model.  It is always between **0-1**. Generally, the higher the R-squared, the better the model fits your data. However, it is not mean that a model with higher R-square always perform well.

### Limitations

1. R-squared **cannot determine whether** the coefficient estimates and predictions are **biased**, which is why you must assess the residual plots.

2. R-squared **does not indicate whether a regression model is adequate**. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data!

3. The R-squared in your output is a biased estimate of the population R-squared. 


### <font color= #FF0000>Are Low R-squared Values Inherently Bad?</font>

No! there are two major reasons why the modle is fine to have a low R-squared values.
1.  In **some fields**, it is entirely expected that your R-squared values will be low. For example, any field that attempts to predict human behavior, such as psychology, typically has R-squared values lower than 50%. Humans are simply harder to predict than, say, physical processes.
2.  Furthermore, if your R-squared value is low but you have statistically significant predictors, you can still draw important conclusions about how changes in the predictor values are associated with changes in the response value. Regardless of the R-squared, the significant coefficients still represent the mean change in the response for one unit of change in the predictor while holding other predictors in the model constant. Obviously, this type of information can be extremely valuable.

![](https://blog.minitab.com/hubfs/Imported_Blog_Media/flp_highvar.png)

![](https://blog.minitab.com/hubfs/Imported_Blog_Media/flp_lowvar.png)




### <font color= #FF0000>Are High R-square Values Inherently Good?</font>

Nope, a high R-square doesn't stands for a good fit.  Here is an example, 

The first fig is the fitted plot between semiconductor electron mobility and the natural log of the density.

<img src="https://blog.minitab.com/hubfs/Imported_Blog_Media/flplinear-1.gif"  alt="semiconductor" />

From the figure, one can obtain that there is a sound fitting performance with the R-square is 98.5%. However, look closer to see how the regression line systematically over and under-predicts the data (bias) at different points along the curve. You can also see patterns in the Residuals versus Fits plot, rather than the randomness that you want to see.  The results show this is a bad fit.  (This example also indicates that a residual plot is important to access the performance of a model.)

<img src="https://blog.minitab.com/hubfs/Imported_Blog_Media/reslinear-1.gif"  alt="Residual" />

Similar biases may also occurred when the linear model id missing important predictors, polynormal terms and interaction terms. This is **specification bias** (设定误差) caused by an underspecified model. Those kind of bias can be fixed by adding proper terms to the model.

#### Five Reasons Why Your R-squared Can Be Too High

1. R-squared is a biased estimate
2. You might be overfitting your model
3. Data mining and chance correlations： **Data mining can produce high R-squared values even with entirely random data!** Before performing regression analysis, you should already have an idea of what the important variables are along with their relationships, coefficient signs, and effect magnitudes based on previous research. Unfortunately, recent trends have moved away from this approach thanks to large, readily available databases and automated procedures that build regression models.
4. Trends in Panel (Time Series) Data
5. Form of a Variable： It's possible that you're including different forms of the same variable for both the response variable and a predictor variable.



### Adjusted R-squared and predicted R-squared

**Q1**:Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms.

*Owning to **SSE** will reduced if we add **excessive independent variables**, thus R-square can be bigger.  And we'll see, this can in fact be problematic.  To handle this situation, adjusted R-square was proposed. This function can penalize the excessive meaningless independent variables.*



<center>
<img src="http://latex.codecogs.com/svg.latex?R^2_A=1-(1-R^2)\frac{n-1}{n-p-1}" border="0"/>
</center>

**Q2:** If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as [overfitting the model](https://blog.minitab.com/blog/adventures-in-statistics/the-danger-of-overfitting-regression-models) and it produces misleadingly high R-squared values and a lessened ability to make predictions.

The predicted R-squared indicates how well a regression model predicts responses for new observations. This statistic helps you determine when the model fits the original data but is less capable of providing valid predictions for new observations. **This indicators is vital for model evaluation.**

### Conclusions 

The R-square is a good tool to evaluate the performance of a linear model. However, it can not tell us the whole story. We need to evaluate the R-square values in conjunction with **residual plot, other models statistics and subject area knowledge**.

Despite R-square provides a relationship between the predicted results and the observed values, it doesn't provide a formal hypothesis for this relationship.  Whether this relations is significant is questionable.

 





---

## Reference

1. [Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)
2. [8 Tips for Interpreting R-Squared](https://www.displayr.com/8-tips-for-interpreting-r-squared/)
3. [Linear or Nonlinear Regression? That Is the Question.](https://blog.minitab.com/blog/adventures-in-statistics-2/linear-or-nonlinear-regression-that-is-the-question)
4. Rodgers, Joseph Lee, and W. Alan Nicewander. "Thirteen Ways to Look at the Correlation Coefficient." The American Statistician 42.1 (1988): 59-66
5. An introduction to statistics with Python

---
Forward from cite[2]

Hopefully if you have landed on this post you have a basic idea of what the R-Squared statistic means. The R-Squared statistic is a number between 0 and 1, or, 0% and 100%, that quantifies the variance explained in a statistical model. Unfortunately, R Squared comes under many different names. It is the same thing as r-squared, R-square, the coefficient of determination, variance explained, the squared correlation, r2, and R2.

We get quite a few questions about its interpretation from users of Q and Displayr, so I am taking the opportunity to answer the most common questions as a series of tips for using R2. Read on to find out more about using R-Squared to work out overall fit, why it's a good idea to plot the data when interpreting R-Squared, how to interpret R-Squared values and why you should not use R-Squared to compare models.

1. Don't conclude a model is "good" based on the R-squared
The basic mistake that people make with R-squared is to try and work out if a model is "good" or not, based on its value. There are two flavors of this question:

"My R-Squared is 75%. Is this good?"
"My R-Squared is only 20%; I was told that it needs to be 90%".
The problem with both of these questions it that it is just a bit silly to work out if a model is good or not based on the value of the R-Squared statistic. Sure it would be great if you could check a model by looking at its R-Squared, but it makes no sense to do so. Most of the rest of the post explains why.

I will point out a caveat to this tip. It is pretty common to develop rules of thumb. For example, in driver analysis, models often have R-Squared values of around 0.20 to 0.40. But, keep in mind, that even if you are doing a driver analysis, having an R-Squared in this range, or better, does not make the model valid. Read on to find out more about how to interpret R Squared.

2. Use R-Squared to work out overall fit
Sometimes people take point 1 a bit further, and suggest that R-Squared is always bad. Or, that it is bad for special types of models (e.g., don't use R-Squared for non-linear models). This is a case of throwing the baby out with the bath water. There are quite a few caveats, but as a general statistic for summarizing the strength of a relationship, R-Squared is awesome. All else being equal, a model that explained 95% of the variance is likely to be a whole lot better than one that explains 5% of the variance, and likely will produce much, much better predictions.

Of course, often all is not equal, so read on.

3. Plot the data
When interpreting the R-Squared it is almost always a good idea to plot the data. That is, create a plot of the observed data and the predicted values of the data. This can reveal situations where R-Squared is highly misleading. For example, if the observed and predicted values do not appear as a cloud formed around a straight line, then the R-Squared, and the model itself, will be misleading. Similarly, outliers can make the R-Squared statistic be exaggerated or be much smaller than is appropriate to describe the overall pattern in the data.

4. Be very afraid if you see a value of 0.9 or more
In 25 years of building models, of everything from retail IPOs through to drug testing, I have never seen a good model with an R-Squared of more than 0.9. Such high values always mean that something is wrong, usually seriously wrong.

5. Take context into account
What is a good R-Squared value? Well, you need to take context into account. There are a lot of different factors that can cause the value to be high or low. This makes it dangerous to conclude that a model is good or bad based solely on the value of R-Squared. For example:

When your predictor or outcome variables are categorical (e.g., rating scales) or counts, the R-Squared will typically be lower than with truly numeric data.
The more true noise in the data, the lower the R-Squared. For example, if building models based on stated preferences of people, there is a lot of noise so a high R-Squared is hard to achieve. By contrast, models of astronomical phenomena are the other way around.
When you have more observations, the R-Squared gets lower.
When you have more predictor variables, the R-Squared gets higher (this is offset by the previous point; the lower the ratio of observations to predictor variables, the higher the R-Squared).
If your data is not a simple random sample the R-Squared can be inflated. For example, consider models based on time series data or geographic data. These are rarely simple random samples, and tend to get much higher R-Squared statistics.
When your model excludes variables that are obviously important, the R-Squared will necessarily be small. For example, if you have a model looking at how brand imagery drives brand preference, and your model ignores practical things like price, distribution, flavor, and quality, the R-Squared is inevitably going to be small even if your model is great.
Models based on aggregated data (e.g., state-level data) have much higher R-Squared statistics than those based on case-level data.
6. Think long and hard about causality
For the R-Squared to have any meaning at all in the vast majority of applications it is important that the model says something useful about causality. Consider, for example, a model that predicts adults' height based on their weight and gets an R-Squared of 0.49.  Is such a model meaningful? It depends on the context. But, for most contexts the model is unlikely to be useful. The implication, that if we get adults to eat more they will get taller, is rarely true.

But, consider a model that predicts tomorrow's exchange rate and has an R-Squared of 0.01. If the model is sensible in terms of its causal assumptions, then there is a good chance that this model is accurate enough to make its owner very rich.

7. Don't use R-Squared to compare models
A natural thing to do is to compare models based on their R-Squared statistics. If one model has a higher R-Squared value, surely it is better? This is, as a pretty general rule, an awful idea. There are two different reasons for this:

In many situations the R-Squared is misleading when compared across models. Examples include comparing a model based on aggregated data with one based on disaggregate data, or models where the variables are being transformed.
Even in situations where the R-Squared may be meaningful, there are always better tools for comparing models. These include F-Tests, Bayes' Factors, Information Criteria, and out-of-sample predictive accuracy.
8. Don't interpret pseudo R-Squared statistics as explaining variance
Technically, R-Squared is only valid for linear models with numeric data. While I find it useful for lots of other types of models, it is rare to see it reported for models using categorical outcome variables (e.g., logit models). Many pseudo R-squared models have been developed for such purposes (e.g., McFadden's Rho, Cox & Snell). These are designed to mimic R-Squared in that 0 means a bad model and 1 means a great model. However, they are fundamentally different from R-Squared in that they do not indicate the variance explained by a model. For example, if McFadden's Rho is 50%, even with linear data, this does not mean that it explains 50% of the variance. No such interpretation is possible. In particular, many of these statistics can never ever get to a value of 1.0, even if the model is "perfect".
