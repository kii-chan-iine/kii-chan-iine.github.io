---
layout: post
title: 深度学习笔记
date: 2021-05-18
Author: Kii. Chan
categories:
tags: [深度学习笔记]
comments: true

---

[toc]

# Pytorch 正则化

## L1 regularization

对于每个 ω 我们都向目标函数增加一个λ|ω| 。
L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。
相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。

PyTorch里的optimizer只能实现L2正则化，L1正则化只能手动实现：

```python
regularization_loss = 0
for param in model.parameters():
    regularization_loss += torch.sum(abs(param))

calssify_loss = criterion(pred,target)
loss = classify_loss + lamda * regularization_loss

optimizer.zero_grad()
loss.backward()
optimizer.step()
```

## L2 regularization

对于网络中的每个权重 ω ，向目标函数中增加一个$\frac{1}{2} \lambda \omega^{2}$，其中 λ 是正则化强度。这样该式子关于梯度就是 λω 了。L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以 $w += -\lambda * W$向着0线性下降。选择一个合适的权重衰减系数λ非常重要，这个需要根据具体的情况去尝试，初步尝试可以使用 `1e-4` 或者 `1e-3`

在PyTorch中某些optimizer优化器的参数`weight_decay (float, optional)`就是 L2 正则项，它的默认值为0。

```python
optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.001)
```

# 损失函数和优化器

## 损失函数

损失函数，又叫目标函数，是编译一个神经网络模型必须的两个参数之一。另一个必不可少的参数是优化器。

损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/af83a0a3-a0a2-4ba6-83e7-5b3eab9869a2/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/af83a0a3-a0a2-4ba6-83e7-5b3eab9869a2/Untitled.png)

上图是一个用来模拟线性方程自动学习的示意图。粗线是真实的线性方程，虚线是迭代过程的示意，w1 是第一次迭代的权重，w2 是第二次迭代的权重，w3 是第三次迭代的权重。随着迭代次数的增加，我们的目标是使得 wn 无限接近真实值。

那么怎么让 w 无限接近真实值呢？其实这就是损失函数和优化器的作用了。图中 1/2/3 这三个标签分别是 3 次迭代过程中预测 Y 值和真实 Y 值之间的差值（这里差值就是损失函数的意思了，当然了，实际应用中存在多种差值计算的公式），这里的差值示意图上是用绝对差来表示的，那么在多维空间时还有平方差，均方差等多种不同的距离计算公式，也就是损失函数了，这么一说是不是容易理解了呢？

这里示意的是一维度方程的情况，那么发挥一下想象力，扩展到多维度，是不是就是深度学习的本质了？

下面介绍几种常见的损失函数的计算方法，pytorch 中定义了很多类型的预定义损失函数，需要用到的时候再学习其公式也不迟。

我们先定义两个二维数组，然后用不同的损失函数计算其损失值。

```python
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
sample = Variable(torch.ones(2,2))
a=torch.Tensor(2,2)
a[0,0]=0
a[0,1]=1
a[1,0]=2
a[1,1]=3
target = Variable (a)
```

sample 的值为：[[1,1],[1,1]]。

target 的值为：[[0,1],[2,3]]。

### nn.L1Loss

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d3c6f5a-f429-4e6a-a2c5-ef1829ae6b55/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d3c6f5a-f429-4e6a-a2c5-ef1829ae6b55/Untitled.png)

```python
criterion = nn.L1Loss()
loss = criterion(sample, target)
print(loss)
```

最后结果是：1。

它的计算逻辑是这样的：

- 先计算绝对差总和：|0-1|+|1-1|+|2-1|+|3-1|=4；

### nn.SmoothL1Loss

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fa6bf9cc-87f2-4189-bad3-a871815d9212/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fa6bf9cc-87f2-4189-bad3-a871815d9212/Untitled.png)

SmoothL1Loss 也叫作 Huber Loss，误差在 (-1,1) 上是平方损失，其他情况是 L1 损失。

```python
criterion = nn.SmoothL1Loss()
loss = criterion(sample, target)
print(loss)

最后结果是：0.625。
```

### nn.MSELoss

平方损失函数。其计算公式是预测值和真实值之间的平方和的平均数。

```python
criterion = nn.MSELoss()
loss = criterion(sample, target)
print(loss)
最后结果是：1.5。
```

### nn.BCELoss

二分类用的交叉熵，其计算公式较复杂，这里主要是有个概念即可，一般情况下不会用到。

```python
criterion = nn.BCELoss()
loss = criterion(sample, target)
print(loss)
最后结果是：-13.8155。
```

### nn.CrossEntropyLoss

交叉熵损失函数

该公式用的也较多，比如在图像分类神经网络模型中就常常用到该公式。

```python
criterion = nn.CrossEntropyLoss()
loss = criterion(sample, target)
print(loss)
最后结果是：报错，看来不能直接这么用！
```

看文档我们知道 nn.CrossEntropyLoss 损失函数是用于图像识别验证的，对输入参数有各式要求，这里有这个概念就可以了，在图像识别一文中会有正确的使用方法。

### nn.NLLLoss

负对数似然损失函数（Negative Log Likelihood）

在前面接上一个 LogSoftMax 层就等价于交叉熵损失了。注意这里的 xlabel 和上个交叉熵损失里的不一样，这里是经过 log 运算后的数值。这个损失函数一般也是用在图像识别模型上。

```python
criterion = F.nll_loss()
loss = criterion(sample, target)
print(loss)
loss=F.nll_loss(sample,target)
最后结果会报错！
```

Nn.NLLLoss 和 nn.CrossEntropyLoss 的功能是非常相似的！通常都是用在多分类模型中，实际应用中我们一般用 NLLLoss 比较多。

### nn.NLLLoss2d

和上面类似，但是多了几个维度，一般用在图片上。

```python
input, (N, C, H, W)
target, (N, H, W)
```

比如用全卷积网络做分类时，最后图片的每个点都会预测一个类别标签。

```python
criterion = nn.NLLLoss2d()
loss = criterion(sample, target)
print(loss)
同样结果报错！
```

## 优化器Optim

所有的优化函数都位于torch.optim包下，常用的优化器有：SGD,Adam,Adadelta,Adagrad,Adamax等，下面就各优化器分析。

### 使用

```python
optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr = 0.0001)
```

lr：学习率，大于0的浮点数 momentum:动量参数，大于0的浮点数 parameters：Variable参数，要优化的对象

### 基类 Optimizer

```python
torch.optim.Optimizer(params, defaults)
```

params (iterable) —— Variable 或者 dict的iterable。指定了什么参数应当被优化。 defaults —— (dict)：包含了优化选项默认值的字典（一个参数组没有指定的参数选项将会使用默认值）。

### 方法：

- load_state_dict(state_dict)：加载optimizer状态。
- state_dict()：以dict返回optimizer的状态。包含两项：state - 一个保存了当前优化状态的dict，param_groups - 一个包含了全部参数组的dict。
- add_param_group(param_group)：给 optimizer 管理的参数组中增加一组参数，可为该组参数定制 lr,momentum, weight_decay 等，在 finetune 中常用。
- step(closure) ：进行单次优化 (参数更新)。
- zero_grad() ：清空所有被优化过的Variable的梯度。

## 优化算法

### 随机梯度下降算法 SGD算法

SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：

```python
torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float) ：学习率 momentum (float, 可选) ：动量因子（默认：0） weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认：0） dampening (float, 可选) :动量的抑制因子（默认：0） nesterov (bool, 可选) :使用Nesterov动量（默认：False） 可实现 SGD 优化算法，带动量 SGD 优化算法，带 NAG(Nesterov accelerated gradient)动量 SGD 优化算法,并且均可拥有 weight_decay 项。

对于训练数据集，我们首先将其分成n个batch，每个batch包含m个样本。我们每次更新都利用一个batch的数据，而非整个数据集。这样做使得训练数据太大时，利用整个数据集更新往往时间上不现实。batch的方法可以减少机器的压力，并且可以快速收敛。 当训练集有冗余时，batch方法收敛更快。 优缺点： SGD完全依赖于当前batch的梯度，所以η可理解为允许当前batch的梯度多大程度影响参数更新。对所有的参数更新使用同样的learning rate，选择合适的learning rate比较困难，容易收敛到局部最优。

### *平均随机梯度下降算法 ASGD算法*

ASGD 就是用空间换时间的一种 SGD。

```python
torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) ： 学习率（默认：1e-2） lambd (float, 可选) ：衰减项（默认：1e-4） alpha (float, 可选) ：eta更新的指数（默认：0.75） t0 (float, 可选) ：指明在哪一次开始平均化（默认：1e6） weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0）

### *Adagrad算法*

AdaGrad算法就是将每一个参数的每一次迭代的梯度取平方累加后在开方，用全局学习率除以这个数，作为学习率的动态更新。

其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数，为了数值稳定大约设置为10^-7 。

```python
torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) ：学习率（默认: 1e-2） lr_decay (float, 可选) ：学习率衰减（默认: 0） weight_decay (float, 可选) ： 权重衰减（L2惩罚）（默认: 0） 优缺点： Adagrad 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。在深度学习算法中，深度过深会造成训练提早结束。

### *自适应学习率调整 Adadelta算法*

Adadelta是对Adagrad的扩展，主要针对三个问题：

学习率后期非常小的问题； 手工设置初始学习率； 更新xt时，两边单位不统一 针对以上的三个问题，Adadelta提出新的Adag解决方法。Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。

```python
torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict rho (float, 可选) ： 用于计算平方梯度的运行平均值的系数（默认：0.9） eps (float, 可选)： 为了增加数值计算的稳定性而加到分母里的项（默认：1e-6） lr (float, 可选)： 在delta被应用到参数更新之前对它缩放的系数（默认：1.0） weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0） 优缺点： Adadelta已经不依赖于全局学习率。训练初中期，加速效果不错，很快，训练后期，反复在局部最小值附近抖动。

### *RMSprop算法*

RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。 RMSprop 采用均方根作为分 母，可缓解 Adagrad 学习率下降较快的问题， 并且引入均方根，可以减少摆动。

```python
torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) ：学习率（默认：1e-2） momentum (float, 可选) : 动量因子（默认：0） alpha (float, 可选) : 平滑常数（默认：0.99） eps (float, 可选) : 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） centered (bool, 可选):如果为True，计算中心化的RMSProp，并且用它的方差预测值对梯度进行归一化 weight_decay (float, 可选)：权重衰减（L2惩罚）（默认: 0）

### *自适应矩估计 Adam算法*

```python
torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：1e-3） betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999） eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0） 优缺点： Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。 Adam结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点。

- 计算效率高
- 很少的内存需求
- 梯度的对角线重缩放不变（这意味着亚当将梯度乘以仅带正因子的对角矩阵是不变的，以便更好地理解此堆栈交换）
- 非常适合数据和/或参数较大的问题
- 适用于非固定目标
- 适用于非常嘈杂和/或稀疏梯度的问题
- 超参数具有直观的解释，通常需要很少的调整（我们将在配置部分中对此进行详细介绍）

### *Adamax算法（Adamd的无穷范数变种）*

Adamax 是对 Adam 增加了一个学习率上限的概念，所以也称之为 Adamax。

```python
torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：2e-3） betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数 eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0） 优缺点：

Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。 Adamax学习率的边界范围更简单。

### *SparseAdam算法*

针对稀疏张量的一种“阉割版”Adam 优化方法。

```python
torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：2e-3） betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数 eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）

### *L-BFGS算法*

L-BFGS 属于拟牛顿算法。 L-BFGS 是对 BFGS 的改进，特点就是节省内存。

```python
torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, 
tolerance_grad=1e-05, tolerance_change=1e-09, 
history_size=100, line_search_fn=None)
```

lr (float) – 学习率（默认：1） max_iter (int) – 每一步优化的最大迭代次数（默认：20）) max_eval (int) – 每一步优化的最大函数评价次数（默认：max * 1.25） tolerance_grad (float) – 一阶最优的终止容忍度（默认：1e-5） tolerance_change (float) – 在函数值/参数变化量上的终止容忍度（默认：1e-9） history_size (int) – 更新历史的大小（默认：100）

### *弹性反向传播算法 Rprop算法*

该优化方法适用于 full-batch，不适用于 mini-batch。不推荐。

```python
torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：1e-2） etas (Tuple[float, float], 可选) – 一对（etaminus，etaplis）, 它们分别是乘法的增加和减小的因子（默认：0.5，1.2） step_sizes (Tuple[float, float], 可选) – 允许的一对最小和最大的步长（默认：1e-6，50） 优缺点： 该优化方法适用于 full-batch，不适用于 mini-batch。几种优化器

## 学习率

```python
# scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
# # (1) 等间隔调整学习率 StepLR
# torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)
# # (2) 多间隔调整学习率 MultiStepLR
# torch.optim.lr_sheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)
# # (3) 指数衰减调整学习率 ExponentialLR
# torch.optim.lr_sheduler.ExponentialLR(optimizer, gamma, last_epoch)
# # (4) 余弦退火函数调整学习率：
# torch.optim.lr_sheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)
# # （5）根据指标调整学习率 ReduceLROnPlateau
# torch.optim.lr_sheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10,
#  verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)
```

