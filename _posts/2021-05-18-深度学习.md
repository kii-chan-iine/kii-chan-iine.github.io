---
layout: post
title: 深度学习笔记
date: 2021-05-18
Author: Kii. Chan
categories:
tags: [深度学习笔记]
comments: true

---

[toc]
# 过拟合

## Early stopping

一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。

## 数据集扩增

- 从数据源头采集更多数据
- 复制原有数据并加上随机噪声
- 重采样
- 根据当前数据集估计数据分布参数，使用该分布产生更多数据等

## Drop out

## Pytorch 正则化

  正则项是为了降低模型的复杂度，从而避免模型区过分拟合训练数据，包括噪声与异常点（outliers）。从另一个角度上来讲，正则化即是假设模型参数服从先验概率，即为模型参数添加先验，只是不同的正则化方式的先验分布是不一样的。这样就规定了参数的分布，使得模型的复杂度降低（试想一下，限定条件多了，是不是模型的复杂度降低了呢），这样模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。还有个解释便是，从贝叶斯学派来看：加了先验，在数据少的时候，先验知识可以防止过拟合；从频率学派来看：正则项限定了参数的取值，从而提高了模型的稳定性，而稳定性强的模型不会过拟合，即控制模型空间。

### L1，L2范数

![img](https://img-blog.csdnimg.cn/20181108162130208.png)
L1
$$||\theta||_1=|\theta_1|+|\theta_2|$$
L2

$$||\theta||_2=\sqrt{\theta_1^2+\theta_2^2}$$

### L1 regularization

L1正则化的形式是：$\lambda|\theta_i|$,与目标函数结合后的形式就是：$L(θ)=L(θ)+λ\sum^n_i|θ_i|$。需注意，$L_1$正则化除了和L2正则化一样可以约束数量级外，L1正则化还能起到使参数更加稀疏的作用，稀疏化的结果使优化后的参数一部分为0，另一部分为非零实值。非零实值的那部分参数可起到选择重要参数或特征维度的作用，同时可起到去除噪声的效果。此外，L1正则化和L2正则化可以联合使用：$\lambda_1|θ_i|+\frac{1}{2}λ_2θ_i^2$。这种形式也被称为“Elastic网络正则化”。



对于每个 ω 我们都向目标函数增加一个λ|ω| 。
L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。
相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。

PyTorch里的optimizer只能实现L2正则化，L1正则化只能手动实现：

```python
regularization_loss = 0
for param in model.parameters():
    regularization_loss += torch.sum(abs(param))

calssify_loss = criterion(pred,target)
loss = classify_loss + lamda * regularization_loss

optimizer.zero_grad()
loss.backward()
optimizer.step()
```

### L2 regularization

对于L2正则化：$C=C_0+\frac{\lambda}{2n}\sum_i\omega^2_i$，相比于未加正则化之前,权重的偏导多了一项$\frac{\lambda}{n}\omega$，偏置的偏导没变化，那么在梯度下降时ω的更新变为：
$$
\omega \rightarrow \omega -\eta \left( \frac{\partial C_0}{\partial \omega}+\frac{\lambda}{n}\omega \right) =\left( 1-\frac{\eta \lambda}{n} \right) \omega -\eta \frac{\partial C_0}{\partial \omega}
$$
可以看出ω的系数使得权重下降加速，因此L2正则也称weight decay(caffe中损失层的weight_decay参数与此有关)-pytorch中也是。

对于随机梯度下降(对一个mini-batch中的所有x的偏导求平均)：
$$
\omega \rightarrow \left( 1-\frac{\eta \lambda}{n} \right) \omega -\frac{\eta}{m}\sum_x{\frac{\partial C_x}{\partial \omega}}
$$
$$
b\rightarrow b-\frac{\eta}{m}\sum_x{\frac{\partial C_x}{\partial b}}
$$

对于$L_2$正则化：$C=C_0+\frac{\lambda}{n}\sum_i|\omega_i|$，梯度下降的更新为：

$$
\omega \rightarrow \omega -\frac{\eta \lambda }{n}sgn \left( w \right) -\eta \frac{\partial C_0}{\partial \omega}
$$
符号函数在ω大于0时为1，小于0时为-1，在ω=0时|ω|没有导数，因此可令$sgn(0)=0$，在0处不使用$L_2$正则化。



<font color='orange'>L1相比于L2，有所不同：</font>

- L1减少的是一个常量，L2减少的是权重的固定比例
- 孰快孰慢取决于权重本身的大小，权重刚大时可能L2快，较小时L1快
- L1使权重稀疏，L2使权重平滑，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0

实践中L2正则化通常优于L1正则化。

- **L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择**
- **L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合**



---

对于网络中的每个权重 ω ，向目标函数中增加一个$\frac{1}{2} \lambda \omega^{2}$，其中 λ 是正则化强度。这样该式子关于梯度就是 λω 了。L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以 $w += -\lambda * W$向着0线性下降。选择一个合适的权重衰减系数λ非常重要，这个需要根据具体的情况去尝试，初步尝试可以使用 `1e-4` 或者 `1e-3`

在PyTorch中某些optimizer优化器的参数`weight_decay (float, optional)`就是 L2 正则项，它的默认值为0。

```python
optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.001)
```

# 损失函数和优化器

## 损失函数

损失函数，又叫目标函数，是编译一个神经网络模型必须的两个参数之一。另一个必不可少的参数是优化器。

损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/af83a0a3-a0a2-4ba6-83e7-5b3eab9869a2/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/af83a0a3-a0a2-4ba6-83e7-5b3eab9869a2/Untitled.png)

上图是一个用来模拟线性方程自动学习的示意图。粗线是真实的线性方程，虚线是迭代过程的示意，w1 是第一次迭代的权重，w2 是第二次迭代的权重，w3 是第三次迭代的权重。随着迭代次数的增加，我们的目标是使得 wn 无限接近真实值。

那么怎么让 w 无限接近真实值呢？其实这就是损失函数和优化器的作用了。图中 1/2/3 这三个标签分别是 3 次迭代过程中预测 Y 值和真实 Y 值之间的差值（这里差值就是损失函数的意思了，当然了，实际应用中存在多种差值计算的公式），这里的差值示意图上是用绝对差来表示的，那么在多维空间时还有平方差，均方差等多种不同的距离计算公式，也就是损失函数了，这么一说是不是容易理解了呢？

这里示意的是一维度方程的情况，那么发挥一下想象力，扩展到多维度，是不是就是深度学习的本质了？

下面介绍几种常见的损失函数的计算方法，pytorch 中定义了很多类型的预定义损失函数，需要用到的时候再学习其公式也不迟。

我们先定义两个二维数组，然后用不同的损失函数计算其损失值。

```python
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
sample = Variable(torch.ones(2,2))
a=torch.Tensor(2,2)
a[0,0]=0
a[0,1]=1
a[1,0]=2
a[1,1]=3
target = Variable (a)
```

sample 的值为：[[1,1],[1,1]]。

target 的值为：[[0,1],[2,3]]。

### nn.L1Loss

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d3c6f5a-f429-4e6a-a2c5-ef1829ae6b55/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d3c6f5a-f429-4e6a-a2c5-ef1829ae6b55/Untitled.png)

```python
criterion = nn.L1Loss()
loss = criterion(sample, target)
print(loss)
```

最后结果是：1。

它的计算逻辑是这样的：

- 先计算绝对差总和：|0-1|+|1-1|+|2-1|+|3-1|=4；

### nn.SmoothL1Loss

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fa6bf9cc-87f2-4189-bad3-a871815d9212/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fa6bf9cc-87f2-4189-bad3-a871815d9212/Untitled.png)

SmoothL1Loss 也叫作 Huber Loss，误差在 (-1,1) 上是平方损失，其他情况是 L1 损失。

```python
criterion = nn.SmoothL1Loss()
loss = criterion(sample, target)
print(loss)

最后结果是：0.625。
```

### nn.MSELoss

平方损失函数。其计算公式是预测值和真实值之间的平方和的平均数。

```python
criterion = nn.MSELoss()
loss = criterion(sample, target)
print(loss)
最后结果是：1.5。
```

### nn.BCELoss

二分类用的交叉熵，其计算公式较复杂，这里主要是有个概念即可，一般情况下不会用到。

```python
criterion = nn.BCELoss()
loss = criterion(sample, target)
print(loss)
最后结果是：-13.8155。
```

### nn.CrossEntropyLoss

交叉熵损失函数

该公式用的也较多，比如在图像分类神经网络模型中就常常用到该公式。

```python
criterion = nn.CrossEntropyLoss()
loss = criterion(sample, target)
print(loss)
最后结果是：报错，看来不能直接这么用！
```

看文档我们知道 nn.CrossEntropyLoss 损失函数是用于图像识别验证的，对输入参数有各式要求，这里有这个概念就可以了，在图像识别一文中会有正确的使用方法。

### nn.NLLLoss

负对数似然损失函数（Negative Log Likelihood）

在前面接上一个 LogSoftMax 层就等价于交叉熵损失了。注意这里的 xlabel 和上个交叉熵损失里的不一样，这里是经过 log 运算后的数值。这个损失函数一般也是用在图像识别模型上。

```python
criterion = F.nll_loss()
loss = criterion(sample, target)
print(loss)
loss=F.nll_loss(sample,target)
最后结果会报错！
```

Nn.NLLLoss 和 nn.CrossEntropyLoss 的功能是非常相似的！通常都是用在多分类模型中，实际应用中我们一般用 NLLLoss 比较多。

### nn.NLLLoss2d

和上面类似，但是多了几个维度，一般用在图片上。

```python
input, (N, C, H, W)
target, (N, H, W)
```

比如用全卷积网络做分类时，最后图片的每个点都会预测一个类别标签。

```python
criterion = nn.NLLLoss2d()
loss = criterion(sample, target)
print(loss)
同样结果报错！
```

## 优化器Optim

所有的优化函数都位于torch.optim包下，常用的优化器有：SGD,Adam,Adadelta,Adagrad,Adamax等，下面就各优化器分析。

### 使用

```python
optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr = 0.0001)
```

lr：学习率，大于0的浮点数 momentum:动量参数，大于0的浮点数 parameters：Variable参数，要优化的对象

### 基类 Optimizer

```python
torch.optim.Optimizer(params, defaults)
```

params (iterable) —— Variable 或者 dict的iterable。指定了什么参数应当被优化。 defaults —— (dict)：包含了优化选项默认值的字典（一个参数组没有指定的参数选项将会使用默认值）。

### 方法：

- load_state_dict(state_dict)：加载optimizer状态。
- state_dict()：以dict返回optimizer的状态。包含两项：state - 一个保存了当前优化状态的dict，param_groups - 一个包含了全部参数组的dict。
- add_param_group(param_group)：给 optimizer 管理的参数组中增加一组参数，可为该组参数定制 lr,momentum, weight_decay 等，在 finetune 中常用。
- step(closure) ：进行单次优化 (参数更新)。
- zero_grad() ：清空所有被优化过的Variable的梯度。

## 优化算法

### 随机梯度下降算法 SGD算法

SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：

```python
torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float) ：学习率 momentum (float, 可选) ：动量因子（默认：0） weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认：0） dampening (float, 可选) :动量的抑制因子（默认：0） nesterov (bool, 可选) :使用Nesterov动量（默认：False） 可实现 SGD 优化算法，带动量 SGD 优化算法，带 NAG(Nesterov accelerated gradient)动量 SGD 优化算法,并且均可拥有 weight_decay 项。

对于训练数据集，我们首先将其分成n个batch，每个batch包含m个样本。我们每次更新都利用一个batch的数据，而非整个数据集。这样做使得训练数据太大时，利用整个数据集更新往往时间上不现实。batch的方法可以减少机器的压力，并且可以快速收敛。 当训练集有冗余时，batch方法收敛更快。 优缺点： SGD完全依赖于当前batch的梯度，所以η可理解为允许当前batch的梯度多大程度影响参数更新。对所有的参数更新使用同样的learning rate，选择合适的learning rate比较困难，容易收敛到局部最优。

### *平均随机梯度下降算法 ASGD算法*

ASGD 就是用空间换时间的一种 SGD。

```python
torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) ： 学习率（默认：1e-2） lambd (float, 可选) ：衰减项（默认：1e-4） alpha (float, 可选) ：eta更新的指数（默认：0.75） t0 (float, 可选) ：指明在哪一次开始平均化（默认：1e6） weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0）

### *Adagrad算法*

AdaGrad算法就是将每一个参数的每一次迭代的梯度取平方累加后在开方，用全局学习率除以这个数，作为学习率的动态更新。

其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数，为了数值稳定大约设置为10^-7 。

```python
torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) ：学习率（默认: 1e-2） lr_decay (float, 可选) ：学习率衰减（默认: 0） weight_decay (float, 可选) ： 权重衰减（L2惩罚）（默认: 0） 优缺点： Adagrad 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。在深度学习算法中，深度过深会造成训练提早结束。

### *自适应学习率调整 Adadelta算法*

Adadelta是对Adagrad的扩展，主要针对三个问题：

学习率后期非常小的问题； 手工设置初始学习率； 更新xt时，两边单位不统一 针对以上的三个问题，Adadelta提出新的Adag解决方法。Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。

```python
torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict rho (float, 可选) ： 用于计算平方梯度的运行平均值的系数（默认：0.9） eps (float, 可选)： 为了增加数值计算的稳定性而加到分母里的项（默认：1e-6） lr (float, 可选)： 在delta被应用到参数更新之前对它缩放的系数（默认：1.0） weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0） 优缺点： Adadelta已经不依赖于全局学习率。训练初中期，加速效果不错，很快，训练后期，反复在局部最小值附近抖动。

### *RMSprop算法*

RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。 RMSprop 采用均方根作为分 母，可缓解 Adagrad 学习率下降较快的问题， 并且引入均方根，可以减少摆动。

```python
torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
```

params (iterable) ：待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) ：学习率（默认：1e-2） momentum (float, 可选) : 动量因子（默认：0） alpha (float, 可选) : 平滑常数（默认：0.99） eps (float, 可选) : 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） centered (bool, 可选):如果为True，计算中心化的RMSProp，并且用它的方差预测值对梯度进行归一化 weight_decay (float, 可选)：权重衰减（L2惩罚）（默认: 0）

### *自适应矩估计 Adam算法*

```python
torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：1e-3） betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999） eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0） 优缺点： Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。 Adam结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点。

- 计算效率高
- 很少的内存需求
- 梯度的对角线重缩放不变（这意味着亚当将梯度乘以仅带正因子的对角矩阵是不变的，以便更好地理解此堆栈交换）
- 非常适合数据和/或参数较大的问题
- 适用于非固定目标
- 适用于非常嘈杂和/或稀疏梯度的问题
- 超参数具有直观的解释，通常需要很少的调整（我们将在配置部分中对此进行详细介绍）

### *Adamax算法（Adamd的无穷范数变种）*

Adamax 是对 Adam 增加了一个学习率上限的概念，所以也称之为 Adamax。

```python
torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：2e-3） betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数 eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0） 优缺点：

Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。 Adamax学习率的边界范围更简单。

### *SparseAdam算法*

针对稀疏张量的一种“阉割版”Adam 优化方法。

```python
torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：2e-3） betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数 eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）

### *L-BFGS算法*

L-BFGS 属于拟牛顿算法。 L-BFGS 是对 BFGS 的改进，特点就是节省内存。

```python
torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, 
tolerance_grad=1e-05, tolerance_change=1e-09, 
history_size=100, line_search_fn=None)
```

lr (float) – 学习率（默认：1） max_iter (int) – 每一步优化的最大迭代次数（默认：20）) max_eval (int) – 每一步优化的最大函数评价次数（默认：max * 1.25） tolerance_grad (float) – 一阶最优的终止容忍度（默认：1e-5） tolerance_change (float) – 在函数值/参数变化量上的终止容忍度（默认：1e-9） history_size (int) – 更新历史的大小（默认：100）

### *弹性反向传播算法 Rprop算法*

该优化方法适用于 full-batch，不适用于 mini-batch。不推荐。

```python
torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))
```

params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：1e-2） etas (Tuple[float, float], 可选) – 一对（etaminus，etaplis）, 它们分别是乘法的增加和减小的因子（默认：0.5，1.2） step_sizes (Tuple[float, float], 可选) – 允许的一对最小和最大的步长（默认：1e-6，50） 优缺点： 该优化方法适用于 full-batch，不适用于 mini-batch。几种优化器

## 学习率

```python
# scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
# # (1) 等间隔调整学习率 StepLR
# torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)
# # (2) 多间隔调整学习率 MultiStepLR
# torch.optim.lr_sheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)
# # (3) 指数衰减调整学习率 ExponentialLR
# torch.optim.lr_sheduler.ExponentialLR(optimizer, gamma, last_epoch)
# # (4) 余弦退火函数调整学习率：
# torch.optim.lr_sheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)
# # （5）根据指标调整学习率 ReduceLROnPlateau
# torch.optim.lr_sheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10,
#  verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)
```

# 小样本机器学习

## 可能性

实现小样本的及其学习：

1. 先验知识（更确切的说：世界模型）
2. 数据的利用率

## 基于先验知识

### 基于预训练的范式

Transfer Learning。

**Transfer learning是否可以减少训练数据本质取决于1， 训练backbone的大数据集和最终需要小数据学习的特定数据集的差距。 2， backbone 到底掌握了多少解决不同领域问题的基础知识。**在无法量化1和2的时候，这个效果就有点炼金术的感觉。 值得一提的是，当下的因果推断给出了对这一问题定量的一定依据。因为backbone里掌握的知识如果越接近下游任务的因， 则transfer Learning的效果会更好。

1. 当下开始慢慢进入主流的一个迁移学习范式，是所谓的**自监督学习**，被lecun这类大牛认定为深度学习的救赎。 这类学习范式通常免除了数据收集的魔咒就是标注。这个学习的通常范式是<font color='orange'>把这些原始数据的一部分遮盖住</font>， 让机器猜测被遮盖的部分， 或者把数据分段，用过去一段的数据预测下一段的数据，机器猜测和预测的过程就会学习到一个生成模型， 学到视频或自然语言背后的结构性信息。

2. 另一类崛起的范式是**contrasive learning**，对比性学习，比如在一个图片里抠出一小块，放到一大堆图片里让机器发掘是哪个图片里出来的， 这本质有点像聚类， 让机器充分的去比较相同或比较不同，最终发现图片里的结构信息。 这两种范式都可以在认知科学里找到一定对应，尤其是第一种， 和生物的预测性编码息息相关，有兴趣的同学可以关注贝叶斯大脑。

### 基于元学习的范式

靶向few shots learning的问题。 元学习指的是通过学习一系列任务集，而不是一个个数据，来掌握一个基本模型.

这个概念有时候和transfer learning很容易混淆，但是又不同。因为在meta - learninig 里， 往往训练是按照一个个episode 进行， 模型试图学习的是不仅仅是通用的特征，而是可以迅速在少量数据提取模型的方法本身， 这点有点像是一<font color='orange'>个训练模型的模型，或者学习如何学习</font>。

### 模块化系统

模块化的系统，可以通过**模块的复用或组合**迅速的实现小样本学习。 我本人在ICLR2020的网络， 通过先预训练不同认知功能的模块（比如认知地图，或者事件记忆），实现了在新环境里快速学习导航任务的泛化能力。 基础模块的学习本身其实可以看做是一个元学习的方法，因此可以看做是2的一种特殊情况

### 记忆系统

人具备情景记忆的能力。大量的经历被抽取到海马中， 使得人能够从相似的经历中抽象出一些共同的模型，在新的事件出现的时候， 通过做比较的方法， 来得到一个Q函数， 从而快速的学习，所谓以史为镜，可以明得失。

Continual and Multi-task Reinforcement Learning With Shared Episodic Memory

## 通过提高数据利用效率

### **改善优化方法**

 这就不得不提提当下深度学习速度慢的一大根源，就是随机梯度下降法

 但是在传统机器学习里， 有很多学习方法可以更充分的利用每个数据， 比如recursive least square这样的直接求最优解的方法，或者SVM这种通过对偶空间得到的优化方法。 甚至生物学习的基本方法Hebbian learning ，也有比深度学习当下的随机梯度下降法更高的数据利用率。在hebbian learning中，不同刺激引起的神经元先后放电会加强两者的联系，从而可以直接把两个不同的信号联系起来。因此， 如何改善学习方法也是实现小样本学习的重要途径。

在优化方面，一个小数据学习的致命杀手是灾难性遗忘， 也就是当模型需要连续的学习一些改变， 比如先识别1，2，3 ，再识别4，5，6， 学过1，2，3的网络往往很快就忘记了4，5，6，这无疑降低了数据利用率。 有一些优化的方法可以有效的缓解这一问题 ，其中有些还用到了脑启发的算法。 比如最近余山老师的文章。
[Continual learning of context-dependent processing in neural networks](https://www.nature.com/articles/s42256-019-0080-x)

### **减少模型的参数量**

训练过深度模型的同学都知道， 通常情况下，模型的参数越多，收敛速度就越慢。因此， 如果模型在达到类似效果的同时有效减少模型的参数数量， 会对小样本学习起到非常大的助力。

这其中的一类非常典型的代表就是用一个随机的网络直接提取特征， 而不去训练内在的权重， 然后只训练一个读出层得到需要的输出。由于 读出层的参数数量往往比整个网络小很多， 从而减少需要的模型参数数量。 比较典型的如训练RNN用到的蓄水池训练。

生物大脑的网络连接synapse每时每刻都在进行着微小的随机扰动， 但是我们的日常认知功能却从来都十分稳定，这说明决定一个网络功能的可能不是网络权重大小的具体值。 一个被称为low rank pertubation的理论指出， 控制这样一个巨大的神经元数量平方级参量的网络，只需要控制一组和神经元数量成线性的向量即可， 这组向量叠加在整个随机网络之上，就可以让网络表现出各种不同的认知功能。