---
layout: post
title: 模型评价指标
date: 2021-05-18
Author: Kii. Chan
categories:
tags: [深度学习笔记]
comments: true

---

# Pytorch 正则化

## 2.1 L1 regularization

对于每个 ω 我们都向目标函数增加一个λ|ω| 。
L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。
相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。

PyTorch里的optimizer只能实现L2正则化，L1正则化只能手动实现：

```python
regularization_loss = 0
for param in model.parameters():
    regularization_loss += torch.sum(abs(param))

calssify_loss = criterion(pred,target)
loss = classify_loss + lamda * regularization_loss

optimizer.zero_grad()
loss.backward()
optimizer.step()
```

## 2.2 L2 regularization

对于网络中的每个权重 ω ，向目标函数中增加一个$\frac{1}{2} \lambda \omega^{2}$，其中 λ 是正则化强度。这样该式子关于梯度就是 λω 了。L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以 $w += -\lambda * W$向着0线性下降。选择一个合适的权重衰减系数λ非常重要，这个需要根据具体的情况去尝试，初步尝试可以使用 `1e-4` 或者 `1e-3`

在PyTorch中某些optimizer优化器的参数`weight_decay (float, optional)`就是 L2 正则项，它的默认值为0。

```python
optimizer = torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.001)
```