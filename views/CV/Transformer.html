<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer | KII IINE</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="manifest" href="/manifest.json">
    <link rel="apple-touch-icon" href="/icons/apple-touch-icon-152x152.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2c2c2c">
    <meta name="description" content="明早一起去看海 望向未来">
    <meta name="theme-color" content="#22979b">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#22979b">
    <meta name="msapplication-TileImage" content="/icons/msapplication-icon-144x144.png">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="google-site-verification" content="XCppppl60fPQTlwxDodwZIhMarkybEgwVpcEz85KTuQ">
    
    <link rel="preload" href="/assets/css/0.styles.c67cb200.css" as="style"><link rel="preload" href="/assets/js/app.bd60e0ea.js" as="script"><link rel="preload" href="/assets/js/3.a67abeb3.js" as="script"><link rel="preload" href="/assets/js/1.c373aa88.js" as="script"><link rel="preload" href="/assets/js/20.215d6ee5.js" as="script"><link rel="preload" href="/assets/js/11.747f0d2b.js" as="script"><link rel="prefetch" href="/assets/js/10.0c65cdf0.js"><link rel="prefetch" href="/assets/js/12.beb609e5.js"><link rel="prefetch" href="/assets/js/13.7d5668c8.js"><link rel="prefetch" href="/assets/js/14.dbeeea81.js"><link rel="prefetch" href="/assets/js/15.bba7d888.js"><link rel="prefetch" href="/assets/js/16.5899c8d3.js"><link rel="prefetch" href="/assets/js/17.472ebefb.js"><link rel="prefetch" href="/assets/js/18.adbd9d19.js"><link rel="prefetch" href="/assets/js/19.9ffc4a46.js"><link rel="prefetch" href="/assets/js/21.9814dfa9.js"><link rel="prefetch" href="/assets/js/22.8056cd43.js"><link rel="prefetch" href="/assets/js/23.50497de0.js"><link rel="prefetch" href="/assets/js/24.f3505051.js"><link rel="prefetch" href="/assets/js/25.c52620ea.js"><link rel="prefetch" href="/assets/js/26.6abe9d00.js"><link rel="prefetch" href="/assets/js/27.5be79d4f.js"><link rel="prefetch" href="/assets/js/28.7f365f70.js"><link rel="prefetch" href="/assets/js/29.41ace5f1.js"><link rel="prefetch" href="/assets/js/30.ac3fb413.js"><link rel="prefetch" href="/assets/js/31.2637de43.js"><link rel="prefetch" href="/assets/js/32.18a14f2a.js"><link rel="prefetch" href="/assets/js/33.52ee2c1c.js"><link rel="prefetch" href="/assets/js/34.b6fac093.js"><link rel="prefetch" href="/assets/js/35.0919dbfd.js"><link rel="prefetch" href="/assets/js/36.677e3b04.js"><link rel="prefetch" href="/assets/js/37.7045c611.js"><link rel="prefetch" href="/assets/js/38.422bb444.js"><link rel="prefetch" href="/assets/js/39.19a27d38.js"><link rel="prefetch" href="/assets/js/4.09dda623.js"><link rel="prefetch" href="/assets/js/40.20e48572.js"><link rel="prefetch" href="/assets/js/41.f85e542e.js"><link rel="prefetch" href="/assets/js/42.bdbc626d.js"><link rel="prefetch" href="/assets/js/43.3b5511e7.js"><link rel="prefetch" href="/assets/js/44.7095c92d.js"><link rel="prefetch" href="/assets/js/45.af5548ef.js"><link rel="prefetch" href="/assets/js/46.2f0d12a7.js"><link rel="prefetch" href="/assets/js/47.9c06cd18.js"><link rel="prefetch" href="/assets/js/48.3eb127a3.js"><link rel="prefetch" href="/assets/js/49.47c3ce4b.js"><link rel="prefetch" href="/assets/js/5.9efeece9.js"><link rel="prefetch" href="/assets/js/50.0a8735ce.js"><link rel="prefetch" href="/assets/js/51.aabdf005.js"><link rel="prefetch" href="/assets/js/6.45c24d02.js"><link rel="prefetch" href="/assets/js/7.3391c463.js"><link rel="prefetch" href="/assets/js/8.1d292254.js"><link rel="prefetch" href="/assets/js/9.49750083.js">
    <link rel="stylesheet" href="/assets/css/0.styles.c67cb200.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>KII IINE</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>明早一起去看海 望向未来</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2023
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/favicon.ico" alt="KII IINE" class="logo"> <span class="site-name">KII IINE</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/funny/" class="nav-link"><i class="undefined"></i>
  funny
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine/kii-chan-iine.github.io/tree/develop" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/avatar.jpeg" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    KII IINE
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>34</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>12</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/funny/" class="nav-link"><i class="undefined"></i>
  funny
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine/kii-chan-iine.github.io/tree/develop" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Transformer</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2023
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Transformer</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>kii</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>8/13/2023</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>CV</span><span class="tag-item" data-v-1ff7123e>DL</span></i></div></div> <div class="theme-reco-content content__default"><div id="boxx" data-v-f4ca0dac><div data-v-f4ca0dac><p v-if="true" class="custom-block-title" data-v-f4ca0dac></p> <p v-if="true" data-v-f4ca0dac></p></div></div> <div class="custom-block tip"><p class="title">前言</p><p>Transformer的numpy实现</p></div> <p>下面的代码自下而上的实现Transformer的相关模块功能。这份文档只实现了主要代码。由于时间关系，我无法实现所有函数。对于没有实现的函数，默认用全大写函数名指出，如SOFTMAX</p> <p>由于时间限制，以下文档只是实现了Transformer前向传播的过程。</p> <h2 id="输入层"><a href="#输入层" class="header-anchor">#</a> 输入层</h2> <p>输入层包括Word Embedding和Positional Encoding。Word Embedding可以认为是预训练的词向量，Positional Encoding用于捕获词语的相对位置信息。</p> <p>$\begin{aligned} PE(pos, 2i) &amp;= sin(pos / 10000^{\frac{2i}{d}}) \ PE(pos, 2i+1) &amp;= cos(pos / 10000^{\frac{2i}{d}}) \end{aligned}$</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># Word embedding matrix。通常从文件读入，这里随机初始化</span>
<span class="token comment"># word_embedding = np.arange(10)</span>
<span class="token comment"># word_embedding.reshape(vocabulary_size, word_embedding_size)</span>

max_seq_len <span class="token operator">=</span> <span class="token number">200</span> <span class="token comment"># 假定的最大序列长度</span>
position_size <span class="token operator">=</span> <span class="token number">512</span> <span class="token comment"># Position Embedding的维度</span>

<span class="token comment"># position_encoding是一个类似于word embeding的二维矩阵</span>
<span class="token comment"># 其中pos是序列中词语的位置，j是维度</span>
position_encoding <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
          <span class="token punctuation">[</span>pos <span class="token operator">/</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token operator">*</span> <span class="token punctuation">(</span>j <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> position_size<span class="token punctuation">)</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>position_size<span class="token punctuation">)</span><span class="token punctuation">]</span>
          <span class="token keyword">for</span> pos <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_seq_len<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Shape of position encoding: {}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>position_encoding<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 每个position encoding的偶数列使用sin，奇数列使用cos处理</span>
position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 为了对文本长度对齐，加上Padding行</span>
padding <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>position_size<span class="token punctuation">)</span>
position_encoding <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">(</span>padding<span class="token punctuation">,</span> position_encoding<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Shape of position encoding after adding padding: {}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>position_encoding<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">position_encoding</span><span class="token punctuation">(</span>sentence_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    给定一个batch的句子，输出这些句子的Position Embedding
    &quot;&quot;&quot;</span>
    <span class="token comment"># 模拟输入，batch_size=4</span>
    sentence_lens <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Shape of input: {}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>input_len<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 生成输入的位置索引，shape[batch_size, max_seq_len]</span>
    <span class="token comment"># 避开0的索引，不够长度的部分采用0填充</span>
    pos_index <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_seq_len <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token builtin">len</span> <span class="token keyword">in</span> sentence_lens<span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 利用pos_index在position_encoding中进行Lookup</span>
    position_embedding <span class="token operator">=</span> LOOKUP<span class="token punctuation">(</span>pos_index<span class="token punctuation">,</span> position_encoding<span class="token punctuation">)</span>

    <span class="token comment"># 返回维度[batch_size, max_seq_len, position_size]</span>
    <span class="token keyword">return</span> position_embedding

<span class="token keyword">def</span> <span class="token function">word_embdding</span><span class="token punctuation">(</span>sentence_words<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    给定一个batch句子，输出这些句子的Word Embedding
    &quot;&quot;&quot;</span>
    <span class="token comment"># 将word转换为index，通常输入前就做完了</span>
    word_index <span class="token operator">=</span> WORD2INDEX<span class="token punctuation">(</span>sentences_words<span class="token punctuation">)</span>
    word_embedding <span class="token operator">=</span> LOOKUP<span class="token punctuation">(</span>word_index<span class="token punctuation">,</span> word_embedding<span class="token punctuation">)</span>

    <span class="token comment"># 返回维度[batch_size, max_seq_len, word_embedding_size]</span>
    <span class="token keyword">return</span> word_embedding
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br></div></div><p>输出</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Shape of position encoding: (200, 512)
Shape of position encoding after adding padding: (201, 512)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>得到positional encoding和word embedding之后，将两部分拼接，得到输入向量</p> <h2 id="层标准化"><a href="#层标准化" class="header-anchor">#</a> 层标准化</h2> <p>层标准化将数据标准化为均值为0，标准差为1.以下是实现代码</p> <p>$BN(x_i)=\alpha \times \frac{x_i - \mu}{\sqrt{\delta^2 + \epsilon}}+\beta$</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">base_layer_norm</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    标准化张量x,假设x是三维张量，即
    x.shape = (B, L, D)
    通常第2维是我们要标准化的维度
    &quot;&quot;&quot;</span>
    <span class="token comment"># 求均值</span>
    mean <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token comment"># 求标准差</span>
    std <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> std

<span class="token keyword">def</span> <span class="token function">layer_norm</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    引入可学习参数gamma、beta, epsilon用来防止发生数值计算错误
    &quot;&quot;&quot;</span>
    <span class="token comment"># 求均值</span>
    mean <span class="token operator">=</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span>， keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># 求标准差</span>
    std <span class="token operator">=</span> np<span class="token punctuation">.</span>std<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> gamma <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>std <span class="token operator">+</span> epsilon<span class="token punctuation">)</span> <span class="token operator">+</span> beta<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><h2 id="缩放点积"><a href="#缩放点积" class="header-anchor">#</a> 缩放点积</h2> <p>因为缩放点积(Scaled dot-product Attention)是Self-Attention的基础，因此这里先实现它。该模块输入是K,Q,V三个张量，输出Context上下文张量和Attention张量</p> <p>$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">scaled_dot_product_attention</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Args:
        query: [batch_size, query_len, query_size]
        key: [batch_size, key_len, key_size]
        value: [batch_size, value_len, value_size]
    &quot;&quot;&quot;</span>
    scale <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>key_size<span class="token punctuation">)</span> <span class="token comment"># 缩放比例</span>
    att <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">.</span>swapaxes<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> scale
    <span class="token comment"># 利用softmax将att转换为一个概率分布</span>
    att <span class="token operator">=</span> SOFTMAX<span class="token punctuation">(</span>att<span class="token punctuation">)</span>
    <span class="token comment"># 得到上下文张量</span>
    context <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>att<span class="token punctuation">,</span> value<span class="token punctuation">)</span>

    <span class="token keyword">return</span> contenx<span class="token punctuation">,</span> att
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><h2 id="multi-head-attention"><a href="#multi-head-attention" class="header-anchor">#</a> Multi-head Attention</h2> <p>论文中使用了8个head，也就是把上述的K，Q，V三个张量按照维度分为8份，每份都经过仿射变换后送入到缩放点积中。</p> <p>主要流程为：将K，Q，V进行仿射变换，得到对应的query，key和value；然后将它们根据head数目进行维度划分，送入到对应的缩放点积模块进行训练，得到Context张量和Attention张量；多个head的Context张量拼接后经过线性变换就得到了全局的Context张量；最后为了使模型能够更深，收敛更快，对输出加上了dropout，残差连接和层标准化。</p> <p>下面是代码实现：</p> <p>$MultiHead(Q,K,V)=Concat(head_1, head_2,\cdots,head_h)W_c + b_c$</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">multihead_attention</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> input_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Args:
        query, key, value和缩放点积部分一致
        num_heads: multi-head attention 个数
        input_dim: 输入维度
    &quot;&quot;&quot;</span>
    <span class="token comment"># 恒等映射的残差，先保存下来</span>
    residual <span class="token operator">=</span> query

    <span class="token comment"># 每个head分到的维度大小</span>
    per_head <span class="token operator">=</span> input_dim <span class="token operator">//</span> num_heads

    <span class="token comment"># 对query，key，value进行仿射运算</span>
    <span class="token comment"># W_q,W_k,W_v是三个可学习二维矩阵，shape=[input_dim, (input_dim // num_heads)*num_heads]</span>
    query<span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query<span class="token punctuation">,</span> W_q<span class="token punctuation">)</span> <span class="token operator">+</span> b_q
    key <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>key<span class="token punctuation">,</span> W_k<span class="token punctuation">)</span> <span class="token operator">+</span> b_k
    value <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>key<span class="token punctuation">,</span> W_v<span class="token punctuation">)</span> <span class="token operator">+</span> b_v

    <span class="token comment"># 根据每个head分到的维度对query，key，value重新切分</span>
    qeury <span class="token operator">=</span> query<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> per_head<span class="token punctuation">)</span>
    key <span class="token operator">=</span> key<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> per_head<span class="token punctuation">)</span>
    value <span class="token operator">=</span> value<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> per_head<span class="token punctuation">)</span>

    <span class="token comment"># 对切分的query，key，value进行缩放点积</span>
    context<span class="token punctuation">,</span> att <span class="token operator">=</span> scaled_dot_product_attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">)</span>

    <span class="token comment"># 将各个head的上下文向量拼接得到最终的context向量</span>
    context <span class="token operator">=</span> context<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> per_head <span class="token operator">*</span> num_heads<span class="token punctuation">)</span>

    <span class="token comment"># context还需要经过一个线性变换,其中W_c是可学习二维矩阵，shape=[input_dim, input_dim]</span>
    context <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>context<span class="token punctuation">,</span> W_c<span class="token punctuation">)</span> <span class="token operator">+</span> b_c

    <span class="token comment"># dropout层</span>
    context <span class="token operator">=</span> DROPOUT<span class="token punctuation">(</span>context<span class="token punctuation">)</span>

    <span class="token comment"># 输出前进行残差连接和层标准化</span>
    output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> context<span class="token punctuation">)</span>

    <span class="token comment"># 输出</span>
    <span class="token keyword">return</span> output<span class="token punctuation">,</span> att
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br></div></div><h2 id="mask"><a href="#mask" class="header-anchor">#</a> Mask</h2> <p>Transformer中有Padding Mask和Sequence Mask。Padding Mask在计算Attention时用来消除某些位置的Attention值，使其在上下文张量中不起作用。Sequence Mask用于Decoder部分，主要是Mask掉当前输出词之后的序列，因为解码过程中是不知道后续词信息的。</p> <p>为简单起见，上面的Attention都没有考虑Padding Mask。</p> <h2 id="feed-forward层"><a href="#feed-forward层" class="header-anchor">#</a> Feed Forward层</h2> <p>该全连接网络首先将输入x做了一次仿射变换，然后经过ReLU激活函数，再做一次仿射变化，得到最终的输出。</p> <p>$FFN(x)=ReLU(xW_1+b_1)W_2 + b_2$</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">feed_forward</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 进行一次仿射变换，其中W_1和b_1分别为矩阵和偏置</span>
    out <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W_1<span class="token punctuation">)</span> <span class="token operator">+</span> b_1
    <span class="token comment"># 施加激活函数</span>
    out <span class="token operator">=</span> ReLU<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
    <span class="token comment"># 再进行仿射运算，其中W_2和b_2分别为矩阵和偏置</span>
    out <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>out<span class="token punctuation">,</span> W_2<span class="token punctuation">)</span> <span class="token operator">+</span> b_2

    <span class="token comment"># Dropout</span>
    out <span class="token operator">=</span> DROPOUT<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

    <span class="token comment"># 添加残差连接和层标准化</span>
    <span class="token keyword">return</span> layer_norm<span class="token punctuation">(</span>x <span class="token operator">+</span> out<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><h2 id="encoder"><a href="#encoder" class="header-anchor">#</a> Encoder</h2> <p>整个的Encoder有流程，每一层都是Multi-head Attention和Feed Forward模块组成。代码如下：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Encoder部分一层的结构表示
    每层中有Multi-head Attention和Feed Forward前向网络
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        一些参数设置，如head大小，输入维度等
        &quot;&quot;&quot;</span>
        <span class="token keyword">pass</span>

    <span class="token keyword">def</span> <span class="token function">encode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Multi-head Attention</span>

        <span class="token comment"># 先从inputs中获得对应的query，key，value</span>
        query <span class="token operator">=</span> inputs<span class="token punctuation">.</span>GET_QUERY<span class="token punctuation">(</span><span class="token punctuation">)</span>
        key <span class="token operator">=</span> inputs<span class="token punctuation">.</span>GET_KEY<span class="token punctuation">(</span><span class="token punctuation">)</span>
        value <span class="token operator">=</span> inputs<span class="token punctuation">.</span>GET_VALUE<span class="token punctuation">(</span><span class="token punctuation">)</span>
        context<span class="token punctuation">,</span> attention <span class="token operator">=</span> multihead_attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span>

        <span class="token comment"># Feed forward层</span>
        output <span class="token operator">=</span> feed_forward<span class="token punctuation">(</span>context<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention


<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    完整Encoder的表示
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 定义Encoder所有的层</span>
        self<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> <span class="token punctuation">[</span>layer1<span class="token punctuation">,</span> layer2<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">,</span>layer6<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> input_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 获得嵌入表示</span>
        word_embedding <span class="token operator">=</span> word_embedding<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        position_embedding <span class="token operator">=</span> position_encoding<span class="token punctuation">(</span>inputs_lens<span class="token punctuation">)</span>
        final_embedding <span class="token operator">=</span> word_embedding <span class="token operator">+</span> position_embedding

        <span class="token comment"># 一层层进行编码</span>
        final_attention <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>encoder_layers<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> attention <span class="token operator">=</span> layer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>final_embedding<span class="token punctuation">)</span>
            final_attention<span class="token punctuation">.</span>append<span class="token punctuation">(</span>attention<span class="token punctuation">)</span>

        <span class="token comment"># output只返回最后一层，attention全部返回</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br></div></div><h2 id="decoder"><a href="#decoder" class="header-anchor">#</a> Decoder</h2> <p>Decoder的除了和Encoder一样，有Multi-head Attention和Feed Forward外，还有一层Masked Multi-head Attention在最下面。代码如下：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Decoder部分一层的结构表示
    每层中有两个Multi-head Attention和一个Feed Forward前向网络模块
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_output<span class="token punctuation">,</span> decoder_inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        与Encoder不同，Decoder不仅关注自己的输入，还要考虑Encoder的输出
        &quot;&quot;&quot;</span>
        <span class="token comment"># 下层Multi-head Attention</span>
        <span class="token comment"># 先从decoder_inputs中获得对应的query，key，value</span>
        query <span class="token operator">=</span> decoder_inputs<span class="token punctuation">.</span>GET_QUERY<span class="token punctuation">(</span><span class="token punctuation">)</span>
        key <span class="token operator">=</span> decoder_inputs<span class="token punctuation">.</span>GET_KEY<span class="token punctuation">(</span><span class="token punctuation">)</span>
        value <span class="token operator">=</span> decoer_inputs<span class="token punctuation">.</span>GET_VALUE<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output<span class="token punctuation">,</span> attention1 <span class="token operator">=</span> multihead_attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span>

        <span class="token comment"># 上层Multi-head Attention</span>
        <span class="token comment"># 再从encoder_outputs中获取key和value，decoder的output中获取query</span>
        query <span class="token operator">=</span> output<span class="token punctuation">.</span>GET_QUERY<span class="token punctuation">(</span><span class="token punctuation">)</span>
        key <span class="token operator">=</span> encoder_outputs<span class="token punctuation">.</span>GET_KEY<span class="token punctuation">(</span><span class="token punctuation">)</span>
        value <span class="token operator">=</span> encoder_output<span class="token punctuation">.</span>GET_VALUE<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output<span class="token punctuation">,</span> attention2 <span class="token operator">=</span> multihead_attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span>

        <span class="token comment"># Feed forward层</span>
        output <span class="token operator">=</span> feed_forward<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention1<span class="token punctuation">,</span> attention2

<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    完整的Decoder表示
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 定义Dncoder所有的层</span>
        self<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> <span class="token punctuation">[</span>layer1<span class="token punctuation">,</span> layer2<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">,</span>layer6<span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> input_lens<span class="token punctuation">,</span> encoder_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 获得嵌入表示</span>
        word_embedding <span class="token operator">=</span> word_embedding<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        position_embedding <span class="token operator">=</span> position_encoding<span class="token punctuation">(</span>inputs_lens<span class="token punctuation">)</span>
        final_embedding <span class="token operator">=</span> word_embedding <span class="token operator">+</span> position_embedding

        <span class="token comment"># Sequence Mask。解码过程中要做Sequence Mask</span>
        seq_mask <span class="token operator">=</span> SEQUENCE_MASK<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>

        <span class="token comment"># 一层层进行解码</span>
        self_attentions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        context_attentions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>decoder_layers<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> self_attention<span class="token punctuation">,</span> context_attention <span class="token operator">=</span> layer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>encoder_outputs<span class="token punctuation">,</span> final_embedding<span class="token punctuation">)</span>
            self_attentions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self_attention<span class="token punctuation">)</span>
            context_attentions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>context_attention<span class="token punctuation">)</span>

        <span class="token comment"># output只返回最后一层，attention全部返回</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> self_attentions<span class="token punctuation">,</span> context_attentions
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br></div></div><h2 id="transformer整体"><a href="#transformer整体" class="header-anchor">#</a> Transformer整体</h2> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Transformer整体代码
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        参数设置：参数主要有
        Args:
            src_vocab_size: 源语言词汇表大小
            src_max_len: 源语言语句最大长度
            tgt_vocab_size: 目标语言词汇表大小
            tgt_max_len: 目标语言语句最大长度
            num_layers=6: 默认Encoder和Decoder为6层
            inputs_dim=512: 输入维度默认为512
            num_heads=8: 默认Multi-head Attention个数为8
            feed_forward_dim=2048：前馈网络维度
            drop_out=0.2: Dropout概率
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 构造编码器</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 构造解码器</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src_seq<span class="token punctuation">,</span> src_len<span class="token punctuation">,</span> tgt_seq<span class="token punctuation">,</span> tgt_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        编解码一个batch的过程
        Args:
            src_seq: 源语言序列
            src_len: 源语言序列长度
            tgt_seq: 目标语言序列
            tgt_len: 目标语言序列长度
        &quot;&quot;&quot;</span>
        <span class="token comment"># 编码过程</span>
        output<span class="token punctuation">,</span> encoder_attention <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>src_seq<span class="token punctuation">,</span> src_len<span class="token punctuation">)</span>

        <span class="token comment"># 解码过程</span>
        output<span class="token punctuation">,</span> self_attention<span class="token punctuation">,</span> context_attention <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>tgt_seq<span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> output<span class="token punctuation">)</span>

        <span class="token comment"># 最终要输出概率，所以最终结果还要经过线性层和softmax层</span>
        output <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>output<span class="token punctuation">,</span> W_T<span class="token punctuation">)</span> <span class="token operator">+</span> b_T <span class="token comment"># 其中，W_T和b_T是线性层的二维矩阵和偏置</span>
        <span class="token comment"># 输出概率</span>
        output <span class="token operator">=</span> SOFTMAX<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> encoder_attention<span class="token punctuation">,</span> self_attention<span class="token punctuation">,</span> context_attention
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br></div></div></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#输入层" class="sidebar-link reco-side-输入层" data-v-70334359>输入层</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#层标准化" class="sidebar-link reco-side-层标准化" data-v-70334359>层标准化</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#缩放点积" class="sidebar-link reco-side-缩放点积" data-v-70334359>缩放点积</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#multi-head-attention" class="sidebar-link reco-side-multi-head-attention" data-v-70334359>Multi-head Attention</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#mask" class="sidebar-link reco-side-mask" data-v-70334359>Mask</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#feed-forward层" class="sidebar-link reco-side-feed-forward层" data-v-70334359>Feed Forward层</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#encoder" class="sidebar-link reco-side-encoder" data-v-70334359>Encoder</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#decoder" class="sidebar-link reco-side-decoder" data-v-70334359>Decoder</a></li><li class="level-2" data-v-70334359><a href="/views/CV/Transformer.html#transformer整体" class="sidebar-link reco-side-transformer整体" data-v-70334359>Transformer整体</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><canvas id="vuepress-canvas-cursor"></canvas><!----><div class="vuepress-canvas-nest-element"></div><div class="kanbanniang" data-v-27e9bfa4><div class="banniang-container" style="display:;" data-v-27e9bfa4><div class="messageBox" style="position:fixed;right:75px;bottom:235px;opacity:0.75;height:max-content;width:200px;fon-szie:16px;display:none;" data-v-27e9bfa4></div> <div class="operation" style="display:;" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660425629" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6044" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M577.5584 307.848533l-21.345067-18.699733c-0.062933-0.061867-0.186667-0.123733-0.280533-0.186667l-44.305067-38.689067-53.752533 47.5648L127.009067 587.424l0 177.1392 0 155.0048c0 45.886933 37.454933 83.0976 83.610667 83.0976l183.966933 0L394.586667 735.8688c0-27.512533 22.448-49.8336 50.162133-49.8336l133.7728 0c27.714133 0 50.178133 22.321067 50.178133 49.8336L628.699733 1002.666667l183.966933 0c46.170667 0 83.610667-37.211733 83.610667-83.0976L896.277333 763.9424 896.277333 586.7712 578.5216 308.688 577.5584 307.848533z" p-id="6045" data-v-27e9bfa4></path> <path d="M990.637867 418.164267l-94.360533-82.600533 0-181.290667c0-36.714667-29.952-66.482133-66.894933-66.482133-36.941867 0-66.893867 29.767467-66.893867 66.482133l0 64.197333L556.213333 37.911467c-25.291733-22.103467-63.165867-22.103467-88.4256 0L33.348267 418.164267c-27.730133 24.247467-30.402133 66.264533-5.9808 93.808 24.437333 27.544533 66.692267 30.219733 94.407467 5.938133L512 176.376533l390.2432 341.533867c12.7072 11.130667 28.4608 16.600533 44.181333 16.600533 18.549333 0 37.0048-7.617067 50.209067-22.538667C1021.054933 484.4128 1018.382933 442.4128 990.637867 418.164267z" p-id="6046" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660394444" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5299" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M0 202.7V631c0 83.3 68.3 150.7 152.6 150.7h228.9l8 190.3 224.9-190.3h257c84.3 0 152.6-67.4 152.6-150.7V202.7C1024 119.4 955.7 52 871.4 52H152.6C68.3 52 0 119.4 0 202.7z m658.6 237.9c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S771 512 730.9 512c-40.2 0-72.3-31.7-72.3-71.4z m-220.9 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S550.1 512 510 512c-40.2 0-72.3-31.7-72.3-71.4z m-216.8 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S333.3 512 293.1 512c-40.1 0-72.2-31.7-72.2-71.4z" p-id="5300" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660570409" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2153" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 393.846154c-86.646154 0-157.538462 70.892308-157.538462 157.538461s70.892308 157.538462 157.538462 157.538462 157.538462-70.892308 157.538462-157.538462-70.892308-157.538462-157.538462-157.538461z m393.846154-118.153846h-102.4c-27.569231 0-51.2-13.784615-66.953846-35.446154l-45.292308-68.923077C677.415385 137.846154 643.938462 118.153846 608.492308 118.153846h-192.984616c-35.446154 0-68.923077 19.692308-84.676923 53.169231l-45.292307 68.923077c-13.784615 21.661538-39.384615 35.446154-66.953847 35.446154H118.153846c-43.323077 0-78.769231 35.446154-78.769231 78.76923v472.615385c0 43.323077 35.446154 78.769231 78.769231 78.769231h787.692308c43.323077 0 78.769231-35.446154 78.769231-78.769231V354.461538c0-43.323077-35.446154-78.769231-78.769231-78.76923zM512 787.692308c-129.969231 0-236.307692-106.338462-236.307692-236.307693s106.338462-236.307692 236.307692-236.307692 236.307692 106.338462 236.307692 236.307692-106.338462 236.307692-236.307692 236.307693z" p-id="2154" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660469241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6553" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M706.544835 64.021106h-6.500041a159.889547 159.889547 0 0 0-83.558068 23.557532c-54.583153 33.422204-86.949304 40.439014-104.486726 40.439014-17.538445 0-49.903573-7.016811-104.494912-40.445154a159.88136 159.88136 0 0 0-83.550905-23.551392h-6.507204a127.823224 127.823224 0 0 0-97.182366 44.702108l-172.836417 201.635323c-19.522636 22.774703-20.600177 56.050574-2.609431 80.047104l95.995331 127.994116a63.99757 63.99757 0 0 0 83.198887 17.024745v328.558038c0 52.93256 43.060725 95.995331 95.995331 95.995331h415.98011c52.934606 0 95.995331-43.062771 95.995331-95.995331V535.424502a64.028269 64.028269 0 0 0 42.240033 7.749498 64.013943 64.013943 0 0 0 46.990221-34.528398l63.996546-127.856993c11.522428-23.027459 8.125051-50.721195-8.632611-70.278623L803.743574 108.74675c-24.335245-28.421306-59.770292-44.725644-97.198739-44.725644z" p-id="6554" data-v-27e9bfa4></path></svg></i>
      <a target="_blank" href="https://github.com/kii-chan-iine" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660325062" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3809" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 3.413333c280.849067 0 508.586667 199.273813 508.586667 444.94848 0 140.427947-74.519893 265.53344-190.65856 347.11552V1020.586667l-222.839467-135.168c-30.859947 5.147307-62.552747 8.021333-95.085227 8.021333-280.845653 0-508.586667-199.28064-508.586666-445.078187C3.413333 202.687147 231.150933 3.413333 512 3.413333z m-158.96576 603.921067h317.805227c17.578667 0 31.812267-14.2336 31.812266-31.819093a31.798613 31.798613 0 0 0-31.812266-31.80544h-317.805227c-17.578667 0-31.812267 14.2336-31.812267 31.80544 0.116053 17.585493 14.349653 31.819093 31.812267 31.819093z m-63.511893-190.665387h444.951893c17.578667 0 31.812267-14.2336 31.812267-31.812266a31.802027 31.802027 0 0 0-31.812267-31.81568H289.522347a31.802027 31.802027 0 0 0-31.81568 31.81568c0 17.578667 14.2336 31.812267 31.81568 31.812266z" p-id="3810" data-v-27e9bfa4></path></svg></i></a> <i data-v-27e9bfa4><svg t="1572660347392" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4543" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 34.133333a486.4 486.4 0 1 0 486.4 486.4A486.4 486.4 0 0 0 512 34.133333z m209.4848 632.8064l-55.6032 55.466667-151.517867-151.125333-151.517866 151.1168-55.6032-55.466667 151.517866-151.108267L307.242667 364.714667l55.6032-55.466667 151.517866 151.125333 151.517867-151.1168 55.6032 55.466667-151.517867 151.099733z m0 0" p-id="4544" data-v-27e9bfa4></path></svg></i></div> <canvas id="banniang" width="216" height="281.6" class="live2d" style="position:fixed;right:90px;bottom:-20px;opacity:1;" data-v-27e9bfa4></canvas></div> <div class="showBanNiang" style="display:none;" data-v-27e9bfa4>
    看板娘
  </div></div><!----><div></div></div></div>
    <script src="/assets/js/app.bd60e0ea.js" defer></script><script src="/assets/js/3.a67abeb3.js" defer></script><script src="/assets/js/1.c373aa88.js" defer></script><script src="/assets/js/20.215d6ee5.js" defer></script><script src="/assets/js/11.747f0d2b.js" defer></script>
  </body>
</html>
