<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>深度学习的思考 | KII IINE</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="manifest" href="/manifest.json">
    <link rel="apple-touch-icon" href="/icons/apple-touch-icon-152x152.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2c2c2c">
    <meta name="description" content="明早一起去看海 望向未来">
    <meta name="theme-color" content="#22979b">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#22979b">
    <meta name="msapplication-TileImage" content="/icons/msapplication-icon-144x144.png">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="google-site-verification" content="XCppppl60fPQTlwxDodwZIhMarkybEgwVpcEz85KTuQ">
    
    <link rel="preload" href="/assets/css/0.styles.c67cb200.css" as="style"><link rel="preload" href="/assets/js/app.98ee6730.js" as="script"><link rel="preload" href="/assets/js/3.a67abeb3.js" as="script"><link rel="preload" href="/assets/js/1.c373aa88.js" as="script"><link rel="preload" href="/assets/js/34.139d33a3.js" as="script"><link rel="preload" href="/assets/js/11.747f0d2b.js" as="script"><link rel="prefetch" href="/assets/js/10.0c65cdf0.js"><link rel="prefetch" href="/assets/js/12.beb609e5.js"><link rel="prefetch" href="/assets/js/13.7d5668c8.js"><link rel="prefetch" href="/assets/js/14.f76d10b9.js"><link rel="prefetch" href="/assets/js/15.5d79064d.js"><link rel="prefetch" href="/assets/js/16.7a3e1fe5.js"><link rel="prefetch" href="/assets/js/17.83a48523.js"><link rel="prefetch" href="/assets/js/18.9cb9c097.js"><link rel="prefetch" href="/assets/js/19.70665c34.js"><link rel="prefetch" href="/assets/js/20.15e41a45.js"><link rel="prefetch" href="/assets/js/21.3ce32672.js"><link rel="prefetch" href="/assets/js/22.da6c6f6d.js"><link rel="prefetch" href="/assets/js/23.d6cd23fa.js"><link rel="prefetch" href="/assets/js/24.a489fd57.js"><link rel="prefetch" href="/assets/js/25.6f5606a9.js"><link rel="prefetch" href="/assets/js/26.6abe9d00.js"><link rel="prefetch" href="/assets/js/27.5feb58f6.js"><link rel="prefetch" href="/assets/js/28.7f365f70.js"><link rel="prefetch" href="/assets/js/29.857b8c9d.js"><link rel="prefetch" href="/assets/js/30.6c23ee9d.js"><link rel="prefetch" href="/assets/js/31.2fc64330.js"><link rel="prefetch" href="/assets/js/32.c13cb598.js"><link rel="prefetch" href="/assets/js/33.78f6d0e3.js"><link rel="prefetch" href="/assets/js/35.66965c0b.js"><link rel="prefetch" href="/assets/js/36.677e3b04.js"><link rel="prefetch" href="/assets/js/37.66a40165.js"><link rel="prefetch" href="/assets/js/38.19b003c9.js"><link rel="prefetch" href="/assets/js/39.7aa321b5.js"><link rel="prefetch" href="/assets/js/4.09dda623.js"><link rel="prefetch" href="/assets/js/40.3e19d0f4.js"><link rel="prefetch" href="/assets/js/41.cac70d9c.js"><link rel="prefetch" href="/assets/js/42.2c6fbe96.js"><link rel="prefetch" href="/assets/js/43.7965fb36.js"><link rel="prefetch" href="/assets/js/44.7095c92d.js"><link rel="prefetch" href="/assets/js/45.af5548ef.js"><link rel="prefetch" href="/assets/js/46.2f0d12a7.js"><link rel="prefetch" href="/assets/js/47.9c06cd18.js"><link rel="prefetch" href="/assets/js/48.3eb127a3.js"><link rel="prefetch" href="/assets/js/49.47c3ce4b.js"><link rel="prefetch" href="/assets/js/5.9efeece9.js"><link rel="prefetch" href="/assets/js/50.0a8735ce.js"><link rel="prefetch" href="/assets/js/51.aabdf005.js"><link rel="prefetch" href="/assets/js/6.45c24d02.js"><link rel="prefetch" href="/assets/js/7.3391c463.js"><link rel="prefetch" href="/assets/js/8.1d292254.js"><link rel="prefetch" href="/assets/js/9.49750083.js">
    <link rel="stylesheet" href="/assets/css/0.styles.c67cb200.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>KII IINE</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>明早一起去看海 望向未来</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2023
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/favicon.ico" alt="KII IINE" class="logo"> <span class="site-name">KII IINE</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/funny/" class="nav-link"><i class="undefined"></i>
  funny
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine/kii-chan-iine.github.io/tree/develop" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/avatar.jpeg" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    KII IINE
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>34</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>12</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/funny/" class="nav-link"><i class="undefined"></i>
  funny
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine/kii-chan-iine.github.io/tree/develop" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc></h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2023
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">深度学习的思考</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>KII IINE</span></i> <!----> <!----> <!----></div></div> <div class="theme-reco-content content__default"><h1 id="深度学习的思考"><a href="#深度学习的思考" class="header-anchor">#</a> 深度学习的思考</h1> <p>在VGG中，卷积网络达到了19层，在GoogLeNet中，网络史无前例的达到了22层。那么，网络的精度会随着网络的层数增多而增多吗？在深度学习中，网络层数增多一般会伴着下面几个问题</p> <ol><li>计算资源的消耗</li> <li>模型容易过拟合</li> <li>梯度消失/梯度爆炸问题的产生</li></ol> <p>问题1可以通过GPU集群来解决，对于一个企业资源并不是很大的问题；问题2的过拟合通过采集海量数据，并配合Dropout正则化等方法也可以有效避免；问题3通过Batch Normalization也可以避免。貌似我们只要无脑的增加网络的层数，我们就能从此获益，但实验数据给了我们当头一棒。</p> <h1 id="收敛性"><a href="#收敛性" class="header-anchor">#</a> 收敛性</h1> <ol><li><p>数据库太小一般不会带来不收敛的问题，只要你一直在train总会收敛（rp问题跑飞了不算）。<strong>反而不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间</strong>。<strong>样本少只可能带来过拟合的问题</strong>，你看下你的training set上的loss收敛了吗？如果只是validate set上不收敛那就说明overfitting了，这时候就要考虑各种anti-overfit的trick了，比如dropout，SGD，增大minibatch的数量，减少fc层的节点数量，momentum，finetune等。</p></li> <li><p>.learning rate设大了会带来跑飞（loss突然一直很大）的问题，这个是新手最常见的情况——为啥网络跑着跑着看着要收敛了结果突然飞了呢？<strong>可能性最大的原因是你用了relu作为激活函数的同时使用了softmax或者带有exp的函数做分类层的loss函数</strong>。当某一次训练传到最后一层的时候，某一节点激活过度（比如100），那么exp(100)=Inf，发生溢出，bp后所有的weight会变成NAN，然后从此之后weight就会一直保持NAN，于是loss就飞起来啦。在做GNN实验的时候，经常遇到准确率突然下降的情况，自己也发现不了原因，因为准确率一直不错，索性就一直保留着这个为题，如图，可以看到期间一共跑飞过两次，因为学习率设的并不是非常大所以又拉了回来。如果lr设的过大会出现跑飞再也回不来的情况。这时候你停一下随便挑一个层的weights看一看，很有可能都是NAN了。对于这种情况建议用二分法尝试。0.1~0.0001.不同模型不同任务最优的lr都不一样。</p> <p><img src="https://img-blog.csdnimg.cn/20190603225532802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTQ3OTEwOA==,size_16,color_FFFFFF,t_70" alt="https://img-blog.csdnimg.cn/20190603225532802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTQ3OTEwOA==,size_16,color_FFFFFF,t_70"></p></li> <li><p>尽量收集更多的数据。有个方法是爬flickr，找名人标签，然后稍微人工剔除一下就能收集一套不错的样本。其实收集样本不在于多而在于hard，比如你收集了40张基本姿态表情相同的同一个人的图片不如收集他的10张不同表情的图片。之前做过试验，50张variance大的图per person和300多张类似的图per person训练出来的模型后者就比前者高半个点。</p></li> <li><p>尽量用小模型。如果<strong>数据太少尽量缩小模型复杂度</strong>。考虑减少层数或者减少<strong>kernel numbe</strong>r。</p></li></ol> <h1 id="bn层"><a href="#bn层" class="header-anchor">#</a> BN层</h1> <p>为什么提出BN？</p> <p>深度网络在采用Mini-Batch SGD训练的过程中，隐藏层激活函数的输入分布变化大，导致模型收敛慢。</p> <p><a href="https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D%3D%5Cgamma%5Cfrac%7Bx-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E%7B2%7D-%5Cvarepsilon%7D%7D%2B%5Cbeta%5C%5C" target="_blank" rel="noopener noreferrer">https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D%3D%5Cgamma%5Cfrac%7Bx-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E%7B2%7D-%5Cvarepsilon%7D%7D%2B%5Cbeta%5C%5C<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Batch Normalization参数的形状?</strong></p> <p>对feature map的channel方向求均值和方差, 假设feature map.shape=(b,c,w,h)，那么均值和方差的形状为( 1 , c , 1 , 1)，</p> <p>和</p> <p>的形状分别也是( 1 , c , 1 , 1)，因此于一层BN层可学习的参数数量为2c。</p> <p><strong>Batch Normalization的好处</strong></p> <ul><li><strong>解决了Internal Covariate Shift的问题</strong>：前人采用<strong>很小的学习率/非常小心的权重初始化</strong>来解决Internal Covariate Shift的问题，BN解决了Internal Covariate Shift问题之后，就可以采用较大的学习率，能更快收敛</li> <li>BN减轻了梯度消失，梯度爆炸问题：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/ygfrancois/article/details/90382459" target="_blank" rel="noopener noreferrer">详见<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>BN可支持更多的激活函数</li> <li>BN一定程度上增加了泛化能力，dropout等技术可以去掉。</li></ul> <h1 id="resnet"><a href="#resnet" class="header-anchor">#</a> Resnet</h1> <p>一般情况下，模型退化主要有以下几种原因：</p> <ul><li>过拟合，层数越多，参数越复杂，泛化能力弱</li> <li>梯度消失/梯度爆炸，层数过多，梯度反向传播时由于链式求导连乘使得梯度过大或者过小，使得梯度出现消失/爆炸，对于这种情况，可以通过BN(batch normalization)可以解决</li> <li>由深度网络带来的退化问题，一般情况下，网络层数越深越容易学到一些复杂特征，理论上模型效果越好，但是由于深层网络中含有大量非线性变化，每次变化相当于丢失了特征的一些原始信息，从而导致层数越深退化现象越严重。</li></ul> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled.png"></p> <p>残差块的计算方式为：$F ( x ) = W 2 ⋅ r e l u ( W 1 x )$ 
残差块的输出为:    $r e l u ( H ( x ) ) = r e l u ( F ( x ) + x )$</p> <p><strong>残差块误差优化：</strong> 残差网络通过加入 shortcut connections（或称为 skip connections），变得更加容易被优化。在不用skip连接之前，假设输入是x ，最优输出是x，此时的优化目标是预测输出H ( x ) = x ，加入skip连接后，优化输出H ( x ) 与输入x 的差别，即为残差F ( x ) = H ( x ) − x，此时的优化目标是F(x)的输出值为0。后者会比前者更容易优化。
<strong>用残差更容易优化</strong>：引入残差后的映射对输出的变化更敏感。设$H_{1}(x)$是加入skip连接前的网络映射$H_{2}(x)$是加入skip连接的网络映射。对于输入x = 5，设此时$H_{1}(5)=5.1,H_2(x)=5.1$,那么$H_{2}(5)=F(5)+5,F(5)=0.1$。当输出变为5.2时，F(x)由0.1变为0.2，明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化。
简单的加法不会给网络增加额外的参数和计算量，同时可以大大增加模型的训练速度，提高训练效果。并且当模型的层数加深时，能够有效地解决退化问题。
<strong>残差网络为什么是有效的</strong>：对于大型的网络，无论把残差块添加到神经网络的中间还是末端，都不会影响网络的表现。因为可以给残差快中的weight设置很大的L2正则化水平，使得$F(x)=0$，这样使得加入残差块至少不会使得网络变差，此时的残块等价于恒等映射。若此时残差块中的weight学到了有用的信息，那就会比恒等映射更好，对网络的性能有帮助。
总结： ResNet有很多旁路支线可以将输入直接连到后面的层，使得后面的层可以直接学习残差，简化了学习难度。传统的卷积层和全连接层在信息传递时，或多或少会存在信息丢失，损耗等问题。<strong>ResNet将输入信息绕道传到输出，保护了信息的完整性.</strong></p> <h1 id="nn-sequential"><a href="#nn-sequential" class="header-anchor">#</a> nn.Sequential</h1> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># hyper parameters</span>
in_dim<span class="token operator">=</span><span class="token number">1</span>
n_hidden_1<span class="token operator">=</span><span class="token number">1</span>
n_hidden_2<span class="token operator">=</span><span class="token number">1</span>
out_dim<span class="token operator">=</span><span class="token number">1</span>

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> n_hidden_1<span class="token punctuation">,</span> n_hidden_2<span class="token punctuation">,</span> out_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

      	self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> n_hidden_1<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>，
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden_1<span class="token punctuation">,</span> n_hidden_2<span class="token punctuation">)</span>，
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>，
            <span class="token comment"># 最后一层不需要添加激活函数</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden_2<span class="token punctuation">,</span> out_dim<span class="token punctuation">)</span>
             <span class="token punctuation">)</span>

  	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
      	x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
      	<span class="token keyword">return</span> x
<span class="token comment">#其实这个Sequential就是相当于把里面的东西打包了，将网络层和激活函数结合起来。</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><h1 id="激活函数"><a href="#激活函数" class="header-anchor">#</a> 激活函数</h1> <p>激活函数（relu，prelu，elu，+BN）对比on cifar10</p> <p>可参考上一篇：</p> <p><a href="https://www.cnblogs.com/jins-note/p/9646602.html" target="_blank" rel="noopener noreferrer">激活函数 ReLU、LReLU、PReLU、CReLU、ELU、SELU  的定义和区别<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>一．理论基础</p> <p>1.1激活函数</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150404043-1209381965.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150404043-1209381965.png"></p> <p>1.2 elu论文（FAST AND ACCURATE DEEP NETWORK LEARNING BY</p> <p>EXPONENTIAL LINEAR UNITS (ELUS)）</p> <p>1.2.1 摘要</p> <p>论文中提到，elu函数可以加速训练并且可以提高分类的准确率。它有以下特征：</p> <p>1）elu由于其正值特性，可以像relu,lrelu,prelu一样缓解梯度消失的问题。</p> <p>2）相比relu，elu存在负值，可以将激活单元的输出均值往0推近，达到</p> <p>batchnormlization的效果且减少了计算量。（输出均值接近0可以减少偏移效应进而使梯</p> <p>度接近于自然梯度。）</p> <p>3）Lrelu和prelu虽然有负值存在，但是不能确保是一个噪声稳定的去激活状态。</p> <p>4）Elu在负值时是一个指数函数，对于输入特征只定性不定量。</p> <p>1.2.2.bias shift correction speeds up learning</p> <p>为了减少不必要的偏移移位效应，做出如下改变：（i）输入单元的激活可以</p> <p>以零为中心，或（ii）可以使用具有负值的激活函数。 我们介绍一个新的</p> <p>激活函数具有负值，同时保持正参数的特性，即elus。</p> <p>1.2.4实验</p> <p>作者把elu函数用于无监督学习中的autoencoder和有监督学习中的卷积神经网络；</p> <p>elu与relu，lrelu，SReLU做对比实验；数据集选择mnist，cifar10，cifar100.</p> <p>2ALL-CNN for cifar-10</p> <p>2.1结构设计</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150412758-258836552.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150412758-258836552.png"></p> <p>ALL-CNN结构来自论文（STRIVING FOR SIMPLICITY:</p> <p>THE ALL CONVOLUTIONAL NET）主要工作是把pool层用stride=2的卷积来代替，提出了一些全卷积网络架构，kernel=3时效果最好，最合适之类的，比较好懂，同时效果也不错，比原始的cnn效果好又没有用到一些比较大的网络结构如resnet等。</p> <p>附上：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>Lrelu实现：
<span class="token keyword">def</span> <span class="token function">lrelu</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> leak<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">&quot;lrelu&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">return</span> tf<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>x<span class="token punctuation">,</span> leak <span class="token operator">*</span> x<span class="token punctuation">)</span>

Prelu实现：
<span class="token keyword">def</span> <span class="token function">parametric_relu</span><span class="token punctuation">(</span>_x<span class="token punctuation">)</span><span class="token punctuation">:</span>
alphas <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'alpha'</span><span class="token punctuation">,</span> _x<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
dtype <span class="token operator">=</span> tf<span class="token punctuation">.</span>float32
<span class="token punctuation">)</span>
pos <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>_x<span class="token punctuation">)</span>
neg <span class="token operator">=</span> alphas <span class="token operator">*</span> <span class="token punctuation">(</span>_x <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>_x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.5</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>alphas<span class="token punctuation">)</span>
<span class="token keyword">return</span> pos <span class="token operator">+</span> neg

BN实现：
<span class="token keyword">def</span> <span class="token function">batch_norm</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> n_out<span class="token punctuation">,</span>scope<span class="token operator">=</span><span class="token string">'bn'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">&quot;&quot;&quot;
  Batch normalization on convolutional maps.
  Args:
    x: Tensor, 4D BHWD input maps
    n_out: integer, depth of input maps
    phase_train: boolean tf.Variable, true indicates training phase
    scope: string, variable scope

  Return:
    normed: batch-normalized maps
  &quot;&quot;&quot;</span>
  <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>scope<span class="token punctuation">)</span><span class="token punctuation">:</span>
    beta <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>n_out<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      name<span class="token operator">=</span><span class="token string">'beta'</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    gamma <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>n_out<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      name<span class="token operator">=</span><span class="token string">'gamma'</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    tf<span class="token punctuation">.</span>add_to_collection<span class="token punctuation">(</span><span class="token string">'biases'</span><span class="token punctuation">,</span> beta<span class="token punctuation">)</span>
    tf<span class="token punctuation">.</span>add_to_collection<span class="token punctuation">(</span><span class="token string">'weights'</span><span class="token punctuation">,</span> gamma<span class="token punctuation">)</span>

    batch_mean<span class="token punctuation">,</span> batch_var <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>moments<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'moments'</span><span class="token punctuation">)</span>
    ema <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>ExponentialMovingAverage<span class="token punctuation">(</span>decay<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">mean_var_with_update</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      ema_apply_op <span class="token operator">=</span> ema<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token punctuation">[</span>batch_mean<span class="token punctuation">,</span> batch_var<span class="token punctuation">]</span><span class="token punctuation">)</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>control_dependencies<span class="token punctuation">(</span><span class="token punctuation">[</span>ema_apply_op<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token keyword">return</span> tf<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>batch_mean<span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>batch_var<span class="token punctuation">)</span>
    <span class="token comment">#mean, var = control_flow_ops.cond(phase_train,</span>
    <span class="token comment"># mean, var = control_flow_ops.cond(phase_train,</span>
    <span class="token comment">#   mean_var_with_update,</span>
    <span class="token comment">#   lambda: (ema.average(batch_mean), ema.average(batch_var)))</span>
    mean<span class="token punctuation">,</span> var <span class="token operator">=</span> mean_var_with_update<span class="token punctuation">(</span><span class="token punctuation">)</span>
    normed <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>batch_normalization<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mean<span class="token punctuation">,</span> var<span class="token punctuation">,</span>
      beta<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> normed
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br></div></div><p>在cifar10 上测试结果如下：</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150502254-1055081325.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150502254-1055081325.png"></p> <p>以loss所有结果如下：relu+bn&gt;elu&gt;prelu&gt;elubn&gt;relu</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150510382-1875945396.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150510382-1875945396.png"></p> <p>所有的测试准确率如下</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150517165-1710089123.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150517165-1710089123.png"></p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150525985-582258259.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150525985-582258259.png"></p> <p>relu+bn组合准确率最高，relu+bn&gt;elu&gt;prelu&gt;elubn&gt;relu</p> <p>可见elu在激活函数里表现最好，但是它不必加BN，这样减少了BN的计算量。</p> <p>3.ALL-CNN for cifar-100</p> <p>cifar100数据集</p> <p>CIFAR-100 python version,下载完之后解压，在cifar-100-python下会出现：meta,test和train</p> <p>三个文件，他们都是python用cPickle封装的pickled对象</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>解压：tar -zxvf xxx.tar.gz
cifar-100-python/
cifar-100-python/file.txt~
cifar-100-python/train
cifar-100-python/test
cifar-100-python/meta
def unpickle(file):
import cPickle
fo = open(file, ‘rb’)
dict = cPickle.load(fo)
fo.close()
return dict
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>通过以上代码可以将其转换成一个dict对象，test和train的dict中包含以下元素：</p> <p>data——一个nx3072的numpy数组,每一行都是(32,32,3)的RGB图像,n代表图像个数</p> <p>coarse_labels——一个范围在0-19的包含n个元素的列表,对应图像的大类别</p> <p>fine_labels——一个范围在0-99的包含n个元素的列表,对应图像的小类别</p> <p>而meta的dict中只包含fine_label_names,第i个元素对应其真正的类别。</p> <p>二进制版本（我用的）：</p> <p>&lt;1 x coarse label&gt;&lt;1 x fine label&gt;&lt;3072 x pixel&gt;</p> <p>…</p> <p>&lt;1 x coarse label&gt;&lt;1 x fine label&gt;&lt;3072 x pixel&gt;</p> <p>网络结构直接在cifar10的基础上输出100类即可，只对cifar100的精细标签100个进行分类任务，因此代码里取输入数据集第二个值做为标签。（tensorflow的cifar10代码）</p> <p><code>label_bytes =2 # 2 for CIFAR-100 #取第二个标签100维 result.label = tf.cast( tf.strided_slice(record_bytes, [1], [label_bytes]), tf.int32)</code></p> <p>在all CNN 9层上，大约50k步，relu+bn组合测试的cifar100 test error为0.36</p> <p>PS:</p> <p>Activation Function Cheetsheet</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201811/1470684-20181107220712431-1920470308.png" alt="https://img2018.cnblogs.com/blog/1470684/201811/1470684-20181107220712431-1920470308.png"></p> <h1 id="层"><a href="#层" class="header-anchor">#</a> 层</h1> <ol><li>Linear:线性层，最原始的称谓，单层即无隐层。熟悉torch的同学都清楚torch.nn.Linear就是提供了一个in_dim * out_dim的tensor layer而已。</li> <li>Dense：密集层，可以指单层linear也可以指多层堆叠，可无隐层也可有但一般多指后者。熟悉keras的同学也知道dense层其实就是多层线性层的堆叠。(pytorch中的是不是没有，而是Linear？)</li> <li>MLP：多层感知器（Multi-layer perceptron neural networks），指多层linear的堆叠，有隐层。</li> <li>FC：全连接层(fully connected layer)，单层多层均可以表示，是对Linear Classifier最笼统的一种称谓。</li></ol> <h1 id="评价指标"><a href="#评价指标" class="header-anchor">#</a> 评价指标</h1> <p>写文章时候可以选用一下几个
1、均方误差：MSE（Mean Squared Error）
2、均方根误差：RMSE（Root Mean Squard Error）RMSE=sqrt（MSE）。
3、平均绝对误差：MAE（Mean Absolute Error）
4、决定系数：R2（R-Square）
一般来说，R-Squared 越大，表示模型拟合效果越好。R-Squared 反映的是大概有多准，因为，随着样本数量的增加，R-Square必然增加，无法真正定量说明准确程度，只能大概定量。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error<span class="token punctuation">,</span>mean_absolute_error<span class="token punctuation">,</span>r2_score

mse <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span>
rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span><span class="token punctuation">)</span>
mae <span class="token operator">=</span> mean_absolute_error<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span>
r2 <span class="token operator">=</span> r2_score<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>MAPE需要自己编写</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">mape</span><span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">return</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y_true<span class="token punctuation">)</span> <span class="token operator">/</span> y_true<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mape<span class="token punctuation">(</span>testPredict<span class="token punctuation">,</span>testY<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h1 id="损失函数和优化器"><a href="#损失函数和优化器" class="header-anchor">#</a> 损失函数和优化器</h1> <h2 id="损失函数-general"><a href="#损失函数-general" class="header-anchor">#</a> 损失函数（General）</h2> <p>损失函数，又叫目标函数，是编译一个神经网络模型必须的两个参数之一。另一个必不可少的参数是优化器。</p> <p>损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。</p> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%201.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%201.png"></p> <p>上图是一个用来模拟线性方程自动学习的示意图。粗线是真实的线性方程，虚线是迭代过程的示意，w1 是第一次迭代的权重，w2 是第二次迭代的权重，w3 是第三次迭代的权重。随着迭代次数的增加，我们的目标是使得 wn 无限接近真实值。</p> <p>那么怎么让 w 无限接近真实值呢？其实这就是损失函数和优化器的作用了。图中 1/2/3 这三个标签分别是 3 次迭代过程中预测 Y 值和真实 Y 值之间的差值（这里差值就是损失函数的意思了，当然了，实际应用中存在多种差值计算的公式），这里的差值示意图上是用绝对差来表示的，那么在多维空间时还有平方差，均方差等多种不同的距离计算公式，也就是损失函数了，这么一说是不是容易理解了呢？</p> <p>这里示意的是一维度方程的情况，那么发挥一下想象力，扩展到多维度，是不是就是深度学习的本质了？</p> <p>下面介绍几种常见的损失函数的计算方法，pytorch 中定义了很多类型的预定义损失函数，需要用到的时候再学习其公式也不迟。</p> <p>我们先定义两个二维数组，然后用不同的损失函数计算其损失值。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
sample <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">0</span>
a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">1</span>
a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">2</span>
a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span>
target <span class="token operator">=</span> Variable <span class="token punctuation">(</span>a<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>sample 的值为：[[1,1],[1,1]]。</p> <p>target 的值为：[[0,1],[2,3]]。</p> <h3 id="nn-l1loss"><a href="#nn-l1loss" class="header-anchor">#</a> nn.L1Loss</h3> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%202.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%202.png"></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>L1Loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>最后结果是：1。</p> <p>它的计算逻辑是这样的：</p> <ul><li>先计算绝对差总和：|0-1|+|1-1|+|2-1|+|3-1|=4；</li></ul> <h3 id="nn-smoothl1loss"><a href="#nn-smoothl1loss" class="header-anchor">#</a> nn.SmoothL1Loss</h3> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%203.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%203.png"></p> <p>SmoothL1Loss 也叫作 Huber Loss，误差在 (-1,1) 上是平方损失，其他情况是 L1 损失。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>SmoothL1Loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

最后结果是：<span class="token number">0.625</span>。
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h3 id="nn-mseloss"><a href="#nn-mseloss" class="header-anchor">#</a> nn.MSELoss</h3> <p>平方损失函数。其计算公式是预测值和真实值之间的平方和的平均数。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
最后结果是：<span class="token number">1.5</span>。
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="nn-bceloss"><a href="#nn-bceloss" class="header-anchor">#</a> nn.BCELoss</h3> <p>二分类用的交叉熵，其计算公式较复杂，这里主要是有个概念即可，一般情况下不会用到。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
最后结果是：<span class="token operator">-</span><span class="token number">13.8155</span>。
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="nn-crossentropyloss"><a href="#nn-crossentropyloss" class="header-anchor">#</a> nn.CrossEntropyLoss</h3> <p>交叉熵损失函数</p> <p>该公式用的也较多，比如在图像分类神经网络模型中就常常用到该公式。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
最后结果是：报错，看来不能直接这么用！
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>看文档我们知道 nn.CrossEntropyLoss 损失函数是用于图像识别验证的，对输入参数有各式要求，这里有这个概念就可以了，在图像识别一文中会有正确的使用方法。</p> <h3 id="nn-nllloss"><a href="#nn-nllloss" class="header-anchor">#</a> nn.NLLLoss</h3> <p>负对数似然损失函数（Negative Log Likelihood）</p> <p>在前面接上一个 LogSoftMax 层就等价于交叉熵损失了。注意这里的 xlabel 和上个交叉熵损失里的不一样，这里是经过 log 运算后的数值。这个损失函数一般也是用在图像识别模型上。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
loss<span class="token operator">=</span>F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span>sample<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
最后结果会报错！
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>Nn.NLLLoss 和 nn.CrossEntropyLoss 的功能是非常相似的！通常都是用在多分类模型中，实际应用中我们一般用 NLLLoss 比较多。</p> <h3 id="nn-nllloss2d"><a href="#nn-nllloss2d" class="header-anchor">#</a> nn.NLLLoss2d</h3> <p>和上面类似，但是多了几个维度，一般用在图片上。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
target<span class="token punctuation">,</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>比如用全卷积网络做分类时，最后图片的每个点都会预测一个类别标签。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss2d<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
同样结果报错！
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h2 id="损失函数-regression"><a href="#损失函数-regression" class="header-anchor">#</a> 损失函数（Regression）</h2> <p>机器学习中的所有算法都依赖于最小化或最大化函数，我们将其称为“目标函数”。最小化的函数组称为“损失函数”。损失函数是衡量预测模型在能够预测预期结果方面的表现有多好的指标。寻找最小值的最常用方法是“梯度下降”。想想这个函数的作用，如起伏的山脉和梯度下降就像滑下山到达最低点。</p> <p>没有一种损失函数适用于所有类型的数据。它取决于许多因素，包括异常值的存在，机器学习算法的选择，梯度下降的时间效率，易于找到衍生物和预测的置信度。</p> <p>损失函数可大致分为两类：<strong>分类和回归损失</strong>。在这篇文章中，专注于讨论回归损失函数。</p> <p><img src="https://img-blog.csdnimg.cn/20190521011639824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MzY4Mzc3,size_16,color_FFFFFF,t_70" alt="https://img-blog.csdnimg.cn/20190521011639824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4MzY4Mzc3,size_16,color_FFFFFF,t_70"></p> <h3 id="回归损失"><a href="#回归损失" class="header-anchor">#</a> <strong>回归损失</strong></h3> <h3 id="_1-均方误差"><a href="#_1-均方误差" class="header-anchor">#</a> <strong>1.均方误差</strong></h3> <p><a href="https://medium.freecodecamp.org/machine-learning-mean-squared-error-regression-line-c7dde9a26b93" target="_blank" rel="noopener noreferrer">均方误差（MSE）<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>(又称二次损失，L2损失)是最常用的回归损失函数。MSE是目标变量和预测值之间的平方距离之和。M S E = ∑ i = 1 n ( y i − y i p ) 2 n M S E=\frac{\sum_{i=1}^{n}\left(y_{i}-y_{i}^{p}\right)^{2}}{n}<em>MSE</em>=<em>n</em>∑<em>i</em>=1<em>n</em>(<em>yi</em>−<em>yip</em>)2下面是MSE函数的图，其中真实目标值为100，预测值范围在-10,000到10,000之间。MSE损失（Y轴）在预测（X轴）= 100时达到其最小值。其范围是0到∞。</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*EqTaoCB1NmJnsRYEezSACA.png" alt="https://cdn-images-1.medium.com/max/1600/1*EqTaoCB1NmJnsRYEezSACA.png"></p> <p>MSE损失（Y轴）与预测（X轴）的关系图</p> <h3 id="_2-平均绝对误差"><a href="#_2-平均绝对误差" class="header-anchor">#</a> <strong>2. 平均绝对误差</strong></h3> <p><a href="https://medium.com/@ewuramaminka/mean-absolute-error-mae-sample-calculation-6eed6743838a" target="_blank" rel="noopener noreferrer">平均绝对误差<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>（MAE）(又称L1损失）是用于回归模型的另一种损失函数。MAE是我们的目标和预测变量之间的绝对差异的总和。因此，它在不考虑方向的情况下测量一组预测中的平均误差大小。（如果我们也考虑方向，那将被称为平均偏差误差（MBE），它是残差/误差的总和）。其范围也是0到∞。</p> <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f5ef77bc-96f3-441f-948a-90b521eb51c6/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f5ef77bc-96f3-441f-948a-90b521eb51c6/Untitled.png"></p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*8BQhdKu1nk-tAAbOR17qGg.png" alt="https://cdn-images-1.medium.com/max/1600/1*8BQhdKu1nk-tAAbOR17qGg.png"></p> <p>MAE损失（Y轴）与预测（X轴）的关系图</p> <p><strong>MSE与MAE（L2损失与L1损失）</strong></p> <p><strong>简而言之，</strong> <strong>使用平方误差更容易解决，但使用绝对误差对异常值更为稳健。但是让我们明白为什么！</strong></p> <p>每当我们训练机器学习模型时，我们的目标是找到最小化损失函数的点。当然，当预测完全等于真实值时，两个函数都达到最小值。</p> <p>这里是两个python代码的快速回顾。</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np

#true：真实目标变量的数组#prep：预测数组def mse（true，pred）：
    return np.sum（（true  -  pred）** 2）

def mae（true，pred）：
	return np.sum（np.abs（true  -  pred））
12345678910
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>让我们看看MAE和均方根误差的值（RMSE，它只是MSE的平方根，使其与MAE的比例相同）。在第一种情况下，预测接近真实值，并且误差在观察值之间具有小的差异。在第二个，有一个异常值观察，误差很高。</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*KibGRET1M6Bu0-8XmjviMA.png" alt="https://cdn-images-1.medium.com/max/1600/1*KibGRET1M6Bu0-8XmjviMA.png"></p> <p><strong>我们从中观察到了什么，它如何帮助我们选择使用哪种损失函数？</strong></p> <p>由于MSE平方误差（y-y_predicted = e），如果e&gt; 1，则误差（e）的值会增加很多。如果我们的数据中有异常值，则e的值将为高，e²将为&gt;&gt; | E |。这将使具有MSE损失的模型比具有MAE损失的模型对异常值更敏感。在上面的第二种情况中，将调整RMSE作为损失的模型，以便以牺牲其他常见示例为代价来最小化单个异常情况，这将降低其整体性能。</p> <p>如果训练数据被异常值破坏（即我们在训练环境中存在错误较大的正值或负值，而不是我们的测试环境），则<strong>MAE损失很有用</strong>。</p> <p>直观地说，我们可以考虑一下这样的：如果我们只给一个预测为所有尽量减少MSE的意见，那么预测应该是所有目标值的均值。但是，如果我们试图最小化MAE，那么预测将是所有观测的<strong>中位数</strong>。我们知道，中值<a href="https://heartbeat.fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07" target="_blank" rel="noopener noreferrer">对异常值的影响<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>比均值<a href="https://heartbeat.fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07" target="_blank" rel="noopener noreferrer">更强<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>，因此使用MAE对异常值处理效果要比MSE更好。</p> <p><strong>使用MAE损失</strong>（尤其是神经网络）的<strong>一个大问题</strong>是它的梯度始终是相同的，这意味着即使对于小的损耗值，梯度也会很大。这对学习不利。为了解决这个问题，我们可以使用随着我们接近最小值而降低的动态学习率。在这种情况下，MSE表现良好，即使具有固定的学习速率也会收敛。MSE损失的梯度对于较大的损失值是高的，并且随着损失接近0而降低，使其在训练结束时更精确（见下图）。</p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*JTC4ReFwSeAt3kvTLq1YoA.png" alt="https://cdn-images-1.medium.com/max/1600/1*JTC4ReFwSeAt3kvTLq1YoA.png"></p> <p><strong>决定使用哪种损失函数</strong>如果异常值表示对业务很重要且应该检测到的异常，那么我们应该使用MSE。另一方面，如果我们认为异常值只表示损坏的数据，那么我们应该选择MAE作为损失。</p> <p>我建议阅读这篇文章，并进行一项很好的研究，<a href="http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/" target="_blank" rel="noopener noreferrer">比较在<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>有异常值存在和不存在的情况下<a href="http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/" target="_blank" rel="noopener noreferrer">使用L1损失和L2损失的回归模型的性能<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。请记住，L1和L2损失只是MAE和MSE的另一个名称。</p> <blockquote><p>L1损失对异常值更为稳健，但其衍生物不连续，使得找到解决方案效率低下。L2损失对异常值敏感，但提供更稳定和封闭的形式解决方案（通过将其导数设置为0）。</p></blockquote> <ul><li>*两者都有问题：**可能存在损失函数都没有给出理想预测的情况。例如，如果我们数据中90％的观察值具有150的真实目标值，则剩余的10％具有0-30之间的目标值。然后，MAE作为损失的模型可能预测所有观察值为150，忽略10％的离群值情况，因为它将试图达到中值。在相同的情况下，使用MSE的模型会给出0到30范围内的许多预测，因为它会偏向异常值。</li> <li>*在这种情况下该怎么办？**一个简单的解决方法是转换目标变量。另一种方法是尝试不同的损失功能。这是我们的第三次亏损功能背后的动机，Huber损失。</li></ul> <h3 id="_3-huber损失"><a href="#_3-huber损失" class="header-anchor">#</a> <strong>3. Huber损失</strong></h3> <p><a href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank" rel="noopener noreferrer">Huber损失<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>(又称平滑平均绝对误差)对数据中的异常值的敏感性低于平方误差损失。它在0处也是可微分的。它基本上是绝对误差，当误差很小时变为二次曲线。该误差必须多小才能使其成为二次方取决于可以调整的超参数δ（delta）。<strong>当δ0时，*<em>Huber损失接近*<em>MAE，当δ∞（大数）时，</em></em> Huber损耗接近</strong>MSE。**</p> <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c3db79d8-667e-472f-a93a-700e164bdaed/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c3db79d8-667e-472f-a93a-700e164bdaed/Untitled.png"></p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*jxidxadWSMLvwLDZz2mycg.png" alt="https://cdn-images-1.medium.com/max/1600/1*jxidxadWSMLvwLDZz2mycg.png"></p> <p>Hoss损失（Y轴）与预测（X轴）的关系图。</p> <p>δ的选择至关重要，因为它决定了你愿意考虑的异常值。大于δ的残差最小化为L1（对大异常值不敏感），而小于δ的残差最小化为“适当”L2。</p> <ul><li>*为什么要使用Huber Loss？**使用MAE训练神经网络的一个大问题是其持续的大梯度，这可能导致在训练结束时使用梯度下降丢失最小值。对于MSE，随着损失接近其最小值，梯度减小，使其更精确。</li></ul> <p>在这种情况下，胡贝尔损失确实很有用，因为它在最小值附近弯曲，从而降低了梯度。而且它比MSE更强大。因此，它结合了MSE和MAE的良好特性。然而，<strong>Huber损失</strong>的<strong>问题</strong>是我们可能需要训练超参数δ，这是一个迭代过程。</p> <h3 id="_4-log-cosh损失"><a href="#_4-log-cosh损失" class="header-anchor">#</a> <strong>4. Log-Cosh损失</strong></h3> <p>Log-cosh是回归任务中使用的另一个函数，比L2更平滑。Log-cosh是预测误差的双曲余弦的对数。</p> <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ab8c86e0-4951-464c-946d-45084fa2f34e/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ab8c86e0-4951-464c-946d-45084fa2f34e/Untitled.png"></p> <p><img src="https://cdn-images-1.medium.com/max/1600/1*BAbgW_JdwyAWLZR2dE1Ujg.png" alt="https://cdn-images-1.medium.com/max/1600/1*BAbgW_JdwyAWLZR2dE1Ujg.png"></p> <p>Log-cosh Loss（Y轴）与预测（X轴）的关系图。</p> <ul><li>*优点：**当x值较小 时，<code>log(cosh(x))</code>约等于<code>(x ** 2) / 2</code>；当x值较大时，约等于<code>abs(x) - log(2)</code>。这意味着’logcosh’的作用大部分类似于均方误差，但不会受到偶然误差预测的强烈影响。它具有Huber损失的所有优点，并且它在各处都是可区分的，与Huber损失不同。</li> <li>*为什么我们需要二阶导数？**像<a href="https://heartbeat.fritz.ai/boosting-your-machine-learning-models-using-xgboost-d2cabb3e948f" target="_blank" rel="noopener noreferrer">XGBoost<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>这样的许多ML模型实现使用牛顿方法来找到最优，这就是为什么需要二阶导数（Hessian）。对于像XGBoost这样的ML框架，两个可区分的函数更有利。</li></ul> <p><img src="https://cdn-images-1.medium.com/max/1600/1*FNxOsZLqXVZNFOxGoG9A1Q.png" alt="https://cdn-images-1.medium.com/max/1600/1*FNxOsZLqXVZNFOxGoG9A1Q.png"></p> <p>XgBoost中使用的目标函数。注意对1阶和2阶导数的依赖性</p> <p>但Log-cosh损失并不完美。对于非常大的脱靶预测是恒定的，它仍然存在梯度和粗麻布的问题，因此导致没有XGBoost的分裂。</p> <p>Huber和Log-cosh损失函数的Python代码：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code># huber lossdef huber(true, pred, delta):
    loss = np.where(np.abs(true-pred) &lt; delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))
    return np.sum(loss)

# log cosh lossdef logcosh(true, pred):
    loss = np.log(np.cosh(pred - true))
    return np.sum(loss)
123456789
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><h3 id="_5-分位数损失"><a href="#_5-分位数损失" class="header-anchor">#</a> <strong>5.分位数损失</strong></h3> <p>在大多数现实世界预测问题中，我们通常有兴趣了解预测中的不确定性。了解预测范围而不仅仅是点估计可以显着改善许多业务问题的决策过程。</p> <p>当我们对预测区间而不是仅预测点预测感兴趣时，<a href="https://towardsdatascience.com/deep-quantile-regression-c85481548b5a" target="_blank" rel="noopener noreferrer">分位数损失函数<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>变得有用。来自最小二乘回归的预测区间基于以下假设：残差在独立变量的值上具有恒定的方差。我们认为违反这一假设的线性回归模型。我们也不能仅仅通过使用非线性函数或基于树的模型来更好地建模。这种情况来将不能利用线性回归模型来拟合。这是分位数损失和分位数回归将派上用场了，因为基于分位数损失的回归甚至对于具有非恒定方差或非正态分布的残差提供了合理的预测区间。</p> <p>让我们看一个工作示例，以更好地理解为什么基于分位数损失的回归在异方差数据中表现良好。</p> <p><strong>分位数回归与普通最小二乘回归</strong></p> <p><img src="https://cdn-images-1.medium.com/max/800/1*A61Xn0hlPcoMKDns5KFD-A.png" alt="https://cdn-images-1.medium.com/max/800/1*A61Xn0hlPcoMKDns5KFD-A.png"></p> <p>左：具有恒定的残差方差。右：Y的方差随X2增加。</p> <p><img src="https://cdn-images-1.medium.com/max/800/1*h_iOn3gSUa2bk6o0foudDA.png" alt="https://cdn-images-1.medium.com/max/800/1*h_iOn3gSUa2bk6o0foudDA.png"></p> <p>橙色线表示两种情况的OLS估计值</p> <p><img src="https://cdn-images-1.medium.com/max/800/1*hdqrLhTXity54wmfXAtBGw.png" alt="https://cdn-images-1.medium.com/max/800/1*hdqrLhTXity54wmfXAtBGw.png"></p> <p>分位数回归。虚线表示基于回归的0.05和0.95分位数损失函数</p> <p><strong>了解分位数损失函数</strong></p> <p>基于分位数的回归旨在估计给定某些预测变量值的响应变量的条件“分位数”。分位数损失实际上只是MAE的延伸（当分位数为50％时，它是MAE）。</p> <p>我们的想法是根据我们是否希望为正误差或负误差提供更多价值来选择分位数值。损失函数试图基于所选分位数（γ \gamma<em>γ</em>）的值给予过高估计和低估的不同惩罚。例如，γ \gamma<em>γ</em>= 0.25的分位数损失函数给予过高估计更多的惩罚，并试图将预测值保持在中位数以下</p> <p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/907d4fd0-cb2c-4123-8700-627a98c9356a/Untitled.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/907d4fd0-cb2c-4123-8700-627a98c9356a/Untitled.png"></p> <p><em>γ</em>是所需的分位数，其值介于0和1之间。</p> <p><img src="https://cdn-images-1.medium.com/max/800/1*_Msrko0NVv1d43MaVfsZkA.png" alt="https://cdn-images-1.medium.com/max/800/1*_Msrko0NVv1d43MaVfsZkA.png"></p> <p>分位数损失（Y轴）与预测（X轴）的关系图。Y = 0的真值</p> <p>我们还可以使用此损失函数来计算神经网络或基于树的模型中的预测间隔。下面是梯度提升树回归器的Sklearn实现示例。</p> <p><img src="https://cdn-images-1.medium.com/max/800/0*DQ0t4YXq-xLFsWi1.png" alt="https://cdn-images-1.medium.com/max/800/0*DQ0t4YXq-xLFsWi1.png"></p> <p>使用分位数损失预测区间（梯度增强回归）http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html*</p> <p>上图显示了使用sklearn库的GradientBoostingRegression中可用的分位数损失函数计算的90％预测区间。上限构建为γ \gamma<em>γ</em>= 0.95，下限使用γ \gamma<em>γ</em>= 0.05。</p> <h3 id="比较研究"><a href="#比较研究" class="header-anchor">#</a> <strong>比较研究：</strong></h3> <p>“ <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/" target="_blank" rel="noopener noreferrer">Gradient boosting machines，a tutorial<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> ”中提供了一个很好的比较模拟。为了证明上述所有损失函数的性质，他们模拟了一个采用<a href="https://en.wikipedia.org/wiki/Sinc_function" target="_blank" rel="noopener noreferrer">sinc（<em>x</em>）<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>函数采样的数据集，该函数具有两个人工模拟噪声源：高斯噪声分量ε~ <em>N</em>（0，σ2）和脉冲噪声分量ξ~Bern（<em>p</em>）。添加脉冲噪声项以说明鲁棒性效应。以下是使用不同损失函数拟合GBM回归量的结果。</p> <p><img src="https://cdn-images-1.medium.com/max/800/1*46WnlaWhfPZaVWzSZPviIg.png" alt="https://cdn-images-1.medium.com/max/800/1*46WnlaWhfPZaVWzSZPviIg.png"></p> <p>连续损失函数：（A）MSE损失函数; （B）MAE损失函数; （C）Huber损失函数; （D）分位数损失函数演示将平滑的GBM拟合到有噪声的sinc(x)数据（E）原始sinc（<em>x</em>）函数; （F）MSE和MAE损失的光滑GBM; （Huber损失的光滑GBM，δ= {4,2,1}; （H）分位数损失的光滑GBM，α= {0.5,0.1,0.9}。</p> <p><strong>模拟的一些观察结果：</strong></p> <ul><li>具有MAE损失函数的模型的预测受脉冲噪声的影响较小，而具有MSE损失函数的预测由于引起的偏差而略微偏差。</li> <li>对于具有huber损失函数的模型，预测对于选择的超参数值非常敏感。</li> <li>分位数损失函数可以很好地估计相应的置信水平。</li></ul> <p><strong>单个图中的所有损失函数。</strong></p> <p><img src="https://cdn-images-1.medium.com/max/800/1*BploIBOUrhbgdoB1BK_sOg.png" alt="https://cdn-images-1.medium.com/max/800/1*BploIBOUrhbgdoB1BK_sOg.png"></p> <h2 id="优化器optim"><a href="#优化器optim" class="header-anchor">#</a> 优化器Optim</h2> <h2 id="优化器optim-2"><a href="#优化器optim-2" class="header-anchor">#</a> 优化器Optim</h2> <p>所有的优化函数都位于torch.optim包下，常用的优化器有：SGD,Adam,Adadelta,Adagrad,Adamax等，下面就各优化器分析。</p> <h3 id="使用"><a href="#使用" class="header-anchor">#</a> 使用</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>var1<span class="token punctuation">,</span> var2<span class="token punctuation">]</span><span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.0001</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>lr：学习率，大于0的浮点数
momentum:动量参数，大于0的浮点数
parameters：Variable参数，要优化的对象</p> <h3 id="基类-optimizer"><a href="#基类-optimizer" class="header-anchor">#</a> 基类 Optimizer</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">(</span>params<span class="token punctuation">,</span> defaults<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) —— Variable 或者 dict的iterable。指定了什么参数应当被优化。
defaults —— (dict)：包含了优化选项默认值的字典（一个参数组没有指定的参数选项将会使用默认值）。</p> <h3 id="方法"><a href="#方法" class="header-anchor">#</a> 方法：</h3> <ul><li>load_state_dict(state_dict)：加载optimizer状态。</li> <li>state_dict()：以dict返回optimizer的状态。包含两项：state - 一个保存了当前优化状态的dict，param_groups - 一个包含了全部参数组的dict。</li> <li>add_param_group(param_group)：给 optimizer 管理的参数组中增加一组参数，可为该组参数定制 lr,momentum, weight_decay 等，在 finetune 中常用。</li> <li>step(closure) ：进行单次优化 (参数更新)。</li> <li>zero_grad() ：清空所有被优化过的Variable的梯度。</li></ul> <h2 id="优化算法"><a href="#优化算法" class="header-anchor">#</a> 优化算法</h2> <h3 id="随机梯度下降算法-sgd算法"><a href="#随机梯度下降算法-sgd算法" class="header-anchor">#</a> 随机梯度下降算法 SGD算法</h3> <p>SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dampening<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> nesterov<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float) ：学习率
momentum (float, 可选) ：动量因子（默认：0）
weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认：0）
dampening (float, 可选) :动量的抑制因子（默认：0）
nesterov (bool, 可选) :使用Nesterov动量（默认：False）
可实现 SGD 优化算法，带动量 SGD 优化算法，带 NAG(Nesterov accelerated gradient)动量 SGD 优化算法,并且均可拥有 weight_decay 项。</p> <p>对于训练数据集，我们首先将其分成n个batch，每个batch包含m个样本。我们每次更新都利用一个batch的数据，而非整个数据集。这样做使得训练数据太大时，利用整个数据集更新往往时间上不现实。batch的方法可以减少机器的压力，并且可以快速收敛。
当训练集有冗余时，batch方法收敛更快。
优缺点：
SGD完全依赖于当前batch的梯度，所以η可理解为允许当前batch的梯度多大程度影响参数更新。对所有的参数更新使用同样的learning rate，选择合适的learning rate比较困难，容易收敛到局部最优。</p> <h3 id="平均随机梯度下降算法-asgd算法"><a href="#平均随机梯度下降算法-asgd算法" class="header-anchor">#</a> <em>平均随机梯度下降算法 ASGD算法</em></h3> <p>ASGD 就是用空间换时间的一种 SGD。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>ASGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> lambd<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">,</span> t0<span class="token operator">=</span><span class="token number">1000000.0</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) ： 学习率（默认：1e-2）
lambd (float, 可选) ：衰减项（默认：1e-4）
alpha (float, 可选) ：eta更新的指数（默认：0.75）
t0 (float, 可选) ：指明在哪一次开始平均化（默认：1e6）
weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0）</p> <h3 id="adagrad算法"><a href="#adagrad算法" class="header-anchor">#</a> <em>Adagrad算法</em></h3> <p>AdaGrad算法就是将每一个参数的每一次迭代的梯度取平方累加后在开方，用全局学习率除以这个数，作为学习率的动态更新。</p> <p>其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数，为了数值稳定大约设置为10^-7 。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adagrad<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> lr_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) ：学习率（默认: 1e-2）
lr_decay (float, 可选) ：学习率衰减（默认: 0）
weight_decay (float, 可选) ： 权重衰减（L2惩罚）（默认: 0）
优缺点：
Adagrad 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。在深度学习算法中，深度过深会造成训练提早结束。</p> <h3 id="自适应学习率调整-adadelta算法"><a href="#自适应学习率调整-adadelta算法" class="header-anchor">#</a> <em>自适应学习率调整 Adadelta算法</em></h3> <p>Adadelta是对Adagrad的扩展，主要针对三个问题：</p> <p>学习率后期非常小的问题；
手工设置初始学习率；
更新xt时，两边单位不统一
针对以上的三个问题，Adadelta提出新的Adag解决方法。Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adadelta<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> rho<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">06</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
rho (float, 可选) ： 用于计算平方梯度的运行平均值的系数（默认：0.9）
eps (float, 可选)： 为了增加数值计算的稳定性而加到分母里的项（默认：1e-6）
lr (float, 可选)： 在delta被应用到参数更新之前对它缩放的系数（默认：1.0）
weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0）
优缺点：
Adadelta已经不依赖于全局学习率。训练初中期，加速效果不错，很快，训练后期，反复在局部最小值附近抖动。</p> <h3 id="rmsprop算法"><a href="#rmsprop算法" class="header-anchor">#</a> <em>RMSprop算法</em></h3> <p>RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。 RMSprop 采用均方根作为分
母，可缓解 Adagrad 学习率下降较快的问题， 并且引入均方根，可以减少摆动。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>RMSprop<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> centered<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) ：学习率（默认：1e-2）
momentum (float, 可选) : 动量因子（默认：0）
alpha (float, 可选) : 平滑常数（默认：0.99）
eps (float, 可选) : 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
centered (bool, 可选):如果为True，计算中心化的RMSProp，并且用它的方差预测值对梯度进行归一化
weight_decay (float, 可选)：权重衰减（L2惩罚）（默认: 0）</p> <h3 id="自适应矩估计-adam算法"><a href="#自适应矩估计-adam算法" class="header-anchor">#</a> <em>自适应矩估计 Adam算法</em></h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：1e-3）
betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）
eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）
优缺点：
Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
Adam结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点。</p> <ul><li>计算效率高</li> <li>很少的内存需求</li> <li>梯度的对角线重缩放不变（这意味着亚当将梯度乘以仅带正因子的对角矩阵是不变的，以便更好地理解此堆栈交换）</li> <li>非常适合数据和/或参数较大的问题</li> <li>适用于非固定目标</li> <li>适用于非常嘈杂和/或稀疏梯度的问题</li> <li>超参数具有直观的解释，通常需要很少的调整（我们将在配置部分中对此进行详细介绍）</li></ul> <h3 id="adamax算法-adamd的无穷范数变种"><a href="#adamax算法-adamd的无穷范数变种" class="header-anchor">#</a> <em>Adamax算法（Adamd的无穷范数变种）</em></h3> <p>Adamax 是对 Adam 增加了一个学习率上限的概念，所以也称之为 Adamax。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adamax<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.002</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：2e-3）
betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数
eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）
优缺点：</p> <p>Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。
Adamax学习率的边界范围更简单。</p> <h3 id="sparseadam算法"><a href="#sparseadam算法" class="header-anchor">#</a> <em>SparseAdam算法</em></h3> <p>针对稀疏张量的一种“阉割版”Adam 优化方法。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SparseAdam<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：2e-3）
betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数
eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）</p> <h3 id="l-bfgs算法"><a href="#l-bfgs算法" class="header-anchor">#</a> <em>L-BFGS算法</em></h3> <p>L-BFGS 属于拟牛顿算法。 L-BFGS 是对 BFGS 的改进，特点就是节省内存。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>LBFGS<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> max_eval<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
tolerance_grad<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> tolerance_change<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">09</span><span class="token punctuation">,</span> 
history_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> line_search_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>lr (float) – 学习率（默认：1）
max_iter (int) – 每一步优化的最大迭代次数（默认：20）)
max_eval (int) – 每一步优化的最大函数评价次数（默认：max * 1.25）
tolerance_grad (float) – 一阶最优的终止容忍度（默认：1e-5）
tolerance_change (float) – 在函数值/参数变化量上的终止容忍度（默认：1e-9）
history_size (int) – 更新历史的大小（默认：100）</p> <h3 id="弹性反向传播算法-rprop算法"><a href="#弹性反向传播算法-rprop算法" class="header-anchor">#</a> <em>弹性反向传播算法 Rprop算法</em></h3> <p>该优化方法适用于 full-batch，不适用于 mini-batch。不推荐。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Rprop<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> etas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> step_sizes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">06</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：1e-2）
etas (Tuple[float, float], 可选) – 一对（etaminus，etaplis）, 它们分别是乘法的增加和减小的因子（默认：0.5，1.2）
step_sizes (Tuple[float, float], 可选) – 允许的一对最小和最大的步长（默认：1e-6，50）
优缺点：
该优化方法适用于 full-batch，不适用于 mini-batch。</p> <h2 id="测试集优于训练集的原因"><a href="#测试集优于训练集的原因" class="header-anchor">#</a> 测试集优于训练集的原因</h2> <p>（1）<strong>数据集太小的话，如果数据集切分的不均匀，或者说训练集和测试集的分布不均匀</strong>，如果模型能够正确捕捉到数据内部的分布模式话，这可能造成训练集的内部方差大于验证集，会造成训练集的误差更大。这时你要重新切分数据集或者扩充数据集，使其分布一样</p> <p>（2）<strong>由Dropout造成，它能基本上确保您的测试准确性最好，优于您的训练准确性。Dropout迫使你的神经网络成为一个非常大的弱分类器集合，这就意味着，一个单独的分类器没有太高的分类准确性，只有当你把他们串在一起的时候他们才会变得更强大。</strong></p> <p>因为在训练期间，Dropout将这些分类器的随机集合切掉，因此，训练准确率将受到影响。在测试期间，Dropout将自动关闭，并允许使用神经网络中的所有弱分类器，因此，测试精度提高。</p> <h1 id="进度条"><a href="#进度条" class="header-anchor">#</a> 进度条</h1> <p><strong>一、普通进度条</strong></p> <p>示例代码</p> <p><img src="https://pic2.zhimg.com/80/v2-355c0c9a1b87146041838ec7887a7cad_720w.jpg" alt="https://pic2.zhimg.com/80/v2-355c0c9a1b87146041838ec7887a7cad_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic2.zhimg.com/v2-f64b41842668306c5fb1da62cfad94ed_b.jpg" alt="https://pic2.zhimg.com/v2-f64b41842668306c5fb1da62cfad94ed_b.jpg"></p> <p><strong>二、带时间的进度条</strong></p> <p>导入time模块来计算代码运行的时间，加上代码迭代进度使用格式化字符串来输出代码运行进度</p> <p>示例代码</p> <p><img src="https://pic1.zhimg.com/80/v2-b2372d59e028d8a89ba738954a222fc8_720w.jpg" alt="https://pic1.zhimg.com/80/v2-b2372d59e028d8a89ba738954a222fc8_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic1.zhimg.com/v2-00dd65d19beadddad65a0d3711a07218_b.jpg" alt="https://pic1.zhimg.com/v2-00dd65d19beadddad65a0d3711a07218_b.jpg"></p> <p><strong>三、TPDM 进度条</strong></p> <p>这是一个专门生成进度条的工具包，可以使用pip在终端进行下载，当然还能切换进度条风格</p> <p>示例代码</p> <p><img src="https://pic3.zhimg.com/80/v2-b38e3414d5253a0dca08e60ed069d356_720w.jpg" alt="https://pic3.zhimg.com/80/v2-b38e3414d5253a0dca08e60ed069d356_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic3.zhimg.com/v2-42037d2e020ed31268abaa5b10fd0256_b.jpg" alt="https://pic3.zhimg.com/v2-42037d2e020ed31268abaa5b10fd0256_b.jpg"></p> <p><strong>四、progress 进度条</strong></p> <p>只需要定义迭代的次数、进度条类型并在每次迭代时告知进度条即可</p> <p>相关文档：<a href="https://link.zhihu.com/?target=https%3A//pypi.org/project/progress/1.5/" target="_blank" rel="noopener noreferrer">https://pypi.org/project/progress/1.5/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>示例代码</p> <p><img src="https://pic2.zhimg.com/80/v2-9b08de855fbc6f1e0b5f7a066a3712c1_720w.jpg" alt="https://pic2.zhimg.com/80/v2-9b08de855fbc6f1e0b5f7a066a3712c1_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic2.zhimg.com/v2-12eebc070634d4f13e6e4febd208efd9_b.jpg" alt="https://pic2.zhimg.com/v2-12eebc070634d4f13e6e4febd208efd9_b.jpg"></p> <p><strong>五、alive_progress 进度条</strong></p> <p>顾名思义，这个库可以使得进度条变得生动起来，它比原来我们见过的进度条多了一些动画效果，需要使用pip进行下载</p> <p>相关文档：<a href="https://link.zhihu.com/?target=https%3A//github.com/rsalmei/alive-progress" target="_blank" rel="noopener noreferrer">https://github.com/rsalmei/alive-progress<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>示例代码</p> <p><img src="https://pic4.zhimg.com/80/v2-9fde8dbdaaca7120aa07d1deaa4c8483_720w.jpg" alt="https://pic4.zhimg.com/80/v2-9fde8dbdaaca7120aa07d1deaa4c8483_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic2.zhimg.com/v2-ad7829884b8f61051be639d54dc00a01_b.jpg" alt="https://pic2.zhimg.com/v2-ad7829884b8f61051be639d54dc00a01_b.jpg"></p> <p><strong>六、可视化进度条</strong></p> <p>用 PySimpleGUI 得到图形化进度条，我们可以加一行简单的代码，在命令行脚本中得到图形化进度条，也是使用pip进行下载</p> <p>示例代码</p> <p><img src="https://pic3.zhimg.com/80/v2-c0fe7244d948af8ad052137da57e645a_720w.jpg" alt="https://pic3.zhimg.com/80/v2-c0fe7244d948af8ad052137da57e645a_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic3.zhimg.com/v2-2ead8fba626f2d25a58ecd46953950b2_b.jpg" alt="https://pic3.zhimg.com/v2-2ead8fba626f2d25a58ecd46953950b2_b.jpg"></p> <h1 id="小样本"><a href="#小样本" class="header-anchor">#</a> 小样本</h1> <p>机器学习里，模型越复杂、越具有强表达能力越容易牺牲对未来数据的解释能力，而专注于解释训练数据。这种现象会导致训练数据效果非常好，但遇到测试数据效果会大打折扣。这一现象叫<strong>过拟合（overfitting）</strong>。</p> <p>深层神经网络因为其结构，所以具有相较传统模型有很强的表达能力，从而也就需要更多的数据来避免过拟合的发生，以保证训练的模型在新的数据上也能有可以接受的表现。</p> <hr> <p>对于classification model，有这样一个结论:
<img src="https://img-blog.csdn.net/20180503084154538?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pIX1poYWk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述">
上式中N是训练样本数量，η大于等于0小于等于1，h是classification model的VC dimension。具体见wiki：VC dimension。</p> <p>其中的这项：</p> <p><img src="https://img-blog.csdn.net/20180503084238864?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pIX1poYWk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p> <p>也叫model complexity penalty。可以看到，test error小于training error加上model complexity penalty的概率是1-η。如果现在训练模型的算法能使得training error很小，而model complexity penalty又很小，就能保证test error也很小的概率是 1-η。所以要使得模型的generalization比较好，要保证training error和model complexity penalty都能比较小。观察model complexity penalty项，可以看到，h越大，model complexity penalty就会越大。N越大，model complexity penalty则会越小。大致上讲，越复杂的模型有着越大的h（VC dimension），所以为了使得模型有着好的generalization，需要有较大的N来压低model complexity penalty。 这就是为什么深度学习的模型需要大量的数据来训练，否则模型的generalization会比较差，也就是过拟合。</p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">2 years ago</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#损失函数-general" class="sidebar-link reco-side-损失函数-general" data-v-70334359>损失函数（General）</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#nn-l1loss" class="sidebar-link reco-side-nn-l1loss" data-v-70334359>nn.L1Loss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#nn-smoothl1loss" class="sidebar-link reco-side-nn-smoothl1loss" data-v-70334359>nn.SmoothL1Loss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#nn-mseloss" class="sidebar-link reco-side-nn-mseloss" data-v-70334359>nn.MSELoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#nn-bceloss" class="sidebar-link reco-side-nn-bceloss" data-v-70334359>nn.BCELoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#nn-crossentropyloss" class="sidebar-link reco-side-nn-crossentropyloss" data-v-70334359>nn.CrossEntropyLoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#nn-nllloss" class="sidebar-link reco-side-nn-nllloss" data-v-70334359>nn.NLLLoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#nn-nllloss2d" class="sidebar-link reco-side-nn-nllloss2d" data-v-70334359>nn.NLLLoss2d</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#损失函数-regression" class="sidebar-link reco-side-损失函数-regression" data-v-70334359>损失函数（Regression）</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#回归损失" class="sidebar-link reco-side-回归损失" data-v-70334359>回归损失</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#_1-均方误差" class="sidebar-link reco-side-_1-均方误差" data-v-70334359>1.均方误差</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#_2-平均绝对误差" class="sidebar-link reco-side-_2-平均绝对误差" data-v-70334359>2. 平均绝对误差</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#_3-huber损失" class="sidebar-link reco-side-_3-huber损失" data-v-70334359>3. Huber损失</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#_4-log-cosh损失" class="sidebar-link reco-side-_4-log-cosh损失" data-v-70334359>4. Log-Cosh损失</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#_5-分位数损失" class="sidebar-link reco-side-_5-分位数损失" data-v-70334359>5.分位数损失</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#比较研究" class="sidebar-link reco-side-比较研究" data-v-70334359>比较研究：</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#优化器optim" class="sidebar-link reco-side-优化器optim" data-v-70334359>优化器Optim</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#优化器optim-2" class="sidebar-link reco-side-优化器optim-2" data-v-70334359>优化器Optim</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#使用" class="sidebar-link reco-side-使用" data-v-70334359>使用</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#基类-optimizer" class="sidebar-link reco-side-基类-optimizer" data-v-70334359>基类 Optimizer</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#方法" class="sidebar-link reco-side-方法" data-v-70334359>方法：</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#优化算法" class="sidebar-link reco-side-优化算法" data-v-70334359>优化算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#随机梯度下降算法-sgd算法" class="sidebar-link reco-side-随机梯度下降算法-sgd算法" data-v-70334359>随机梯度下降算法 SGD算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#平均随机梯度下降算法-asgd算法" class="sidebar-link reco-side-平均随机梯度下降算法-asgd算法" data-v-70334359>平均随机梯度下降算法 ASGD算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#adagrad算法" class="sidebar-link reco-side-adagrad算法" data-v-70334359>Adagrad算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#自适应学习率调整-adadelta算法" class="sidebar-link reco-side-自适应学习率调整-adadelta算法" data-v-70334359>自适应学习率调整 Adadelta算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#rmsprop算法" class="sidebar-link reco-side-rmsprop算法" data-v-70334359>RMSprop算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#自适应矩估计-adam算法" class="sidebar-link reco-side-自适应矩估计-adam算法" data-v-70334359>自适应矩估计 Adam算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#adamax算法-adamd的无穷范数变种" class="sidebar-link reco-side-adamax算法-adamd的无穷范数变种" data-v-70334359>Adamax算法（Adamd的无穷范数变种）</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#sparseadam算法" class="sidebar-link reco-side-sparseadam算法" data-v-70334359>SparseAdam算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#l-bfgs算法" class="sidebar-link reco-side-l-bfgs算法" data-v-70334359>L-BFGS算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#弹性反向传播算法-rprop算法" class="sidebar-link reco-side-弹性反向传播算法-rprop算法" data-v-70334359>弹性反向传播算法 Rprop算法</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/trash/Basic.html#测试集优于训练集的原因" class="sidebar-link reco-side-测试集优于训练集的原因" data-v-70334359>测试集优于训练集的原因</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><canvas id="vuepress-canvas-cursor"></canvas><!----><div class="vuepress-canvas-nest-element"></div><div class="kanbanniang" data-v-27e9bfa4><div class="banniang-container" style="display:;" data-v-27e9bfa4><div class="messageBox" style="position:fixed;right:75px;bottom:235px;opacity:0.75;height:max-content;width:200px;fon-szie:16px;display:none;" data-v-27e9bfa4></div> <div class="operation" style="display:;" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660425629" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6044" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M577.5584 307.848533l-21.345067-18.699733c-0.062933-0.061867-0.186667-0.123733-0.280533-0.186667l-44.305067-38.689067-53.752533 47.5648L127.009067 587.424l0 177.1392 0 155.0048c0 45.886933 37.454933 83.0976 83.610667 83.0976l183.966933 0L394.586667 735.8688c0-27.512533 22.448-49.8336 50.162133-49.8336l133.7728 0c27.714133 0 50.178133 22.321067 50.178133 49.8336L628.699733 1002.666667l183.966933 0c46.170667 0 83.610667-37.211733 83.610667-83.0976L896.277333 763.9424 896.277333 586.7712 578.5216 308.688 577.5584 307.848533z" p-id="6045" data-v-27e9bfa4></path> <path d="M990.637867 418.164267l-94.360533-82.600533 0-181.290667c0-36.714667-29.952-66.482133-66.894933-66.482133-36.941867 0-66.893867 29.767467-66.893867 66.482133l0 64.197333L556.213333 37.911467c-25.291733-22.103467-63.165867-22.103467-88.4256 0L33.348267 418.164267c-27.730133 24.247467-30.402133 66.264533-5.9808 93.808 24.437333 27.544533 66.692267 30.219733 94.407467 5.938133L512 176.376533l390.2432 341.533867c12.7072 11.130667 28.4608 16.600533 44.181333 16.600533 18.549333 0 37.0048-7.617067 50.209067-22.538667C1021.054933 484.4128 1018.382933 442.4128 990.637867 418.164267z" p-id="6046" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660394444" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5299" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M0 202.7V631c0 83.3 68.3 150.7 152.6 150.7h228.9l8 190.3 224.9-190.3h257c84.3 0 152.6-67.4 152.6-150.7V202.7C1024 119.4 955.7 52 871.4 52H152.6C68.3 52 0 119.4 0 202.7z m658.6 237.9c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S771 512 730.9 512c-40.2 0-72.3-31.7-72.3-71.4z m-220.9 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S550.1 512 510 512c-40.2 0-72.3-31.7-72.3-71.4z m-216.8 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S333.3 512 293.1 512c-40.1 0-72.2-31.7-72.2-71.4z" p-id="5300" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660570409" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2153" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 393.846154c-86.646154 0-157.538462 70.892308-157.538462 157.538461s70.892308 157.538462 157.538462 157.538462 157.538462-70.892308 157.538462-157.538462-70.892308-157.538462-157.538462-157.538461z m393.846154-118.153846h-102.4c-27.569231 0-51.2-13.784615-66.953846-35.446154l-45.292308-68.923077C677.415385 137.846154 643.938462 118.153846 608.492308 118.153846h-192.984616c-35.446154 0-68.923077 19.692308-84.676923 53.169231l-45.292307 68.923077c-13.784615 21.661538-39.384615 35.446154-66.953847 35.446154H118.153846c-43.323077 0-78.769231 35.446154-78.769231 78.76923v472.615385c0 43.323077 35.446154 78.769231 78.769231 78.769231h787.692308c43.323077 0 78.769231-35.446154 78.769231-78.769231V354.461538c0-43.323077-35.446154-78.769231-78.769231-78.76923zM512 787.692308c-129.969231 0-236.307692-106.338462-236.307692-236.307693s106.338462-236.307692 236.307692-236.307692 236.307692 106.338462 236.307692 236.307692-106.338462 236.307692-236.307692 236.307693z" p-id="2154" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660469241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6553" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M706.544835 64.021106h-6.500041a159.889547 159.889547 0 0 0-83.558068 23.557532c-54.583153 33.422204-86.949304 40.439014-104.486726 40.439014-17.538445 0-49.903573-7.016811-104.494912-40.445154a159.88136 159.88136 0 0 0-83.550905-23.551392h-6.507204a127.823224 127.823224 0 0 0-97.182366 44.702108l-172.836417 201.635323c-19.522636 22.774703-20.600177 56.050574-2.609431 80.047104l95.995331 127.994116a63.99757 63.99757 0 0 0 83.198887 17.024745v328.558038c0 52.93256 43.060725 95.995331 95.995331 95.995331h415.98011c52.934606 0 95.995331-43.062771 95.995331-95.995331V535.424502a64.028269 64.028269 0 0 0 42.240033 7.749498 64.013943 64.013943 0 0 0 46.990221-34.528398l63.996546-127.856993c11.522428-23.027459 8.125051-50.721195-8.632611-70.278623L803.743574 108.74675c-24.335245-28.421306-59.770292-44.725644-97.198739-44.725644z" p-id="6554" data-v-27e9bfa4></path></svg></i>
      <a target="_blank" href="https://github.com/kii-chan-iine" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660325062" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3809" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 3.413333c280.849067 0 508.586667 199.273813 508.586667 444.94848 0 140.427947-74.519893 265.53344-190.65856 347.11552V1020.586667l-222.839467-135.168c-30.859947 5.147307-62.552747 8.021333-95.085227 8.021333-280.845653 0-508.586667-199.28064-508.586666-445.078187C3.413333 202.687147 231.150933 3.413333 512 3.413333z m-158.96576 603.921067h317.805227c17.578667 0 31.812267-14.2336 31.812266-31.819093a31.798613 31.798613 0 0 0-31.812266-31.80544h-317.805227c-17.578667 0-31.812267 14.2336-31.812267 31.80544 0.116053 17.585493 14.349653 31.819093 31.812267 31.819093z m-63.511893-190.665387h444.951893c17.578667 0 31.812267-14.2336 31.812267-31.812266a31.802027 31.802027 0 0 0-31.812267-31.81568H289.522347a31.802027 31.802027 0 0 0-31.81568 31.81568c0 17.578667 14.2336 31.812267 31.81568 31.812266z" p-id="3810" data-v-27e9bfa4></path></svg></i></a> <i data-v-27e9bfa4><svg t="1572660347392" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4543" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 34.133333a486.4 486.4 0 1 0 486.4 486.4A486.4 486.4 0 0 0 512 34.133333z m209.4848 632.8064l-55.6032 55.466667-151.517867-151.125333-151.517866 151.1168-55.6032-55.466667 151.517866-151.108267L307.242667 364.714667l55.6032-55.466667 151.517866 151.125333 151.517867-151.1168 55.6032 55.466667-151.517867 151.099733z m0 0" p-id="4544" data-v-27e9bfa4></path></svg></i></div> <canvas id="banniang" width="216" height="281.6" class="live2d" style="position:fixed;right:90px;bottom:-20px;opacity:1;" data-v-27e9bfa4></canvas></div> <div class="showBanNiang" style="display:none;" data-v-27e9bfa4>
    看板娘
  </div></div><!----><div></div></div></div>
    <script src="/assets/js/app.98ee6730.js" defer></script><script src="/assets/js/3.a67abeb3.js" defer></script><script src="/assets/js/1.c373aa88.js" defer></script><script src="/assets/js/34.139d33a3.js" defer></script><script src="/assets/js/11.747f0d2b.js" defer></script>
  </body>
</html>
