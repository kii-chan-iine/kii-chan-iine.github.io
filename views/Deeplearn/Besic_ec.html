<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>深度学习基础知识 | KII IINE</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="manifest" href="/manifest.json">
    <link rel="apple-touch-icon" href="/icons/apple-touch-icon-152x152.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2c2c2c">
    <meta name="description" content="明早一起去看海 望向未来">
    <meta name="theme-color" content="#22979b">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#22979b">
    <meta name="msapplication-TileImage" content="/icons/msapplication-icon-144x144.png">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="google-site-verification" content="XCppppl60fPQTlwxDodwZIhMarkybEgwVpcEz85KTuQ">
    
    <link rel="preload" href="/assets/css/0.styles.6100f0f2.css" as="style"><link rel="preload" href="/assets/js/app.1b8bc834.js" as="script"><link rel="preload" href="/assets/js/3.8dcb9561.js" as="script"><link rel="preload" href="/assets/js/1.4ecd26b1.js" as="script"><link rel="preload" href="/assets/js/16.35af7cf7.js" as="script"><link rel="preload" href="/assets/js/11.00d873ad.js" as="script"><link rel="prefetch" href="/assets/js/10.4a06537f.js"><link rel="prefetch" href="/assets/js/12.f53aed7f.js"><link rel="prefetch" href="/assets/js/13.fd228e98.js"><link rel="prefetch" href="/assets/js/14.a64670fe.js"><link rel="prefetch" href="/assets/js/15.8689dedc.js"><link rel="prefetch" href="/assets/js/17.e379235c.js"><link rel="prefetch" href="/assets/js/18.9385e641.js"><link rel="prefetch" href="/assets/js/19.b6499f0f.js"><link rel="prefetch" href="/assets/js/20.332b1dcc.js"><link rel="prefetch" href="/assets/js/21.ab0efc11.js"><link rel="prefetch" href="/assets/js/22.89f81f85.js"><link rel="prefetch" href="/assets/js/23.c708c3d4.js"><link rel="prefetch" href="/assets/js/24.2a3f3703.js"><link rel="prefetch" href="/assets/js/25.825b07d0.js"><link rel="prefetch" href="/assets/js/26.cfd2b8b9.js"><link rel="prefetch" href="/assets/js/27.b0d2d3d5.js"><link rel="prefetch" href="/assets/js/28.bbf21bad.js"><link rel="prefetch" href="/assets/js/4.7e28a81a.js"><link rel="prefetch" href="/assets/js/5.214381fa.js"><link rel="prefetch" href="/assets/js/6.8a45c9fe.js"><link rel="prefetch" href="/assets/js/7.03f3b098.js"><link rel="prefetch" href="/assets/js/8.11bf26a3.js"><link rel="prefetch" href="/assets/js/9.3de46fbc.js">
    <link rel="stylesheet" href="/assets/css/0.styles.6100f0f2.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>KII IINE</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>明早一起去看海 望向未来</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <!---->
          2021
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/favicon.ico" alt="KII IINE" class="logo"> <span class="site-name">KII IINE</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/avatar.jpeg" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    KII IINE
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>12</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>9</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>深度学习基础知识</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <!---->
          2021
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">深度学习基础知识</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>kii</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>7/20/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>deeplearn</span></i></div></div> <div class="theme-reco-content content__default"><div id="boxx" data-v-f4ca0dac><div data-v-f4ca0dac><p v-if="true" class="custom-block-title" data-v-f4ca0dac></p> <p v-if="true" data-v-f4ca0dac></p></div></div> <div class="custom-block tip"><p class="title">前言</p><p>这里主要讲深度学习的一些基础知识。</p></div> <h1 id="_1-深度学学习的一些知识"><a href="#_1-深度学学习的一些知识" class="header-anchor">#</a> 1 深度学学习的一些知识</h1> <h2 id="bn"><a href="#bn" class="header-anchor">#</a> BN</h2> <p>我们假设有一批图像的feature maps传入网络中（如上）。其中，N表示batch_size，9*9表示图像的大小，5表示channel。</p> <p>BN做了一件什么事呢。</p> <p>（1）把不同batch_size的同一个channel的feature map进行求均值，得到mean</p> <p>（2）把不同batch的同一个channel的feature map进行求标准差，得到std</p> <p>（3）最后对每一个channel的每一个feature map减去对应channel的mean，再除以std，就得到了新的N<em>9</em>9*5的feature maps</p> <p><img src="https://img-blog.csdnimg.cn/20190601170251915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA3ODYxOA==,size_16,color_FFFFFF,t_70" alt="img"></p> <blockquote><p>BN层的的好处：</p> <ol><li>Internal Covariate Shift(本层的输出会导致下一层的输入的分布发生变化，因而导致训练效果变差)</li> <li><a href="https://blog.csdn.net/ygfrancois/article/details/90382459" target="_blank" rel="noopener noreferrer">减轻了梯度消失<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>和梯度爆炸的问题：</li> <li>BN可以支持更多的激活函数</li> <li>BN层一定程度上增加了 泛化能力</li></ol></blockquote> <h2 id="layernormalization-ln"><a href="#layernormalization-ln" class="header-anchor">#</a> LayerNormalization（LN）</h2> <h3 id="理论"><a href="#理论" class="header-anchor">#</a> 理论</h3> <p>好，各位，刚才的任务完成了，我们进行下一项任务，名叫：LN。。。</p> <p>（1）第一梯队的<strong>所有通道的第一列</strong>，听清楚了，是第一列，给到我你们的均值（mean）</p> <p>（2）给完以后，给到我你们的标准差（std）</p> <p>（3）然后：把你们的数值减去mean，再除以std</p> <p>（4）接着我会给你们一个gamma，把结果乘上去；还有一个beta，加上去</p> <p>（5）OK，第一梯队的所有通道的第一列，给我最终结果。</p> <p>（6）接下来，第一梯队的所有通道的其他列，按照第一列的步骤，开始！</p> <p>（7）其他梯队，按照第一梯队的流程，GO！</p> <p><img src="https://img-blog.csdnimg.cn/20190601170311687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA3ODYxOA==,size_16,color_FFFFFF,t_70" alt="img"></p> <h3 id="实际"><a href="#实际" class="header-anchor">#</a> 实际</h3> <p>1）第一梯队的所有通道的第一列的第一行，听清楚了，是<strong>第一列的第一行</strong>，给到我你们的均值（mean）</p> <p>（2）给完以后，给到我你们的标准差（std）</p> <p>（3）然后：把你们的数值减去mean，再除以std</p> <p>（4）接着我会给你们一个gamma，把结果乘上去；还有一个beta，加上去</p> <p>（5）OK，第一梯队的所有通道的第一列的第一行，给我最终结果。</p> <p>（6）接下来，第一梯队的所有通道的第一列的其他行，按照第一列第一行的步骤，开始！</p> <p>（7）接下来，第一梯队的所有通道的其他列，按照第一列步骤，开始！</p> <p>（8）其他梯队，按照第一梯队的流程，GO！</p> <p>这就是（才是实战中的）LN</p> <p><img src="https://img-blog.csdnimg.cn/20190601170337230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA3ODYxOA==,size_16,color_FFFFFF,t_70" alt="img"></p> <h2 id="in"><a href="#in" class="header-anchor">#</a> IN</h2> <p>好，接下来是最后一个任务了，IN</p> <p>（1）第一梯队，给出你们所有通道所有行所有列的均值（mean）</p> <p>（2）第一梯队，给出标准差（std）</p> <p>（3）乘上gamma和beta，在给到我</p> <p>（4）其他梯队，跟上！</p> <p>（5）任务完成，开饭！</p> <p>这就是IN</p> <p><img src="https://img-blog.csdnimg.cn/20190601170351203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA3ODYxOA==,size_16,color_FFFFFF,t_70" alt="img"></p> <h2 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h2> <p>BN做的事情是沿着channel方向，把每个channel的特征图做标准化</p> <p>LN做的事情是沿着batch方向，同时也沿着time_step方向，把每一个单词做标准化</p> <p>IN则很生猛，直接把单个输入特征图直接整体做标准化</p> <p>因此呢，BN对CNN效果很好，因为CNN本身的目的就是结合不同batch_size的特征，做特征提取；LN对单个词做的标准化，对时序特征的效果特别好；IN是对整体单个输入做标准化，所以在风格迁移的时候，能对单一风格做到非常好的特征提取。</p> <p><strong>BN的缺陷</strong></p> <p><strong>缺陷如下：</strong></p> <p>1、BN是在batch size样本上各个维度做标准化的，所以size越大肯定越能得出合理的μ和σ来做标准化，因此BN比较依赖size的大小。
2、在训练的时候，是分批量进行填入模型的，但是在预测的时候，如果只有一个样本或者很少量的样本来做inference，这个时候用BN显然偏差很大，例如在<strong>线学习场景</strong>。
3、RNN是一个动态的网络，也就是size是变化的，可大可小，造成多样本维度都没法对齐，所以不适合用BN。</p> <p><strong>LN带来的优势：</strong></p> <p>1、Layer Normalization是每个样本内部做标准化，跟size没关系，不受其影响。
2、RNN中LN也不受影响，内部自己做标准化，所以LN的应用面更广。</p> <h2 id="_1-激活函数"><a href="#_1-激活函数" class="header-anchor">#</a> 1 激活函数</h2> <p>所谓激活函数（Activation Function），就是在<a href="https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/382460" target="_blank" rel="noopener noreferrer">人工神经网络<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>的神经元上运行的<a href="https://baike.baidu.com/item/%E5%87%BD%E6%95%B0/301912" target="_blank" rel="noopener noreferrer">函数<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>，负责将神经元的输入映射到输出端。</p> <p><img src="https://bkimg.cdn.bcebos.com/pic/0eb30f2442a7d933739ff390a14bd11373f00119?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5/format,f_auto" alt="img"></p> <h3 id="softmax"><a href="#softmax" class="header-anchor">#</a> Softmax</h3> <p>$$
S_i=\frac{e^i}{\sum e^j }
$$</p> <p>映射区间[0,1],主要用于：离散化概率分布。</p> <p>https://blog.csdn.net/bitcarmanlee/article/details/82320853</p> <p>softmax函数，又称归一化指数函数。它<strong>是二分类函数sigmoid在多分类上的推广</strong>，目的是将多分类的结果以概率的形式展现出来。下图展示了softmax的计算方法：</p> <p>softmax就是先把输出用指数表示，保证输出非负，然后在加权平均，获得0-1之间的预测结果概率。具体如下：</p> <p>1）分子：通过指数函数，将实数输出映射到零到正无穷。</p> <p>2）分母：将所有结果相加，进行归一化。</p> <p><img src="https://i.loli.net/2021/07/16/tvy3bHkmUTRO2QP.png" alt="image-20210716155825571"></p> <p>之其中，这里的$W_yx$就是某个的输出结果(softmax之前)</p> <h3 id="sigmoid"><a href="#sigmoid" class="header-anchor">#</a> Sigmoid</h3> <ul><li>映射区间(0, 1)</li> <li>也称logistic函数</li></ul> <p>$$
f(x)=\frac{1}{1+e^{-x}}
$$</p> <p><img src="https://img-blog.csdnimg.cn/20181130114706469.gif" alt="sigmoid函数"></p> <ul><li>映射区间(0, 1)</li> <li>也称logistic函数</li> <li>存在三个<strong>问题</strong>:
<ol><li>饱和的神经元会&quot;杀死&quot;梯度,指离中心点较远的x处的导数接近于0,停止反向传播的学习过程.</li> <li>sigmoid的输出不是以0为中心,而是0.5,这样在求权重w的梯度时,梯度总是正或负的.</li> <li>指数计算耗时</li></ol></li></ul> <h3 id="relu"><a href="#relu" class="header-anchor">#</a> Relu</h3> <img src="https://bkimg.cdn.bcebos.com/pic/d788d43f8794a4c25b5e4dd902f41bd5ac6e39c6?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U5Mg==,g_7,xp_5,yp_5/format,f_auto" alt="img" style="zoom:50%;"> <h3 id="leaky-relus"><a href="#leaky-relus" class="header-anchor">#</a> Leaky ReLUs</h3> <hr> <p>ReLU是将所有的负值都设为零，相反，Leaky ReLU是给所有负值赋予一个非零斜率。Leaky ReLU激活函数是在声学模型（2013）中首次提出的。以数学的方式我们可以表示为：</p> <p><img src="http://p0.ifengimg.com/pmop/2017/0701/CFC5A1C95A84A6D8CF3FFC1DD30597782AEEAE57_size20_w740_h231.jpeg" alt="img" style="zoom:33%;">ai是（1，+∞）区间内的固定参数。</p> <p><img src="http://p0.ifengimg.com/pmop/2017/0701/C56E5C6FCBB36E70BA5EBC90CBD142BA320B3DF6_size19_w740_h217.jpeg" alt="img"></p> <h3 id="prelu"><a href="#prelu" class="header-anchor">#</a> PRelu</h3> <p>负值部分的斜率是根据数据来定的</p> <h3 id="rrelu"><a href="#rrelu" class="header-anchor">#</a> <strong>RReLU</strong></h3> <p><strong>随机纠正线性单元（RReLU）</strong>,训练的时候负数部分的斜率是不固定的。a_ji是从一个均匀的分布U(I,u)中随机抽取的数值</p> <h3 id="elu"><a href="#elu" class="header-anchor">#</a> ELU</h3> <p>ELU函数公式和曲线如下图</p> <p><img src="https://img-blog.csdn.net/20180104121207844?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="elu函数公式"></p> <p><img src="https://img-blog.csdn.net/20180104121237935?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="elu函数图"></p> <p>是有一定的输出的，而且这部分输出还具有一定的抗干扰能力。这样可以消除ReLU死掉的问题，不过还是有梯度饱和和指数运算的问题。<strong>ELU对于输入特征只定性不定量</strong>。</p> <h3 id="selu-就是在elu前面乘以一个-lambda-并告诉你-lambda-alpha-是多少"><a href="#selu-就是在elu前面乘以一个-lambda-并告诉你-lambda-alpha-是多少" class="header-anchor">#</a> SELU(就是在ELU前面乘以一个$\lambda$，并告诉你$\lambda，\alpha$是多少)</h3> <p>上面那个ELU，<img src="https://math.jianshu.com/math?formula=%CE%B1" alt="α">要设多少？后来又出现一种新的方法，叫做：SELU。它相对于ELU做了一个新的变化：就是现在把每一个值的前面都乘上一个<img src="https://math.jianshu.com/math?formula=%CE%BB" alt="λ">，然后他告诉你说<img src="https://math.jianshu.com/math?formula=%CE%BB" alt="λ">跟<img src="https://math.jianshu.com/math?formula=%CE%B1" alt="α">应该设多少，<img src="https://math.jianshu.com/math?formula=%CE%B1%3D1.67326324%E2%80%A6%E2%80%A6" alt="α=1.67326324……">，然后<img src="https://math.jianshu.com/math?formula=%CE%BB%3D1.050700987%E2%80%A6%E2%80%A6" alt="λ=1.050700987……">。</p> <p><img src="https://upload-images.jianshu.io/upload_images/5631876-a163982aad9150ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/404/format/webp" alt="img"></p> <h3 id="gelu"><a href="#gelu" class="header-anchor">#</a> <font color="red">GELU</font></h3> <p>GELU（高斯误差线性单元）是一个非初等函数形式的激活函数，是RELU的变种。由16年论文 <a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener noreferrer">Gaussian Error Linear Units (GELUs)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 提出，随后被GPT-2、BERT、RoBERTa、ALBERT 等NLP模型所采用。</p> <h3 id="全家福"><a href="#全家福" class="header-anchor">#</a> 全家福</h3> <p><img src="https://images2017.cnblogs.com/blog/606386/201711/606386-20171102101447857-1756364198.png" alt="ReLU系列对比"></p> <h3 id="swish"><a href="#swish" class="header-anchor">#</a> <a href="https://arxiv.org/abs/1710.05941" target="_blank" rel="noopener noreferrer">Swish<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h3> <p>还有一个新的激活函数叫做<strong>Swish</strong>。这个<strong>Swish</strong>激活函数长什么样子，它是一个非常神奇的激活函数，他把<strong>sigmoid</strong>乘上<img src="https://math.jianshu.com/math?formula=z" alt="z">得到她的output。</p> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/image-20210716172123869.png" alt="image-20210716172123869"></p> <p>β是个常数或可训练的参数.Swish 具备无上界有下界、平滑、非单调的特性。
Swish 在深层模型上的效果优于 ReLU。</p> <hr> <p>GELU 与 Swish 激活函数（x · σ(βx)）的函数形式和性质非常相像，一个是固定系数 1.702，另一个是可变系数 β（可以是可训练的参数，也可以是通过搜索来确定的常数），两者的实际应用表现也相差不大。</p> <h3 id="tanh"><a href="#tanh" class="header-anchor">#</a> tanh</h3> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/image-20210716172707674.png" alt="image-20210716172707674"></p> <p><img src="https://images2018.cnblogs.com/blog/606386/201807/606386-20180712202915278-1408388561.png" alt="蓝色sigmoid-红色tanh"></p> <h3 id="为什么tanh相比sigmoid收敛更快"><a href="#为什么tanh相比sigmoid收敛更快" class="header-anchor">#</a> 为什么tanh相比sigmoid收敛更快:</h3> <ol><li><p>梯度消失问题程度
$tanh′(x)=1−tanh(x)^2∈(0,1)$</p> <p>sigmoid:$ s′(x)=s(x)×(1−s(x))∈(0,1/4)$
可以看出tanh(x)的<font color="red">梯度消失问题比sigmoid要轻</font>.梯度如果过早消失,收敛速度较慢.</p></li> <li><p><font color="red">以零为中心的影响</font>
如果当前参数(w0,w1)的最佳优化方向是(+d0, -d1),则根据反向传播计算公式,我们希望 x0 和 x1 符号相反。但是如果上一级神经元采用 Sigmoid 函数作为激活函数，sigmoid不以0为中心，输出值恒为正，那么我们无法进行最快的参数更新，而是走 Z 字形逼近最优解。[<a href="https://www.cnblogs.com/makefile/p/activation-function.html#fn4" target="_blank" rel="noopener noreferrer">4]<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li></ol> <h2 id="激活函数的作用"><a href="#激活函数的作用" class="header-anchor">#</a> 激活函数的作用</h2> <ol><li><p>加入非线性因素</p></li> <li><p>充分组合特征</p> <p>**为什么ReLU,Maxout等能够提供网络的非线性建模能力？**它们看起来是分段线性函数，然而并不满足完整的线性要求：加法f(x+y)=f(x)+f(y)和乘法f(ax)=a×f(x)或者写作f(αx1+βx2)=αf(x1)+βf(x2)f(αx1+βx2)=αf(x1)+βf(x2)。非线性意味着得到的输出不可能由输入的线性组合重新得到（重现）。**假如网络中不使用非线性激活函数，那么这个网络可以被一个单层感知器代替得到相同的输出，**因为线性层加起来后还是线性的，可以被另一个线性函数替代。</p></li></ol> <h2 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="header-anchor">#</a> <font color="red">梯度消失与梯度爆炸</font></h2> <p>​	在反向传播过程中需要对激活han函数进行求导，如果导数大于1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加这就是梯度爆炸。同样如果导数小于1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。</p> <p><strong>【<font color="red">梯度消失</font>】<strong>原因有：一是在</strong>深层网络</strong>中，二是采用了<strong>不合适的损失函数</strong>，比如sigmoid。当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习。</p> <p><strong>【<font color="red">梯度爆炸</font>】<strong>一般出现在</strong>深层网络</strong>和<strong>权值初始化值太大</strong>的情况下。在深层神经网络或循环神经网络中，<font color="blue"><strong>误差的梯度可在更新中累积相乘</strong></font>。如果网络层之间的<strong>梯度值大于 1.0</strong>，那么<strong>重复相乘会导致梯度呈指数级增长</strong>，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。</p> <h3 id="原因"><a href="#原因" class="header-anchor">#</a> 原因</h3> <h4 id="深层网络"><a href="#深层网络" class="header-anchor">#</a> 深层网络</h4> <p>深度网络是多层非线性函数的堆砌，整个深度网络可以视为是一个<strong>复合的非线性多元函数</strong>。（这些非线性多元函数其实就是每层的激活函数），那么对loss function求不同层的权值偏导，相当于应用梯度下降的链式法则，链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数传播。</p> <p>如果接近<font color="cyan">输出层</font>的激活函数求导后梯度值大于1，那么层数增多的时候，最终求出的梯度很容易指数级增长，就会产生<strong>梯度爆炸</strong>；相反，如果小于1，那么经过链式法则的连乘形式，也会很容易衰减至0，就会产生<strong>梯度消失</strong>。</p> <p>从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，<font color="cyan">梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足</font>。</p> <h4 id="激活函数"><a href="#激活函数" class="header-anchor">#</a> <strong>激活函数</strong></h4> <p>以下图的反向传播为例（假设每一层只有一个神经元且对于每一层<img src="https://www.zhihu.com/equation?tex=y_i%3D%5Csigma%5Cleft%28z_i%5Cright%29%3D%5Csigma%5Cleft%28w_ix_i%2Bb_i%5Cright%29" alt="[公式]">，其中<img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]">为sigmoid函数）</p> <p><img src="https://pic3.zhimg.com/80/v2-ea9beb6c28c7d4e89be89dc5f4cbae2e_720w.png" alt="img"></p> <p>可以推导出：</p> <img src="https://pic1.zhimg.com/80/v2-8e6665fb67f086c0864583caa48c8d30_720w.jpg" alt="img" style="zoom:67%;"> <p>原因看下图，sigmoid导数的图像。</p> <p><img src="https://pic3.zhimg.com/80/v2-cd452d42a0f5dcad974098dda44c4622_720w.jpg" alt="img"></p> <p>如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，而我们初始化的网络权值<img src="https://www.zhihu.com/equation?tex=%7Cw%7C" alt="[公式]">通常都小于1，因此<img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq%5Cfrac%7B1%7D%7B4%7D" alt="[公式]">，因此对于上面的链式求导，层数越多，求导结果<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_1%7D" alt="[公式]">越小，因而很容易发生梯度消失。</p> <h4 id="初始化权重的值过大"><a href="#初始化权重的值过大" class="header-anchor">#</a> <strong>初始化权重的值过大</strong></h4> <img src="https://pic1.zhimg.com/80/v2-8e6665fb67f086c0864583caa48c8d30_720w.jpg" alt="img" style="zoom:80%;"> <p>如上图所示，当<img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%3E1" alt="[公式]">，也就是<img src="https://www.zhihu.com/equation?tex=w" alt="[公式]">比较大的情况。根据链式相乘(反向传播)可得，则前面的网络层比后面的网络层梯度变化更快，很容易发生梯度爆炸的问题。（再理解下)</p> <h3 id="解决办法"><a href="#解决办法" class="header-anchor">#</a> 解决办法</h3> <p>梯度消失和梯度爆炸本质上是一样的，都是因为网络层数太深而引发的梯度反向传播中的连乘效应。</p> <p>解决梯度消失、爆炸主要有以下几种方案：</p> <h4 id="换用relu、leakyrelu、elu等激活函数-梯度大部分落在常数上"><a href="#换用relu、leakyrelu、elu等激活函数-梯度大部分落在常数上" class="header-anchor">#</a> 换用Relu、LeakyRelu、Elu等激活函数(<strong>梯度大部分落在常数上</strong>)</h4> <p>ReLu：让激活函数的导数为1</p> <p>LeakyReLu：包含了ReLu的几乎所有有点，同时解决了ReLu中0区间带来的影响</p> <p>ELU：和LeakyReLu一样，都是为了解决0区间问题，相对于来，elu计算更耗时一些（为什么）</p> <p>具体可以看<a href="#activation">关于各种激活函数的解析与讨论</a></p> <h4 id="batchnormalization"><a href="#batchnormalization" class="header-anchor">#</a> BatchNormalization</h4> <p>BN本质上是解决传播过程中的梯度问题，具体待补充完善，查看<a href="...">BN</a></p> <h4 id="resnet残差结构"><a href="#resnet残差结构" class="header-anchor">#</a> ResNet残差结构</h4> <p><img src="https://pic4.zhimg.com/80/v2-68f5136f96c6ecce7ccc7b9e9a569f63_720w.jpg" alt="img"></p> <h4 id="lstm结构"><a href="#lstm结构" class="header-anchor">#</a> LSTM结构</h4> <p><strong>STM</strong>全称是长短期记忆网络（long-short term memory networks），LSTM的结构设计可以改善RNN中的梯度消失的问题。主要原因在于LSTM内部复杂的“门”(gates)，如下图所示。</p> <p><img src="https://pic1.zhimg.com/80/v2-2b5e5e1f76374c764d24ae5d70e94288_720w.jpg" alt="img"></p> <p>LSTM 通过它内部的“门”可以在接下来更新的时候“记住”前几次训练的”残留记忆“。</p> <h4 id="预训练加finetunning"><a href="#预训练加finetunning" class="header-anchor">#</a> 预训练加finetunning</h4> <p>此方法来自Hinton在06年发表的论文上，其基本思想是每次训练一层隐藏层节点，将上一层隐藏层的输出作为输入，而本层的输出作为下一层的输入，这就是逐层预训练。</p> <p>训练完成后，再对整个网络进行“微调（fine-tunning）”。</p> <p>目前应用的不是很多了。</p> <p>此方法相当于是找全局最优，然后整合起来寻找全局最优，但是现在<font color="red">基本都是直接拿imagenet的预训练模型直接进行fine-tunning</font>。</p> <h4 id="梯度剪切、正则"><a href="#梯度剪切、正则" class="header-anchor">#</a> 梯度剪切、正则</h4> <p><font color="blue">梯度剪切</font>，其思想是<strong>设值一个剪切阈值，如果更新梯度时，梯度超过了这个阈值，那么就将其强制限制在这个范围之内</strong>。这样可以防止梯度爆炸。</p> <p><font color="blue">另一种防止梯度爆炸的手段是采用权重正则化</font>，正则化主要是通过<strong>对网络权重做正则</strong>来限制过拟合，但是根据正则项在损失函数中的形式：</p> <p>可以看出，如果发生梯度爆炸，那么权值的范数就会变的非常大，反过来，通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。</p> <p>参考：</p> <p>https://zhuanlan.zhihu.com/p/72589432</p> <p>https://www.jianshu.com/p/3f35e555d5ba</p> <p>https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98/22761355?fr=aladdin</p> <p>https://zhuanlan.zhihu.com/p/51490163</p> <h1 id="_2-优化算法"><a href="#_2-优化算法" class="header-anchor">#</a> 2 优化算法</h1> <p>https://blog.csdn.net/qunnie_yi/article/details/80129952</p> <p>https://blog.csdn.net/fengchao03/article/details/78208414</p> <h2 id="adam"><a href="#adam" class="header-anchor">#</a> Adam</h2> <p>https://baijiahao.baidu.com/s?id=1668617930732883837&amp;wfr=spider&amp;for=pc</p> <h1 id="_3-损失函数-cost-fun-损失函数-loss-func"><a href="#_3-损失函数-cost-fun-损失函数-loss-func" class="header-anchor">#</a> 3 损失函数（cost fun）/损失函数 （loss func）</h1> <h2 id="二次代价函数"><a href="#二次代价函数" class="header-anchor">#</a> 二次代价函数</h2> <img src="http://latex.codecogs.com/gif.latex?C=\frac{1}{2n}\sum_x||y(x)-a^L(x)||^2"> <h2 id="交叉熵"><a href="#交叉熵" class="header-anchor">#</a> 交叉熵</h2> <img src="http://latex.codecogs.com/gif.latex?C=-\frac{1}{n}\sum_x[ylna+(1-y)ln(1-a)]"> <p>MSEloss</p> <h1 id="_4-几个网络"><a href="#_4-几个网络" class="header-anchor">#</a> 4 几个网络</h1> <h2 id="resnet"><a href="#resnet" class="header-anchor">#</a> resnet</h2> <p>模型退化原因：</p> <ol><li>过拟合，层数越多，参数越复杂，泛化能力弱</li> <li>梯度消失/梯度爆炸，层数过多，<strong>梯度反向传播时由于链式求导连乘</strong>使得梯度过大或者过小，使得梯度出现消失/爆炸，对于这种情况，可以通过BN(batch normalization)可以解决</li> <li>由深度网络带来的退化问题，一般情况下，网络层数越深越容易学到一些复杂特征，理论上模型效果越好，但是由于深层网络中含有大量非线性变化，每次变化相当于丢失了特征的一些原始信息，从而导致层数越深退化现象越严重。</li></ol> <p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/d15d749d-1836-49e7-84fd-7f35b37e4385/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210716%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20210716T034740Z&amp;X-Amz-Expires=86400&amp;X-Amz-Signature=fe8596bd65a531bf1d958bfab5890439624a052258077ead6270bb16f3dc9d8d&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=filename%20%3D%22Untitled.png%22" alt="img"></p></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#bn" class="sidebar-link reco-side-bn" data-v-70334359>BN</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#layernormalization-ln" class="sidebar-link reco-side-layernormalization-ln" data-v-70334359>LayerNormalization（LN）</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#理论" class="sidebar-link reco-side-理论" data-v-70334359>理论</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#实际" class="sidebar-link reco-side-实际" data-v-70334359>实际</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#in" class="sidebar-link reco-side-in" data-v-70334359>IN</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#总结" class="sidebar-link reco-side-总结" data-v-70334359>总结</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#_1-激活函数" class="sidebar-link reco-side-_1-激活函数" data-v-70334359>1 激活函数</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#softmax" class="sidebar-link reco-side-softmax" data-v-70334359>Softmax</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#sigmoid" class="sidebar-link reco-side-sigmoid" data-v-70334359>Sigmoid</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#relu" class="sidebar-link reco-side-relu" data-v-70334359>Relu</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#leaky-relus" class="sidebar-link reco-side-leaky-relus" data-v-70334359>Leaky ReLUs</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#prelu" class="sidebar-link reco-side-prelu" data-v-70334359>PRelu</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#rrelu" class="sidebar-link reco-side-rrelu" data-v-70334359>RReLU</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#elu" class="sidebar-link reco-side-elu" data-v-70334359>ELU</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#selu-就是在elu前面乘以一个-lambda-并告诉你-lambda-alpha-是多少" class="sidebar-link reco-side-selu-就是在elu前面乘以一个-lambda-并告诉你-lambda-alpha-是多少" data-v-70334359>SELU(就是在ELU前面乘以一个$\lambda$，并告诉你$\lambda，\alpha$是多少)</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#gelu" class="sidebar-link reco-side-gelu" data-v-70334359></a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#全家福" class="sidebar-link reco-side-全家福" data-v-70334359>全家福</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#swish" class="sidebar-link reco-side-swish" data-v-70334359>Swish</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#tanh" class="sidebar-link reco-side-tanh" data-v-70334359>tanh</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#为什么tanh相比sigmoid收敛更快" class="sidebar-link reco-side-为什么tanh相比sigmoid收敛更快" data-v-70334359>为什么tanh相比sigmoid收敛更快:</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#激活函数的作用" class="sidebar-link reco-side-激活函数的作用" data-v-70334359>激活函数的作用</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#梯度消失与梯度爆炸" class="sidebar-link reco-side-梯度消失与梯度爆炸" data-v-70334359></a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#原因" class="sidebar-link reco-side-原因" data-v-70334359>原因</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#解决办法" class="sidebar-link reco-side-解决办法" data-v-70334359>解决办法</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#adam" class="sidebar-link reco-side-adam" data-v-70334359>Adam</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#二次代价函数" class="sidebar-link reco-side-二次代价函数" data-v-70334359>二次代价函数</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#交叉熵" class="sidebar-link reco-side-交叉熵" data-v-70334359>交叉熵</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Besic_ec.html#resnet" class="sidebar-link reco-side-resnet" data-v-70334359>resnet</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><canvas id="vuepress-canvas-cursor"></canvas><!----><div class="vuepress-canvas-nest-element"></div><div class="kanbanniang" data-v-27e9bfa4><div class="banniang-container" style="display:;" data-v-27e9bfa4><div class="messageBox" style="position:fixed;right:75px;bottom:235px;opacity:0.75;height:max-content;width:200px;fon-szie:16px;display:none;" data-v-27e9bfa4></div> <div class="operation" style="display:;" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660425629" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6044" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M577.5584 307.848533l-21.345067-18.699733c-0.062933-0.061867-0.186667-0.123733-0.280533-0.186667l-44.305067-38.689067-53.752533 47.5648L127.009067 587.424l0 177.1392 0 155.0048c0 45.886933 37.454933 83.0976 83.610667 83.0976l183.966933 0L394.586667 735.8688c0-27.512533 22.448-49.8336 50.162133-49.8336l133.7728 0c27.714133 0 50.178133 22.321067 50.178133 49.8336L628.699733 1002.666667l183.966933 0c46.170667 0 83.610667-37.211733 83.610667-83.0976L896.277333 763.9424 896.277333 586.7712 578.5216 308.688 577.5584 307.848533z" p-id="6045" data-v-27e9bfa4></path> <path d="M990.637867 418.164267l-94.360533-82.600533 0-181.290667c0-36.714667-29.952-66.482133-66.894933-66.482133-36.941867 0-66.893867 29.767467-66.893867 66.482133l0 64.197333L556.213333 37.911467c-25.291733-22.103467-63.165867-22.103467-88.4256 0L33.348267 418.164267c-27.730133 24.247467-30.402133 66.264533-5.9808 93.808 24.437333 27.544533 66.692267 30.219733 94.407467 5.938133L512 176.376533l390.2432 341.533867c12.7072 11.130667 28.4608 16.600533 44.181333 16.600533 18.549333 0 37.0048-7.617067 50.209067-22.538667C1021.054933 484.4128 1018.382933 442.4128 990.637867 418.164267z" p-id="6046" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660394444" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5299" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M0 202.7V631c0 83.3 68.3 150.7 152.6 150.7h228.9l8 190.3 224.9-190.3h257c84.3 0 152.6-67.4 152.6-150.7V202.7C1024 119.4 955.7 52 871.4 52H152.6C68.3 52 0 119.4 0 202.7z m658.6 237.9c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S771 512 730.9 512c-40.2 0-72.3-31.7-72.3-71.4z m-220.9 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S550.1 512 510 512c-40.2 0-72.3-31.7-72.3-71.4z m-216.8 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S333.3 512 293.1 512c-40.1 0-72.2-31.7-72.2-71.4z" p-id="5300" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660570409" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2153" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 393.846154c-86.646154 0-157.538462 70.892308-157.538462 157.538461s70.892308 157.538462 157.538462 157.538462 157.538462-70.892308 157.538462-157.538462-70.892308-157.538462-157.538462-157.538461z m393.846154-118.153846h-102.4c-27.569231 0-51.2-13.784615-66.953846-35.446154l-45.292308-68.923077C677.415385 137.846154 643.938462 118.153846 608.492308 118.153846h-192.984616c-35.446154 0-68.923077 19.692308-84.676923 53.169231l-45.292307 68.923077c-13.784615 21.661538-39.384615 35.446154-66.953847 35.446154H118.153846c-43.323077 0-78.769231 35.446154-78.769231 78.76923v472.615385c0 43.323077 35.446154 78.769231 78.769231 78.769231h787.692308c43.323077 0 78.769231-35.446154 78.769231-78.769231V354.461538c0-43.323077-35.446154-78.769231-78.769231-78.76923zM512 787.692308c-129.969231 0-236.307692-106.338462-236.307692-236.307693s106.338462-236.307692 236.307692-236.307692 236.307692 106.338462 236.307692 236.307692-106.338462 236.307692-236.307692 236.307693z" p-id="2154" data-v-27e9bfa4></path></svg></i> <!---->
      <a target="_blank" href="https://github.com/kii-chan-iine" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660325062" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3809" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 3.413333c280.849067 0 508.586667 199.273813 508.586667 444.94848 0 140.427947-74.519893 265.53344-190.65856 347.11552V1020.586667l-222.839467-135.168c-30.859947 5.147307-62.552747 8.021333-95.085227 8.021333-280.845653 0-508.586667-199.28064-508.586666-445.078187C3.413333 202.687147 231.150933 3.413333 512 3.413333z m-158.96576 603.921067h317.805227c17.578667 0 31.812267-14.2336 31.812266-31.819093a31.798613 31.798613 0 0 0-31.812266-31.80544h-317.805227c-17.578667 0-31.812267 14.2336-31.812267 31.80544 0.116053 17.585493 14.349653 31.819093 31.812267 31.819093z m-63.511893-190.665387h444.951893c17.578667 0 31.812267-14.2336 31.812267-31.812266a31.802027 31.802027 0 0 0-31.812267-31.81568H289.522347a31.802027 31.802027 0 0 0-31.81568 31.81568c0 17.578667 14.2336 31.812267 31.81568 31.812266z" p-id="3810" data-v-27e9bfa4></path></svg></i></a> <i data-v-27e9bfa4><svg t="1572660347392" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4543" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 34.133333a486.4 486.4 0 1 0 486.4 486.4A486.4 486.4 0 0 0 512 34.133333z m209.4848 632.8064l-55.6032 55.466667-151.517867-151.125333-151.517866 151.1168-55.6032-55.466667 151.517866-151.108267L307.242667 364.714667l55.6032-55.466667 151.517866 151.125333 151.517867-151.1168 55.6032 55.466667-151.517867 151.099733z m0 0" p-id="4544" data-v-27e9bfa4></path></svg></i></div> <canvas id="banniang" width="216" height="281.6" class="live2d" style="position:fixed;right:90px;bottom:-20px;opacity:1;" data-v-27e9bfa4></canvas></div> <div class="showBanNiang" style="display:none;" data-v-27e9bfa4>
    看板娘
  </div></div><!----><div></div></div></div>
    <script src="/assets/js/app.1b8bc834.js" defer></script><script src="/assets/js/3.8dcb9561.js" defer></script><script src="/assets/js/1.4ecd26b1.js" defer></script><script src="/assets/js/16.35af7cf7.js" defer></script><script src="/assets/js/11.00d873ad.js" defer></script>
  </body>
</html>
