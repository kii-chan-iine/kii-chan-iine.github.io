<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Something for Deeplearn | KII IINE</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="manifest" href="/manifest.json">
    <link rel="apple-touch-icon" href="/icons/apple-touch-icon-152x152.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2c2c2c">
    <meta name="description" content="明早一起去看海 望向未来">
    <meta name="theme-color" content="#22979b">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#22979b">
    <meta name="msapplication-TileImage" content="/icons/msapplication-icon-144x144.png">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="google-site-verification" content="XCppppl60fPQTlwxDodwZIhMarkybEgwVpcEz85KTuQ">
    
    <link rel="preload" href="/assets/css/0.styles.13d379b4.css" as="style"><link rel="preload" href="/assets/js/app.3c565d21.js" as="script"><link rel="preload" href="/assets/js/3.96c41bda.js" as="script"><link rel="preload" href="/assets/js/1.a1c68afd.js" as="script"><link rel="preload" href="/assets/js/15.6906d831.js" as="script"><link rel="preload" href="/assets/js/11.b60f3117.js" as="script"><link rel="prefetch" href="/assets/js/10.cb6acb3b.js"><link rel="prefetch" href="/assets/js/12.5f192381.js"><link rel="prefetch" href="/assets/js/13.b248cd6c.js"><link rel="prefetch" href="/assets/js/14.93d725f3.js"><link rel="prefetch" href="/assets/js/16.51429d16.js"><link rel="prefetch" href="/assets/js/17.9c951efc.js"><link rel="prefetch" href="/assets/js/18.54a48bd7.js"><link rel="prefetch" href="/assets/js/4.670b49a4.js"><link rel="prefetch" href="/assets/js/5.2a9e4b68.js"><link rel="prefetch" href="/assets/js/6.34076dfc.js"><link rel="prefetch" href="/assets/js/7.e7d361eb.js"><link rel="prefetch" href="/assets/js/8.bdafe257.js"><link rel="prefetch" href="/assets/js/9.6279c5d7.js">
    <link rel="stylesheet" href="/assets/css/0.styles.13d379b4.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>KII IINE</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>明早一起去看海 望向未来</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <!---->
          2021
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/favicon.ico" alt="KII IINE" class="logo"> <span class="site-name">KII IINE</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/avatar.jpeg" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    KII IINE
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>2</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>2</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Something for Deeplearn</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <!---->
          2021
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Something for Deeplearn</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>kii</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>6/4/2020</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>deeplearn</span></i></div></div> <div class="theme-reco-content content__default"><div id="boxx" data-v-f4ca0dac><div data-v-f4ca0dac><p v-if="true" class="custom-block-title" data-v-f4ca0dac></p> <p v-if="true" data-v-f4ca0dac></p></div></div> <div class="custom-block tip"><p class="title">前言</p><p>这里主要讲深度学习的一些基础知识。</p></div> <h1 id="深度学习的思考"><a href="#深度学习的思考" class="header-anchor">#</a> 深度学习的思考</h1> <p>在VGG中，卷积网络达到了19层，在GoogLeNet中，网络史无前例的达到了22层。那么，网络的精度会随着网络的层数增多而增多吗？在深度学习中，网络层数增多一般会伴着下面几个问题</p> <ol><li>计算资源的消耗</li> <li>模型容易过拟合</li> <li>梯度消失/梯度爆炸问题的产生</li></ol> <p>问题1可以通过GPU集群来解决，对于一个企业资源并不是很大的问题；问题2的过拟合通过采集海量数据，并配合Dropout正则化等方法也可以有效避免；问题3通过Batch Normalization也可以避免。貌似我们只要无脑的增加网络的层数，我们就能从此获益，但实验数据给了我们当头一棒。</p> <h1 id="收敛性"><a href="#收敛性" class="header-anchor">#</a> 收敛性</h1> <ol><li><p>数据库太小一般不会带来不收敛的问题，只要你一直在train总会收敛（rp问题跑飞了不算）。<strong>反而不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间</strong>。<strong>样本少只可能带来过拟合的问题</strong>，你看下你的training set上的loss收敛了吗？如果只是validate set上不收敛那就说明overfitting了，这时候就要考虑各种anti-overfit的trick了，比如dropout，SGD，增大minibatch的数量，减少fc层的节点数量，momentum，finetune等。</p></li> <li><p>.learning rate设大了会带来跑飞（loss突然一直很大）的问题，这个是新手最常见的情况——为啥网络跑着跑着看着要收敛了结果突然飞了呢？<strong>可能性最大的原因是你用了relu作为激活函数的同时使用了softmax或者带有exp的函数做分类层的loss函数</strong>。当某一次训练传到最后一层的时候，某一节点激活过度（比如100），那么exp(100)=Inf，发生溢出，bp后所有的weight会变成NAN，然后从此之后weight就会一直保持NAN，于是loss就飞起来啦。在做GNN实验的时候，经常遇到准确率突然下降的情况，自己也发现不了原因，因为准确率一直不错，索性就一直保留着这个为题，如图，可以看到期间一共跑飞过两次，因为学习率设的并不是非常大所以又拉了回来。如果lr设的过大会出现跑飞再也回不来的情况。这时候你停一下随便挑一个层的weights看一看，很有可能都是NAN了。对于这种情况建议用二分法尝试。0.1~0.0001.不同模型不同任务最优的lr都不一样。</p> <p><img src="https://img-blog.csdnimg.cn/20190603225532802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTQ3OTEwOA==,size_16,color_FFFFFF,t_70" alt="https://img-blog.csdnimg.cn/20190603225532802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTQ3OTEwOA==,size_16,color_FFFFFF,t_70"></p></li> <li><p>尽量收集更多的数据。有个方法是爬flickr，找名人标签，然后稍微人工剔除一下就能收集一套不错的样本。其实收集样本不在于多而在于hard，比如你收集了40张基本姿态表情相同的同一个人的图片不如收集他的10张不同表情的图片。之前做过试验，50张variance大的图per person和300多张类似的图per person训练出来的模型后者就比前者高半个点。</p></li> <li><p>尽量用小模型。如果<strong>数据太少尽量缩小模型复杂度</strong>。考虑减少层数或者减少<strong>kernel numbe</strong>r。</p></li></ol> <h1 id="bn层"><a href="#bn层" class="header-anchor">#</a> BN层</h1> <p>为什么提出BN？</p> <p>深度网络在采用Mini-Batch SGD训练的过程中，隐藏层激活函数的输入分布变化大，导致模型收敛慢。</p> <p><a href="https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D%3D%5Cgamma%5Cfrac%7Bx-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E%7B2%7D-%5Cvarepsilon%7D%7D%2B%5Cbeta%5C%5C" target="_blank" rel="noopener noreferrer">https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D%3D%5Cgamma%5Cfrac%7Bx-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E%7B2%7D-%5Cvarepsilon%7D%7D%2B%5Cbeta%5C%5C<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Batch Normalization参数的形状?</strong></p> <p>对feature map的channel方向求均值和方差, 假设feature map.shape=(b,c,w,h)，那么均值和方差的形状为( 1 , c , 1 , 1)，</p> <p>和</p> <p>的形状分别也是( 1 , c , 1 , 1)，因此于一层BN层可学习的参数数量为2c。</p> <p><strong>Batch Normalization的好处</strong></p> <ul><li><strong>解决了Internal Covariate Shift的问题</strong>：前人采用<strong>很小的学习率/非常小心的权重初始化</strong>来解决Internal Covariate Shift的问题，BN解决了Internal Covariate Shift问题之后，就可以采用较大的学习率，能更快收敛</li> <li>BN减轻了梯度消失，梯度爆炸问题：<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/ygfrancois/article/details/90382459" target="_blank" rel="noopener noreferrer">详见<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>BN可支持更多的激活函数</li> <li>BN一定程度上增加了泛化能力，dropout等技术可以去掉。</li></ul> <h1 id="resnet"><a href="#resnet" class="header-anchor">#</a> Resnet</h1> <p>一般情况下，模型退化主要有以下几种原因：</p> <ul><li>过拟合，层数越多，参数越复杂，泛化能力弱</li> <li>梯度消失/梯度爆炸，层数过多，梯度反向传播时由于链式求导连乘使得梯度过大或者过小，使得梯度出现消失/爆炸，对于这种情况，可以通过BN(batch normalization)可以解决</li> <li>由深度网络带来的退化问题，一般情况下，网络层数越深越容易学到一些复杂特征，理论上模型效果越好，但是由于深层网络中含有大量非线性变化，每次变化相当于丢失了特征的一些原始信息，从而导致层数越深退化现象越严重。</li></ul> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled.png"></p> <p>残差块的计算方式为：$F ( x ) = W 2 ⋅ r e l u ( W 1 x )$ 
残差块的输出为:    $r e l u ( H ( x ) ) = r e l u ( F ( x ) + x )$</p> <p><strong>残差块误差优化：</strong> 残差网络通过加入 shortcut connections（或称为 skip connections），变得更加容易被优化。在不用skip连接之前，假设输入是x ，最优输出是x，此时的优化目标是预测输出H ( x ) = x ，加入skip连接后，优化输出H ( x ) 与输入x 的差别，即为残差F ( x ) = H ( x ) − x，此时的优化目标是F(x)的输出值为0。后者会比前者更容易优化。
<strong>用残差更容易优化</strong>：引入残差后的映射对输出的变化更敏感。设$H_{1}(x)$是加入skip连接前的网络映射$H_{2}(x)$是加入skip连接的网络映射。对于输入x = 5，设此时$H_{1}(5)=5.1,H_2(x)=5.1$,那么$H_{2}(5)=F(5)+5,F(5)=0.1$。当输出变为5.2时，F(x)由0.1变为0.2，明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化。
简单的加法不会给网络增加额外的参数和计算量，同时可以大大增加模型的训练速度，提高训练效果。并且当模型的层数加深时，能够有效地解决退化问题。
<strong>残差网络为什么是有效的</strong>：对于大型的网络，无论把残差块添加到神经网络的中间还是末端，都不会影响网络的表现。因为可以给残差快中的weight设置很大的L2正则化水平，使得$F(x)=0$，这样使得加入残差块至少不会使得网络变差，此时的残块等价于恒等映射。若此时残差块中的weight学到了有用的信息，那就会比恒等映射更好，对网络的性能有帮助。
总结： ResNet有很多旁路支线可以将输入直接连到后面的层，使得后面的层可以直接学习残差，简化了学习难度。传统的卷积层和全连接层在信息传递时，或多或少会存在信息丢失，损耗等问题。<strong>ResNet将输入信息绕道传到输出，保护了信息的完整性.</strong></p> <h1 id="nn-sequential"><a href="#nn-sequential" class="header-anchor">#</a> nn.Sequential</h1> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># hyper parameters</span>
in_dim<span class="token operator">=</span><span class="token number">1</span>
n_hidden_1<span class="token operator">=</span><span class="token number">1</span>
n_hidden_2<span class="token operator">=</span><span class="token number">1</span>
out_dim<span class="token operator">=</span><span class="token number">1</span>

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">,</span> n_hidden_1<span class="token punctuation">,</span> n_hidden_2<span class="token punctuation">,</span> out_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

      	self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> n_hidden_1<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>，
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden_1<span class="token punctuation">,</span> n_hidden_2<span class="token punctuation">)</span>，
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>，
            <span class="token comment"># 最后一层不需要添加激活函数</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden_2<span class="token punctuation">,</span> out_dim<span class="token punctuation">)</span>
             <span class="token punctuation">)</span>

  	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
      	x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
      	<span class="token keyword">return</span> x
<span class="token comment">#其实这个Sequential就是相当于把里面的东西打包了，将网络层和激活函数结合起来。</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><h1 id="激活函数"><a href="#激活函数" class="header-anchor">#</a> 激活函数</h1> <p>激活函数（relu，prelu，elu，+BN）对比on cifar10</p> <p>可参考上一篇：</p> <p><a href="https://www.cnblogs.com/jins-note/p/9646602.html" target="_blank" rel="noopener noreferrer">激活函数 ReLU、LReLU、PReLU、CReLU、ELU、SELU  的定义和区别<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>一．理论基础</p> <p>1.1激活函数</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150404043-1209381965.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150404043-1209381965.png"></p> <p>1.2 elu论文（FAST AND ACCURATE DEEP NETWORK LEARNING BY</p> <p>EXPONENTIAL LINEAR UNITS (ELUS)）</p> <p>1.2.1 摘要</p> <p>论文中提到，elu函数可以加速训练并且可以提高分类的准确率。它有以下特征：</p> <p>1）elu由于其正值特性，可以像relu,lrelu,prelu一样缓解梯度消失的问题。</p> <p>2）相比relu，elu存在负值，可以将激活单元的输出均值往0推近，达到</p> <p>batchnormlization的效果且减少了计算量。（输出均值接近0可以减少偏移效应进而使梯</p> <p>度接近于自然梯度。）</p> <p>3）Lrelu和prelu虽然有负值存在，但是不能确保是一个噪声稳定的去激活状态。</p> <p>4）Elu在负值时是一个指数函数，对于输入特征只定性不定量。</p> <p>1.2.2.bias shift correction speeds up learning</p> <p>为了减少不必要的偏移移位效应，做出如下改变：（i）输入单元的激活可以</p> <p>以零为中心，或（ii）可以使用具有负值的激活函数。 我们介绍一个新的</p> <p>激活函数具有负值，同时保持正参数的特性，即elus。</p> <p>1.2.4实验</p> <p>作者把elu函数用于无监督学习中的autoencoder和有监督学习中的卷积神经网络；</p> <p>elu与relu，lrelu，SReLU做对比实验；数据集选择mnist，cifar10，cifar100.</p> <p>2ALL-CNN for cifar-10</p> <p>2.1结构设计</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150412758-258836552.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150412758-258836552.png"></p> <p>ALL-CNN结构来自论文（STRIVING FOR SIMPLICITY:</p> <p>THE ALL CONVOLUTIONAL NET）主要工作是把pool层用stride=2的卷积来代替，提出了一些全卷积网络架构，kernel=3时效果最好，最合适之类的，比较好懂，同时效果也不错，比原始的cnn效果好又没有用到一些比较大的网络结构如resnet等。</p> <p>附上：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>Lrelu实现：
<span class="token keyword">def</span> <span class="token function">lrelu</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> leak<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">&quot;lrelu&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">return</span> tf<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>x<span class="token punctuation">,</span> leak <span class="token operator">*</span> x<span class="token punctuation">)</span>

Prelu实现：
<span class="token keyword">def</span> <span class="token function">parametric_relu</span><span class="token punctuation">(</span>_x<span class="token punctuation">)</span><span class="token punctuation">:</span>
alphas <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'alpha'</span><span class="token punctuation">,</span> _x<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.25</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
dtype <span class="token operator">=</span> tf<span class="token punctuation">.</span>float32
<span class="token punctuation">)</span>
pos <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>_x<span class="token punctuation">)</span>
neg <span class="token operator">=</span> alphas <span class="token operator">*</span> <span class="token punctuation">(</span>_x <span class="token operator">-</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>_x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.5</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>alphas<span class="token punctuation">)</span>
<span class="token keyword">return</span> pos <span class="token operator">+</span> neg

BN实现：
<span class="token keyword">def</span> <span class="token function">batch_norm</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> n_out<span class="token punctuation">,</span>scope<span class="token operator">=</span><span class="token string">'bn'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">&quot;&quot;&quot;
  Batch normalization on convolutional maps.
  Args:
    x: Tensor, 4D BHWD input maps
    n_out: integer, depth of input maps
    phase_train: boolean tf.Variable, true indicates training phase
    scope: string, variable scope

  Return:
    normed: batch-normalized maps
  &quot;&quot;&quot;</span>
  <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>scope<span class="token punctuation">)</span><span class="token punctuation">:</span>
    beta <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>n_out<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      name<span class="token operator">=</span><span class="token string">'beta'</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    gamma <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">[</span>n_out<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      name<span class="token operator">=</span><span class="token string">'gamma'</span><span class="token punctuation">,</span> trainable<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    tf<span class="token punctuation">.</span>add_to_collection<span class="token punctuation">(</span><span class="token string">'biases'</span><span class="token punctuation">,</span> beta<span class="token punctuation">)</span>
    tf<span class="token punctuation">.</span>add_to_collection<span class="token punctuation">(</span><span class="token string">'weights'</span><span class="token punctuation">,</span> gamma<span class="token punctuation">)</span>

    batch_mean<span class="token punctuation">,</span> batch_var <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>moments<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'moments'</span><span class="token punctuation">)</span>
    ema <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>ExponentialMovingAverage<span class="token punctuation">(</span>decay<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">mean_var_with_update</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      ema_apply_op <span class="token operator">=</span> ema<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token punctuation">[</span>batch_mean<span class="token punctuation">,</span> batch_var<span class="token punctuation">]</span><span class="token punctuation">)</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>control_dependencies<span class="token punctuation">(</span><span class="token punctuation">[</span>ema_apply_op<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token keyword">return</span> tf<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>batch_mean<span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>batch_var<span class="token punctuation">)</span>
    <span class="token comment">#mean, var = control_flow_ops.cond(phase_train,</span>
    <span class="token comment"># mean, var = control_flow_ops.cond(phase_train,</span>
    <span class="token comment">#   mean_var_with_update,</span>
    <span class="token comment">#   lambda: (ema.average(batch_mean), ema.average(batch_var)))</span>
    mean<span class="token punctuation">,</span> var <span class="token operator">=</span> mean_var_with_update<span class="token punctuation">(</span><span class="token punctuation">)</span>
    normed <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>batch_normalization<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mean<span class="token punctuation">,</span> var<span class="token punctuation">,</span>
      beta<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> normed
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br></div></div><p>在cifar10 上测试结果如下：</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150502254-1055081325.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150502254-1055081325.png"></p> <p>以loss所有结果如下：relu+bn&gt;elu&gt;prelu&gt;elubn&gt;relu</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150510382-1875945396.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150510382-1875945396.png"></p> <p>所有的测试准确率如下</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150517165-1710089123.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150517165-1710089123.png"></p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150525985-582258259.png" alt="https://img2018.cnblogs.com/blog/1470684/201809/1470684-20180914150525985-582258259.png"></p> <p>relu+bn组合准确率最高，relu+bn&gt;elu&gt;prelu&gt;elubn&gt;relu</p> <p>可见elu在激活函数里表现最好，但是它不必加BN，这样减少了BN的计算量。</p> <p>3.ALL-CNN for cifar-100</p> <p>cifar100数据集</p> <p>CIFAR-100 python version,下载完之后解压，在cifar-100-python下会出现：meta,test和train</p> <p>三个文件，他们都是python用cPickle封装的pickled对象</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>解压：tar -zxvf xxx.tar.gz
cifar-100-python/
cifar-100-python/file.txt~
cifar-100-python/train
cifar-100-python/test
cifar-100-python/meta
def unpickle(file):
import cPickle
fo = open(file, ‘rb’)
dict = cPickle.load(fo)
fo.close()
return dict
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>通过以上代码可以将其转换成一个dict对象，test和train的dict中包含以下元素：</p> <p>data——一个nx3072的numpy数组,每一行都是(32,32,3)的RGB图像,n代表图像个数</p> <p>coarse_labels——一个范围在0-19的包含n个元素的列表,对应图像的大类别</p> <p>fine_labels——一个范围在0-99的包含n个元素的列表,对应图像的小类别</p> <p>而meta的dict中只包含fine_label_names,第i个元素对应其真正的类别。</p> <p>二进制版本（我用的）：</p> <p>&lt;1 x coarse label&gt;&lt;1 x fine label&gt;&lt;3072 x pixel&gt;</p> <p>…</p> <p>&lt;1 x coarse label&gt;&lt;1 x fine label&gt;&lt;3072 x pixel&gt;</p> <p>网络结构直接在cifar10的基础上输出100类即可，只对cifar100的精细标签100个进行分类任务，因此代码里取输入数据集第二个值做为标签。（tensorflow的cifar10代码）</p> <p><code>label_bytes =2 # 2 for CIFAR-100 #取第二个标签100维 result.label = tf.cast( tf.strided_slice(record_bytes, [1], [label_bytes]), tf.int32)</code></p> <p>在all CNN 9层上，大约50k步，relu+bn组合测试的cifar100 test error为0.36</p> <p>PS:</p> <p>Activation Function Cheetsheet</p> <p><img src="https://img2018.cnblogs.com/blog/1470684/201811/1470684-20181107220712431-1920470308.png" alt="https://img2018.cnblogs.com/blog/1470684/201811/1470684-20181107220712431-1920470308.png"></p> <h1 id="层"><a href="#层" class="header-anchor">#</a> 层</h1> <ol><li>Linear:线性层，最原始的称谓，单层即无隐层。熟悉torch的同学都清楚torch.nn.Linear就是提供了一个in_dim * out_dim的tensor layer而已。</li> <li>Dense：密集层，可以指单层linear也可以指多层堆叠，可无隐层也可有但一般多指后者。熟悉keras的同学也知道dense层其实就是多层线性层的堆叠。(pytorch中的是不是没有，而是Linear？)</li> <li>MLP：多层感知器（Multi-layer perceptron neural networks），指多层linear的堆叠，有隐层。</li> <li>FC：全连接层(fully connected layer)，单层多层均可以表示，是对Linear Classifier最笼统的一种称谓。</li></ol> <h1 id="评价指标"><a href="#评价指标" class="header-anchor">#</a> 评价指标</h1> <p>写文章时候可以选用一下几个
1、均方误差：MSE（Mean Squared Error）
2、均方根误差：RMSE（Root Mean Squard Error）RMSE=sqrt（MSE）。
3、平均绝对误差：MAE（Mean Absolute Error）
4、决定系数：R2（R-Square）
一般来说，R-Squared 越大，表示模型拟合效果越好。R-Squared 反映的是大概有多准，因为，随着样本数量的增加，R-Square必然增加，无法真正定量说明准确程度，只能大概定量。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error<span class="token punctuation">,</span>mean_absolute_error<span class="token punctuation">,</span>r2_score

mse <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span>
rmse <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span><span class="token punctuation">)</span>
mae <span class="token operator">=</span> mean_absolute_error<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span>
r2 <span class="token operator">=</span> r2_score<span class="token punctuation">(</span>testY<span class="token punctuation">,</span>testPredict<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>MAPE需要自己编写</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">mape</span><span class="token punctuation">(</span>y_true<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token keyword">return</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y_true<span class="token punctuation">)</span> <span class="token operator">/</span> y_true<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mape<span class="token punctuation">(</span>testPredict<span class="token punctuation">,</span>testY<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h1 id="损失函数和优化器"><a href="#损失函数和优化器" class="header-anchor">#</a> 损失函数和优化器</h1> <h2 id="损失函数"><a href="#损失函数" class="header-anchor">#</a> 损失函数</h2> <p>损失函数，又叫目标函数，是编译一个神经网络模型必须的两个参数之一。另一个必不可少的参数是优化器。</p> <p>损失函数是指用于计算标签值和预测值之间差异的函数，在机器学习过程中，有多种损失函数可供选择，典型的有距离向量，绝对值向量等。</p> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%201.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%201.png"></p> <p>上图是一个用来模拟线性方程自动学习的示意图。粗线是真实的线性方程，虚线是迭代过程的示意，w1 是第一次迭代的权重，w2 是第二次迭代的权重，w3 是第三次迭代的权重。随着迭代次数的增加，我们的目标是使得 wn 无限接近真实值。</p> <p>那么怎么让 w 无限接近真实值呢？其实这就是损失函数和优化器的作用了。图中 1/2/3 这三个标签分别是 3 次迭代过程中预测 Y 值和真实 Y 值之间的差值（这里差值就是损失函数的意思了，当然了，实际应用中存在多种差值计算的公式），这里的差值示意图上是用绝对差来表示的，那么在多维空间时还有平方差，均方差等多种不同的距离计算公式，也就是损失函数了，这么一说是不是容易理解了呢？</p> <p>这里示意的是一维度方程的情况，那么发挥一下想象力，扩展到多维度，是不是就是深度学习的本质了？</p> <p>下面介绍几种常见的损失函数的计算方法，pytorch 中定义了很多类型的预定义损失函数，需要用到的时候再学习其公式也不迟。</p> <p>我们先定义两个二维数组，然后用不同的损失函数计算其损失值。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
sample <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">0</span>
a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">1</span>
a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">2</span>
a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">3</span>
target <span class="token operator">=</span> Variable <span class="token punctuation">(</span>a<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>sample 的值为：[[1,1],[1,1]]。</p> <p>target 的值为：[[0,1],[2,3]]。</p> <h3 id="nn-l1loss"><a href="#nn-l1loss" class="header-anchor">#</a> nn.L1Loss</h3> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%202.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%202.png"></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>L1Loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>最后结果是：1。</p> <p>它的计算逻辑是这样的：</p> <ul><li>先计算绝对差总和：|0-1|+|1-1|+|2-1|+|3-1|=4；</li></ul> <h3 id="nn-smoothl1loss"><a href="#nn-smoothl1loss" class="header-anchor">#</a> nn.SmoothL1Loss</h3> <p><img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%203.png" alt="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%20b7cca7604a46493bb8be335c2b5b92d0/Untitled%203.png"></p> <p>SmoothL1Loss 也叫作 Huber Loss，误差在 (-1,1) 上是平方损失，其他情况是 L1 损失。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>SmoothL1Loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

最后结果是：<span class="token number">0.625</span>。
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h3 id="nn-mseloss"><a href="#nn-mseloss" class="header-anchor">#</a> nn.MSELoss</h3> <p>平方损失函数。其计算公式是预测值和真实值之间的平方和的平均数。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
最后结果是：<span class="token number">1.5</span>。
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="nn-bceloss"><a href="#nn-bceloss" class="header-anchor">#</a> nn.BCELoss</h3> <p>二分类用的交叉熵，其计算公式较复杂，这里主要是有个概念即可，一般情况下不会用到。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
最后结果是：<span class="token operator">-</span><span class="token number">13.8155</span>。
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="nn-crossentropyloss"><a href="#nn-crossentropyloss" class="header-anchor">#</a> nn.CrossEntropyLoss</h3> <p>交叉熵损失函数</p> <p>该公式用的也较多，比如在图像分类神经网络模型中就常常用到该公式。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
最后结果是：报错，看来不能直接这么用！
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>看文档我们知道 nn.CrossEntropyLoss 损失函数是用于图像识别验证的，对输入参数有各式要求，这里有这个概念就可以了，在图像识别一文中会有正确的使用方法。</p> <h3 id="nn-nllloss"><a href="#nn-nllloss" class="header-anchor">#</a> nn.NLLLoss</h3> <p>负对数似然损失函数（Negative Log Likelihood）</p> <p>在前面接上一个 LogSoftMax 层就等价于交叉熵损失了。注意这里的 xlabel 和上个交叉熵损失里的不一样，这里是经过 log 运算后的数值。这个损失函数一般也是用在图像识别模型上。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
loss<span class="token operator">=</span>F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span>sample<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
最后结果会报错！
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>Nn.NLLLoss 和 nn.CrossEntropyLoss 的功能是非常相似的！通常都是用在多分类模型中，实际应用中我们一般用 NLLLoss 比较多。</p> <h3 id="nn-nllloss2d"><a href="#nn-nllloss2d" class="header-anchor">#</a> nn.NLLLoss2d</h3> <p>和上面类似，但是多了几个维度，一般用在图片上。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
target<span class="token punctuation">,</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>比如用全卷积网络做分类时，最后图片的每个点都会预测一个类别标签。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss2d<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>sample<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
同样结果报错！
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h2 id="优化器optim"><a href="#优化器optim" class="header-anchor">#</a> 优化器Optim</h2> <p>所有的优化函数都位于torch.optim包下，常用的优化器有：SGD,Adam,Adadelta,Adagrad,Adamax等，下面就各优化器分析。</p> <h3 id="使用"><a href="#使用" class="header-anchor">#</a> 使用</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>var1<span class="token punctuation">,</span> var2<span class="token punctuation">]</span><span class="token punctuation">,</span> lr <span class="token operator">=</span> <span class="token number">0.0001</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>lr：学习率，大于0的浮点数
momentum:动量参数，大于0的浮点数
parameters：Variable参数，要优化的对象</p> <h3 id="基类-optimizer"><a href="#基类-optimizer" class="header-anchor">#</a> 基类 Optimizer</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">(</span>params<span class="token punctuation">,</span> defaults<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) —— Variable 或者 dict的iterable。指定了什么参数应当被优化。
defaults —— (dict)：包含了优化选项默认值的字典（一个参数组没有指定的参数选项将会使用默认值）。</p> <h3 id="方法"><a href="#方法" class="header-anchor">#</a> 方法：</h3> <ul><li>load_state_dict(state_dict)：加载optimizer状态。</li> <li>state_dict()：以dict返回optimizer的状态。包含两项：state - 一个保存了当前优化状态的dict，param_groups - 一个包含了全部参数组的dict。</li> <li>add_param_group(param_group)：给 optimizer 管理的参数组中增加一组参数，可为该组参数定制 lr,momentum, weight_decay 等，在 finetune 中常用。</li> <li>step(closure) ：进行单次优化 (参数更新)。</li> <li>zero_grad() ：清空所有被优化过的Variable的梯度。</li></ul> <h2 id="优化算法"><a href="#优化算法" class="header-anchor">#</a> 优化算法</h2> <h3 id="随机梯度下降算法-sgd算法"><a href="#随机梯度下降算法-sgd算法" class="header-anchor">#</a> 随机梯度下降算法 SGD算法</h3> <p>SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dampening<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> nesterov<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float) ：学习率
momentum (float, 可选) ：动量因子（默认：0）
weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认：0）
dampening (float, 可选) :动量的抑制因子（默认：0）
nesterov (bool, 可选) :使用Nesterov动量（默认：False）
可实现 SGD 优化算法，带动量 SGD 优化算法，带 NAG(Nesterov accelerated gradient)动量 SGD 优化算法,并且均可拥有 weight_decay 项。</p> <p>对于训练数据集，我们首先将其分成n个batch，每个batch包含m个样本。我们每次更新都利用一个batch的数据，而非整个数据集。这样做使得训练数据太大时，利用整个数据集更新往往时间上不现实。batch的方法可以减少机器的压力，并且可以快速收敛。
当训练集有冗余时，batch方法收敛更快。
优缺点：
SGD完全依赖于当前batch的梯度，所以η可理解为允许当前batch的梯度多大程度影响参数更新。对所有的参数更新使用同样的learning rate，选择合适的learning rate比较困难，容易收敛到局部最优。</p> <h3 id="平均随机梯度下降算法-asgd算法"><a href="#平均随机梯度下降算法-asgd算法" class="header-anchor">#</a> <em>平均随机梯度下降算法 ASGD算法</em></h3> <p>ASGD 就是用空间换时间的一种 SGD。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>ASGD<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> lambd<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">,</span> t0<span class="token operator">=</span><span class="token number">1000000.0</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) ： 学习率（默认：1e-2）
lambd (float, 可选) ：衰减项（默认：1e-4）
alpha (float, 可选) ：eta更新的指数（默认：0.75）
t0 (float, 可选) ：指明在哪一次开始平均化（默认：1e6）
weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0）</p> <h3 id="adagrad算法"><a href="#adagrad算法" class="header-anchor">#</a> <em>Adagrad算法</em></h3> <p>AdaGrad算法就是将每一个参数的每一次迭代的梯度取平方累加后在开方，用全局学习率除以这个数，作为学习率的动态更新。</p> <p>其中，r为梯度累积变量，r的初始值为0。ε为全局学习率，需要自己设置。δ为小常数，为了数值稳定大约设置为10^-7 。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adagrad<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> lr_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) ：学习率（默认: 1e-2）
lr_decay (float, 可选) ：学习率衰减（默认: 0）
weight_decay (float, 可选) ： 权重衰减（L2惩罚）（默认: 0）
优缺点：
Adagrad 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。在深度学习算法中，深度过深会造成训练提早结束。</p> <h3 id="自适应学习率调整-adadelta算法"><a href="#自适应学习率调整-adadelta算法" class="header-anchor">#</a> <em>自适应学习率调整 Adadelta算法</em></h3> <p>Adadelta是对Adagrad的扩展，主要针对三个问题：</p> <p>学习率后期非常小的问题；
手工设置初始学习率；
更新xt时，两边单位不统一
针对以上的三个问题，Adadelta提出新的Adag解决方法。Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adadelta<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> rho<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">06</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
rho (float, 可选) ： 用于计算平方梯度的运行平均值的系数（默认：0.9）
eps (float, 可选)： 为了增加数值计算的稳定性而加到分母里的项（默认：1e-6）
lr (float, 可选)： 在delta被应用到参数更新之前对它缩放的系数（默认：1.0）
weight_decay (float, 可选) ：权重衰减（L2惩罚）（默认: 0）
优缺点：
Adadelta已经不依赖于全局学习率。训练初中期，加速效果不错，很快，训练后期，反复在局部最小值附近抖动。</p> <h3 id="rmsprop算法"><a href="#rmsprop算法" class="header-anchor">#</a> <em>RMSprop算法</em></h3> <p>RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。 RMSprop 采用均方根作为分
母，可缓解 Adagrad 学习率下降较快的问题， 并且引入均方根，可以减少摆动。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>RMSprop<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> centered<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) ：待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) ：学习率（默认：1e-2）
momentum (float, 可选) : 动量因子（默认：0）
alpha (float, 可选) : 平滑常数（默认：0.99）
eps (float, 可选) : 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
centered (bool, 可选):如果为True，计算中心化的RMSProp，并且用它的方差预测值对梯度进行归一化
weight_decay (float, 可选)：权重衰减（L2惩罚）（默认: 0）</p> <h3 id="自适应矩估计-adam算法"><a href="#自适应矩估计-adam算法" class="header-anchor">#</a> <em>自适应矩估计 Adam算法</em></h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：1e-3）
betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）
eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）
优缺点：
Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
Adam结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点。</p> <ul><li>计算效率高</li> <li>很少的内存需求</li> <li>梯度的对角线重缩放不变（这意味着亚当将梯度乘以仅带正因子的对角矩阵是不变的，以便更好地理解此堆栈交换）</li> <li>非常适合数据和/或参数较大的问题</li> <li>适用于非固定目标</li> <li>适用于非常嘈杂和/或稀疏梯度的问题</li> <li>超参数具有直观的解释，通常需要很少的调整（我们将在配置部分中对此进行详细介绍）</li></ul> <h3 id="adamax算法-adamd的无穷范数变种"><a href="#adamax算法-adamd的无穷范数变种" class="header-anchor">#</a> <em>Adamax算法（Adamd的无穷范数变种）</em></h3> <p>Adamax 是对 Adam 增加了一个学习率上限的概念，所以也称之为 Adamax。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adamax<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.002</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：2e-3）
betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数
eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）
weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）
优缺点：</p> <p>Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。
Adamax学习率的边界范围更简单。</p> <h3 id="sparseadam算法"><a href="#sparseadam算法" class="header-anchor">#</a> <em>SparseAdam算法</em></h3> <p>针对稀疏张量的一种“阉割版”Adam 优化方法。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SparseAdam<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：2e-3）
betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数
eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）</p> <h3 id="l-bfgs算法"><a href="#l-bfgs算法" class="header-anchor">#</a> <em>L-BFGS算法</em></h3> <p>L-BFGS 属于拟牛顿算法。 L-BFGS 是对 BFGS 的改进，特点就是节省内存。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>LBFGS<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> max_eval<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
tolerance_grad<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">05</span><span class="token punctuation">,</span> tolerance_change<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">09</span><span class="token punctuation">,</span> 
history_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> line_search_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>lr (float) – 学习率（默认：1）
max_iter (int) – 每一步优化的最大迭代次数（默认：20）)
max_eval (int) – 每一步优化的最大函数评价次数（默认：max * 1.25）
tolerance_grad (float) – 一阶最优的终止容忍度（默认：1e-5）
tolerance_change (float) – 在函数值/参数变化量上的终止容忍度（默认：1e-9）
history_size (int) – 更新历史的大小（默认：100）</p> <h3 id="弹性反向传播算法-rprop算法"><a href="#弹性反向传播算法-rprop算法" class="header-anchor">#</a> <em>弹性反向传播算法 Rprop算法</em></h3> <p>该优化方法适用于 full-batch，不适用于 mini-batch。不推荐。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Rprop<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> etas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> step_sizes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">06</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict
lr (float, 可选) – 学习率（默认：1e-2）
etas (Tuple[float, float], 可选) – 一对（etaminus，etaplis）, 它们分别是乘法的增加和减小的因子（默认：0.5，1.2）
step_sizes (Tuple[float, float], 可选) – 允许的一对最小和最大的步长（默认：1e-6，50）
优缺点：
该优化方法适用于 full-batch，不适用于 mini-batch。</p> <h2 id="测试集优于训练集的原因"><a href="#测试集优于训练集的原因" class="header-anchor">#</a> 测试集优于训练集的原因</h2> <p>（1）<strong>数据集太小的话，如果数据集切分的不均匀，或者说训练集和测试集的分布不均匀</strong>，如果模型能够正确捕捉到数据内部的分布模式话，这可能造成训练集的内部方差大于验证集，会造成训练集的误差更大。这时你要重新切分数据集或者扩充数据集，使其分布一样</p> <p>（2）<strong>由Dropout造成，它能基本上确保您的测试准确性最好，优于您的训练准确性。Dropout迫使你的神经网络成为一个非常大的弱分类器集合，这就意味着，一个单独的分类器没有太高的分类准确性，只有当你把他们串在一起的时候他们才会变得更强大。</strong></p> <p>因为在训练期间，Dropout将这些分类器的随机集合切掉，因此，训练准确率将受到影响。在测试期间，Dropout将自动关闭，并允许使用神经网络中的所有弱分类器，因此，测试精度提高。</p> <h1 id="进度条"><a href="#进度条" class="header-anchor">#</a> 进度条</h1> <p><strong>一、普通进度条</strong></p> <p>示例代码</p> <p><img src="https://pic2.zhimg.com/80/v2-355c0c9a1b87146041838ec7887a7cad_720w.jpg" alt="https://pic2.zhimg.com/80/v2-355c0c9a1b87146041838ec7887a7cad_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic2.zhimg.com/v2-f64b41842668306c5fb1da62cfad94ed_b.jpg" alt="https://pic2.zhimg.com/v2-f64b41842668306c5fb1da62cfad94ed_b.jpg"></p> <p><strong>二、带时间的进度条</strong></p> <p>导入time模块来计算代码运行的时间，加上代码迭代进度使用格式化字符串来输出代码运行进度</p> <p>示例代码</p> <p><img src="https://pic1.zhimg.com/80/v2-b2372d59e028d8a89ba738954a222fc8_720w.jpg" alt="https://pic1.zhimg.com/80/v2-b2372d59e028d8a89ba738954a222fc8_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic1.zhimg.com/v2-00dd65d19beadddad65a0d3711a07218_b.jpg" alt="https://pic1.zhimg.com/v2-00dd65d19beadddad65a0d3711a07218_b.jpg"></p> <p><strong>三、TPDM 进度条</strong></p> <p>这是一个专门生成进度条的工具包，可以使用pip在终端进行下载，当然还能切换进度条风格</p> <p>示例代码</p> <p><img src="https://pic3.zhimg.com/80/v2-b38e3414d5253a0dca08e60ed069d356_720w.jpg" alt="https://pic3.zhimg.com/80/v2-b38e3414d5253a0dca08e60ed069d356_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic3.zhimg.com/v2-42037d2e020ed31268abaa5b10fd0256_b.jpg" alt="https://pic3.zhimg.com/v2-42037d2e020ed31268abaa5b10fd0256_b.jpg"></p> <p><strong>四、progress 进度条</strong></p> <p>只需要定义迭代的次数、进度条类型并在每次迭代时告知进度条即可</p> <p>相关文档：<a href="https://link.zhihu.com/?target=https%3A//pypi.org/project/progress/1.5/" target="_blank" rel="noopener noreferrer">https://pypi.org/project/progress/1.5/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>示例代码</p> <p><img src="https://pic2.zhimg.com/80/v2-9b08de855fbc6f1e0b5f7a066a3712c1_720w.jpg" alt="https://pic2.zhimg.com/80/v2-9b08de855fbc6f1e0b5f7a066a3712c1_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic2.zhimg.com/v2-12eebc070634d4f13e6e4febd208efd9_b.jpg" alt="https://pic2.zhimg.com/v2-12eebc070634d4f13e6e4febd208efd9_b.jpg"></p> <p><strong>五、alive_progress 进度条</strong></p> <p>顾名思义，这个库可以使得进度条变得生动起来，它比原来我们见过的进度条多了一些动画效果，需要使用pip进行下载</p> <p>相关文档：<a href="https://link.zhihu.com/?target=https%3A//github.com/rsalmei/alive-progress" target="_blank" rel="noopener noreferrer">https://github.com/rsalmei/alive-progress<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>示例代码</p> <p><img src="https://pic4.zhimg.com/80/v2-9fde8dbdaaca7120aa07d1deaa4c8483_720w.jpg" alt="https://pic4.zhimg.com/80/v2-9fde8dbdaaca7120aa07d1deaa4c8483_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic2.zhimg.com/v2-ad7829884b8f61051be639d54dc00a01_b.jpg" alt="https://pic2.zhimg.com/v2-ad7829884b8f61051be639d54dc00a01_b.jpg"></p> <p><strong>六、可视化进度条</strong></p> <p>用 PySimpleGUI 得到图形化进度条，我们可以加一行简单的代码，在命令行脚本中得到图形化进度条，也是使用pip进行下载</p> <p>示例代码</p> <p><img src="https://pic3.zhimg.com/80/v2-c0fe7244d948af8ad052137da57e645a_720w.jpg" alt="https://pic3.zhimg.com/80/v2-c0fe7244d948af8ad052137da57e645a_720w.jpg"></p> <p>展现形式</p> <p><img src="https://pic3.zhimg.com/v2-2ead8fba626f2d25a58ecd46953950b2_b.jpg" alt="https://pic3.zhimg.com/v2-2ead8fba626f2d25a58ecd46953950b2_b.jpg"></p> <h1 id=""><a href="#" class="header-anchor">#</a></h1></div></section> <footer class="page-edit"><!----> <!----></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Basic.html#损失函数" class="sidebar-link reco-side-损失函数" data-v-70334359>损失函数</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#nn-l1loss" class="sidebar-link reco-side-nn-l1loss" data-v-70334359>nn.L1Loss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#nn-smoothl1loss" class="sidebar-link reco-side-nn-smoothl1loss" data-v-70334359>nn.SmoothL1Loss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#nn-mseloss" class="sidebar-link reco-side-nn-mseloss" data-v-70334359>nn.MSELoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#nn-bceloss" class="sidebar-link reco-side-nn-bceloss" data-v-70334359>nn.BCELoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#nn-crossentropyloss" class="sidebar-link reco-side-nn-crossentropyloss" data-v-70334359>nn.CrossEntropyLoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#nn-nllloss" class="sidebar-link reco-side-nn-nllloss" data-v-70334359>nn.NLLLoss</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#nn-nllloss2d" class="sidebar-link reco-side-nn-nllloss2d" data-v-70334359>nn.NLLLoss2d</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Basic.html#优化器optim" class="sidebar-link reco-side-优化器optim" data-v-70334359>优化器Optim</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#使用" class="sidebar-link reco-side-使用" data-v-70334359>使用</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#基类-optimizer" class="sidebar-link reco-side-基类-optimizer" data-v-70334359>基类 Optimizer</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#方法" class="sidebar-link reco-side-方法" data-v-70334359>方法：</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Basic.html#优化算法" class="sidebar-link reco-side-优化算法" data-v-70334359>优化算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#随机梯度下降算法-sgd算法" class="sidebar-link reco-side-随机梯度下降算法-sgd算法" data-v-70334359>随机梯度下降算法 SGD算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#平均随机梯度下降算法-asgd算法" class="sidebar-link reco-side-平均随机梯度下降算法-asgd算法" data-v-70334359>平均随机梯度下降算法 ASGD算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#adagrad算法" class="sidebar-link reco-side-adagrad算法" data-v-70334359>Adagrad算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#自适应学习率调整-adadelta算法" class="sidebar-link reco-side-自适应学习率调整-adadelta算法" data-v-70334359>自适应学习率调整 Adadelta算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#rmsprop算法" class="sidebar-link reco-side-rmsprop算法" data-v-70334359>RMSprop算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#自适应矩估计-adam算法" class="sidebar-link reco-side-自适应矩估计-adam算法" data-v-70334359>自适应矩估计 Adam算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#adamax算法-adamd的无穷范数变种" class="sidebar-link reco-side-adamax算法-adamd的无穷范数变种" data-v-70334359>Adamax算法（Adamd的无穷范数变种）</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#sparseadam算法" class="sidebar-link reco-side-sparseadam算法" data-v-70334359>SparseAdam算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#l-bfgs算法" class="sidebar-link reco-side-l-bfgs算法" data-v-70334359>L-BFGS算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Basic.html#弹性反向传播算法-rprop算法" class="sidebar-link reco-side-弹性反向传播算法-rprop算法" data-v-70334359>弹性反向传播算法 Rprop算法</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Basic.html#测试集优于训练集的原因" class="sidebar-link reco-side-测试集优于训练集的原因" data-v-70334359>测试集优于训练集的原因</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><canvas id="vuepress-canvas-cursor"></canvas><!----><div class="vuepress-canvas-nest-element"></div><div class="kanbanniang" data-v-ec964632><div class="banniang-container" style="display:;" data-v-ec964632><div class="messageBox" style="position:fixed;right:75px;bottom:235px;opacity:0.75;height:max-content;width:200px;fon-szie:16px;display:none;" data-v-ec964632></div> <div class="operation" style="display:;" data-v-ec964632><i data-v-ec964632><svg t="1572660425629" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6044" width="16" height="16" class="icon" data-v-ec964632><path d="M577.5584 307.848533l-21.345067-18.699733c-0.062933-0.061867-0.186667-0.123733-0.280533-0.186667l-44.305067-38.689067-53.752533 47.5648L127.009067 587.424l0 177.1392 0 155.0048c0 45.886933 37.454933 83.0976 83.610667 83.0976l183.966933 0L394.586667 735.8688c0-27.512533 22.448-49.8336 50.162133-49.8336l133.7728 0c27.714133 0 50.178133 22.321067 50.178133 49.8336L628.699733 1002.666667l183.966933 0c46.170667 0 83.610667-37.211733 83.610667-83.0976L896.277333 763.9424 896.277333 586.7712 578.5216 308.688 577.5584 307.848533z" p-id="6045" data-v-ec964632></path> <path d="M990.637867 418.164267l-94.360533-82.600533 0-181.290667c0-36.714667-29.952-66.482133-66.894933-66.482133-36.941867 0-66.893867 29.767467-66.893867 66.482133l0 64.197333L556.213333 37.911467c-25.291733-22.103467-63.165867-22.103467-88.4256 0L33.348267 418.164267c-27.730133 24.247467-30.402133 66.264533-5.9808 93.808 24.437333 27.544533 66.692267 30.219733 94.407467 5.938133L512 176.376533l390.2432 341.533867c12.7072 11.130667 28.4608 16.600533 44.181333 16.600533 18.549333 0 37.0048-7.617067 50.209067-22.538667C1021.054933 484.4128 1018.382933 442.4128 990.637867 418.164267z" p-id="6046" data-v-ec964632></path></svg></i> <i data-v-ec964632><svg t="1572660394444" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5299" width="16" height="16" class="icon" data-v-ec964632><path d="M0 202.7V631c0 83.3 68.3 150.7 152.6 150.7h228.9l8 190.3 224.9-190.3h257c84.3 0 152.6-67.4 152.6-150.7V202.7C1024 119.4 955.7 52 871.4 52H152.6C68.3 52 0 119.4 0 202.7z m658.6 237.9c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S771 512 730.9 512c-40.2 0-72.3-31.7-72.3-71.4z m-220.9 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S550.1 512 510 512c-40.2 0-72.3-31.7-72.3-71.4z m-216.8 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S333.3 512 293.1 512c-40.1 0-72.2-31.7-72.2-71.4z" p-id="5300" data-v-ec964632></path></svg></i> <i data-v-ec964632><svg t="1572660570409" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2153" width="16" height="16" class="icon" data-v-ec964632><path d="M512 393.846154c-86.646154 0-157.538462 70.892308-157.538462 157.538461s70.892308 157.538462 157.538462 157.538462 157.538462-70.892308 157.538462-157.538462-70.892308-157.538462-157.538462-157.538461z m393.846154-118.153846h-102.4c-27.569231 0-51.2-13.784615-66.953846-35.446154l-45.292308-68.923077C677.415385 137.846154 643.938462 118.153846 608.492308 118.153846h-192.984616c-35.446154 0-68.923077 19.692308-84.676923 53.169231l-45.292307 68.923077c-13.784615 21.661538-39.384615 35.446154-66.953847 35.446154H118.153846c-43.323077 0-78.769231 35.446154-78.769231 78.76923v472.615385c0 43.323077 35.446154 78.769231 78.769231 78.769231h787.692308c43.323077 0 78.769231-35.446154 78.769231-78.769231V354.461538c0-43.323077-35.446154-78.769231-78.769231-78.76923zM512 787.692308c-129.969231 0-236.307692-106.338462-236.307692-236.307693s106.338462-236.307692 236.307692-236.307692 236.307692 106.338462 236.307692 236.307692-106.338462 236.307692-236.307692 236.307693z" p-id="2154" data-v-ec964632></path></svg></i> <!---->
      <a target="_blank" href="https://github.com/vxhly" data-v-ec964632><i data-v-ec964632><svg t="1572660325062" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3809" width="16" height="16" class="icon" data-v-ec964632><path d="M512 3.413333c280.849067 0 508.586667 199.273813 508.586667 444.94848 0 140.427947-74.519893 265.53344-190.65856 347.11552V1020.586667l-222.839467-135.168c-30.859947 5.147307-62.552747 8.021333-95.085227 8.021333-280.845653 0-508.586667-199.28064-508.586666-445.078187C3.413333 202.687147 231.150933 3.413333 512 3.413333z m-158.96576 603.921067h317.805227c17.578667 0 31.812267-14.2336 31.812266-31.819093a31.798613 31.798613 0 0 0-31.812266-31.80544h-317.805227c-17.578667 0-31.812267 14.2336-31.812267 31.80544 0.116053 17.585493 14.349653 31.819093 31.812267 31.819093z m-63.511893-190.665387h444.951893c17.578667 0 31.812267-14.2336 31.812267-31.812266a31.802027 31.802027 0 0 0-31.812267-31.81568H289.522347a31.802027 31.802027 0 0 0-31.81568 31.81568c0 17.578667 14.2336 31.812267 31.81568 31.812266z" p-id="3810" data-v-ec964632></path></svg></i></a> <i data-v-ec964632><svg t="1572660347392" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4543" width="16" height="16" class="icon" data-v-ec964632><path d="M512 34.133333a486.4 486.4 0 1 0 486.4 486.4A486.4 486.4 0 0 0 512 34.133333z m209.4848 632.8064l-55.6032 55.466667-151.517867-151.125333-151.517866 151.1168-55.6032-55.466667 151.517866-151.108267L307.242667 364.714667l55.6032-55.466667 151.517866 151.125333 151.517867-151.1168 55.6032 55.466667-151.517867 151.099733z m0 0" p-id="4544" data-v-ec964632></path></svg></i></div> <canvas id="banniang" width="216" height="281.6" class="live2d" style="position:fixed;right:90px;bottom:-20px;opacity:1;" data-v-ec964632></canvas></div> <div class="showBanNiang" style="display:none;" data-v-ec964632>
    看板娘
  </div></div><!----><div></div></div></div>
    <script src="/assets/js/app.3c565d21.js" defer></script><script src="/assets/js/3.96c41bda.js" defer></script><script src="/assets/js/1.a1c68afd.js" defer></script><script src="/assets/js/15.6906d831.js" defer></script><script src="/assets/js/11.b60f3117.js" defer></script>
  </body>
</html>
