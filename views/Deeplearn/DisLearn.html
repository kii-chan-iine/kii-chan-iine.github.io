<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>机器学习笔记 | KII IINE</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="manifest" href="/manifest.json">
    <link rel="apple-touch-icon" href="/icons/apple-touch-icon-152x152.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2c2c2c">
    <meta name="description" content="明早一起去看海 望向未来">
    <meta name="theme-color" content="#22979b">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#22979b">
    <meta name="msapplication-TileImage" content="/icons/msapplication-icon-144x144.png">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="google-site-verification" content="XCppppl60fPQTlwxDodwZIhMarkybEgwVpcEz85KTuQ">
    
    <link rel="preload" href="/assets/css/0.styles.c67cb200.css" as="style"><link rel="preload" href="/assets/js/app.91cde711.js" as="script"><link rel="preload" href="/assets/js/3.a67abeb3.js" as="script"><link rel="preload" href="/assets/js/1.c373aa88.js" as="script"><link rel="preload" href="/assets/js/24.b604a735.js" as="script"><link rel="preload" href="/assets/js/11.747f0d2b.js" as="script"><link rel="prefetch" href="/assets/js/10.0c65cdf0.js"><link rel="prefetch" href="/assets/js/12.1393d74c.js"><link rel="prefetch" href="/assets/js/13.7d5668c8.js"><link rel="prefetch" href="/assets/js/14.f76d10b9.js"><link rel="prefetch" href="/assets/js/15.5d79064d.js"><link rel="prefetch" href="/assets/js/16.4a55b7d0.js"><link rel="prefetch" href="/assets/js/17.7bec1393.js"><link rel="prefetch" href="/assets/js/18.4245937e.js"><link rel="prefetch" href="/assets/js/19.3be50969.js"><link rel="prefetch" href="/assets/js/20.9eca8dc2.js"><link rel="prefetch" href="/assets/js/21.f12ae56b.js"><link rel="prefetch" href="/assets/js/22.cc92a370.js"><link rel="prefetch" href="/assets/js/23.1ceea30e.js"><link rel="prefetch" href="/assets/js/25.02d45b74.js"><link rel="prefetch" href="/assets/js/26.ef7c9433.js"><link rel="prefetch" href="/assets/js/27.bc18a6a3.js"><link rel="prefetch" href="/assets/js/28.9d729824.js"><link rel="prefetch" href="/assets/js/29.705e80e7.js"><link rel="prefetch" href="/assets/js/30.98f784be.js"><link rel="prefetch" href="/assets/js/31.daa21dfd.js"><link rel="prefetch" href="/assets/js/32.68d62e25.js"><link rel="prefetch" href="/assets/js/33.89244850.js"><link rel="prefetch" href="/assets/js/34.07e7148c.js"><link rel="prefetch" href="/assets/js/35.9c93f9fe.js"><link rel="prefetch" href="/assets/js/36.4036abaf.js"><link rel="prefetch" href="/assets/js/37.b128af3e.js"><link rel="prefetch" href="/assets/js/38.48458f4b.js"><link rel="prefetch" href="/assets/js/39.4d1009d7.js"><link rel="prefetch" href="/assets/js/4.09dda623.js"><link rel="prefetch" href="/assets/js/40.2a6044bf.js"><link rel="prefetch" href="/assets/js/41.ab636f56.js"><link rel="prefetch" href="/assets/js/42.7701b09f.js"><link rel="prefetch" href="/assets/js/43.e99bc73b.js"><link rel="prefetch" href="/assets/js/44.f0eb0862.js"><link rel="prefetch" href="/assets/js/45.b8e57277.js"><link rel="prefetch" href="/assets/js/46.5e55dca6.js"><link rel="prefetch" href="/assets/js/47.27d140f5.js"><link rel="prefetch" href="/assets/js/48.3d8f06d2.js"><link rel="prefetch" href="/assets/js/5.9efeece9.js"><link rel="prefetch" href="/assets/js/6.45c24d02.js"><link rel="prefetch" href="/assets/js/7.3391c463.js"><link rel="prefetch" href="/assets/js/8.1d292254.js"><link rel="prefetch" href="/assets/js/9.49750083.js">
    <link rel="stylesheet" href="/assets/css/0.styles.c67cb200.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>KII IINE</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>明早一起去看海 望向未来</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2023
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/favicon.ico" alt="KII IINE" class="logo"> <span class="site-name">KII IINE</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/funny/" class="nav-link"><i class="undefined"></i>
  funny
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine/kii-chan-iine.github.io/tree/develop" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/avatar.jpeg" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    KII IINE
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>31</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>12</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/funny/" class="nav-link"><i class="undefined"></i>
  funny
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine/kii-chan-iine.github.io/tree/develop" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>机器学习笔记</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2023
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">机器学习笔记</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>kii</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>7/1/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>NLP</span><span class="tag-item" data-v-1ff7123e>deeplearn</span></i></div></div> <div class="theme-reco-content content__default"><div id="boxx" data-v-f4ca0dac><div data-v-f4ca0dac><p v-if="true" class="custom-block-title" data-v-f4ca0dac></p> <p v-if="true" data-v-f4ca0dac></p></div></div> <div class="custom-block tip"><p class="title">前言</p><p>学习札记。</p></div> <h1 id="启用"><a href="#启用" class="header-anchor">#</a> 启用</h1> <h2 id="hadoop"><a href="#hadoop" class="header-anchor">#</a> hadoop</h2> <p><strong>去hadoop的sbin下面（因为加入了用户变量，可以直接运行）</strong></p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>./start-all.sh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>查看进程：如果如下，大概是对的</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token number">105765</span> NameNode
<span class="token number">101913</span> ResourceManager
<span class="token number">106201</span> Jps
<span class="token number">105962</span> SecondaryNameNode
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h2 id="spark-需要先启动hadoop"><a href="#spark-需要先启动hadoop" class="header-anchor">#</a> Spark(需要先启动hadoop)</h2> <ol><li><p>启动：先启动hadoop，再启动spark（sbin下）：master的jps下多了一个Master，slave的jps多了一个Worker</p></li> <li><p>验证：</p> <ol><li><p>本地</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>./bin/run-example SparkPi <span class="token number">10</span> -master local<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div></li> <li><p>spark的./目录下：集群class模式: http://master:8080/  →spark://master:7077</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment">#./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 /usr/local/src/spark-2.4.4-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.4.jar 100</span>
如果是在目录下，可以直接从examples开始
<span class="token punctuation">[</span>root@master **spark-2.4.4-bin-hadoop2.6**<span class="token punctuation">]</span><span class="token comment"># ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 examples/jars/spark-examples_2.11-2.4.4.jar 100</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div></li> <li><p>Yarn模式:http://master:8088/cluster   这个可以不到spark的sbin下单独启用spark服务</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster examples/jars/spark-examples_2.11-2.4.4.jar <span class="token number">10</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div></li></ol></li> <li><p>拷贝hive的文件到spark</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token function">cp</span> /usr/local/src/apache-hive-1.2.2-bin/conf/hive-site.xml /usr/local/src/spark-2.4.4-bin-hadoop2.6/conf

<span class="token function">cp</span> /usr/local/src/apache-hive-1.2.2-bin/lib/mysql-connector-java-5.1.46.jar /usr/local/src/spark-2.4.4-bin-hadoop2.6/jars/
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div></li> <li><p>ds</p></li></ol> <p>Hadoop存在的主要问题：</p> <ol><li>shuffle过程有大量的io操作</li> <li>Hadoop如果要进行数据处理只有map和reduce原语操作，比较单调。</li> <li>Hadoop中的map task和reduce task都是进程，进程创造的数据都互相独立</li></ol> <h2 id="spark3种模式"><a href="#spark3种模式" class="header-anchor">#</a> Spark3种模式</h2> <h3 id="本地local"><a href="#本地local" class="header-anchor">#</a> 本地local</h3> <p>local：只启动一个executor
local[k]:启动k个executor
local[*]：启动跟cpu数目相同的 executor</p> <h3 id="standalone"><a href="#standalone" class="header-anchor">#</a> Standalone</h3> <h3 id="yarn"><a href="#yarn" class="header-anchor">#</a> Yarn</h3> <ol><li>yarn-client(driver在本机spark的进程中，kill掉进程，整个任务挂，主要是交互用的，调试程序)</li> <li>yarn-cluster(driver在AM中，kill掉client进程，仍然可以运行。调试程序完，client可以关)-<font color="red">这个无法在idea提交</font></li></ol> <p>Spark 的核心是建立在统一的抽象弹性分布式数据集（Resiliennt Distributed Datasets，RDD）之上的，这使得 Spark 的各个组件可以无缝地进行集成，能够在同一个应用程序中完成大数据处理。</p> <p>rdd1(x+=1)→rdd2(x+=3)→rdd3</p> <p>先搭建关系和运算结构，过程中会启动新的rdd，但这个是一个逻辑结构，不存储数据。直到action算子来了，才会触发计算。每个rdd在哪个slave上随机，不知道。</p> <div class="language-jsx line-numbers-mode"><pre class="language-jsx"><code>rdd_act1<span class="token punctuation">.</span><span class="token function">take</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
这样才拿出数据来
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>RDD默认128M</p> <h1 id="spark"><a href="#spark" class="header-anchor">#</a> Spark</h1> <p><img src="https://images2017.cnblogs.com/blog/1209698/201801/1209698-20180121000856303-1743213672.png" alt="img"></p> <p><img src="https://images2017.cnblogs.com/blog/1209698/201801/1209698-20180121000905099-719226531.png" alt="img"></p> <h2 id="回顾-任务运行中的一些基本概念"><a href="#回顾-任务运行中的一些基本概念" class="header-anchor">#</a> 回顾：任务运行中的一些基本概念</h2> <ol><li><p>application：聚类任务，程序</p></li> <li><p>driver：驱动，管理cluster_task的驱动</p></li> <li><p>excutor ：执行任务的最小单位（可以认为是千万台中的一台计算单位，如2core 10gb的容器）</p></li> <li><p>worker：集群中可以运行app的节点，即slave节点（计算机）</p></li> <li><p>stage: <font color="cornflowerblue">宽窄依赖，shuffle操作进行分割</font></p></li> <li><p>task:线程，比如同时并行开启3个task</p></li> <li><p>job：Transformer 算子 和action算子；要记得<font color="red">spark是懒加载</font>，不遇到action算子不执行。如果没有action算子，那么他会先编译，而不执行。如下面2个job？</p> <div class="language-scala line-numbers-mode"><pre class="language-scala"><code>main<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">val</span> rdd<span class="token operator">=</span>dt<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>x<span class="token keyword">=&gt;</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reduceBykey<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span>
rdd<span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
rdd1<span class="token punctuation">.</span>map<span class="token punctuation">(</span>x<span class="token keyword">=&gt;</span><span class="token punctuation">(</span>x<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>x<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>action<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>懒加载可以节省磁盘IO，如</p> <div class="language-scala line-numbers-mode"><pre class="language-scala"><code>dt<span class="token punctuation">.</span>map<span class="token punctuation">(</span>x<span class="token keyword">=&gt;</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>filter<span class="token punctuation">(</span>x<span class="token keyword">=&gt;</span><span class="token operator">%</span><span class="token number">2</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>spark会首先优化该语句执行的顺序，先执行filter，节省磁盘IO和资源。scala的lazy就是懒加载</p></li> <li><p>Spark RDD主要由Dependency、Partition、Partitioner组成，<font color="red">Partition</font>是其中之一，这是个<font color="orange">物理概念</font>。一份待处理的原始数据会被按照相应的逻辑(例如jdbc和hdfs的split逻辑)切分成n份，每份数据对应到RDD中的一个Partition，<font color="red">Partition的数量决定了task的数量，影响着程序的并行度</font>，所以理解Partition是了解spark背后运行原理的第一步。</p> <p><img src="C:%5CUsers%5Ckaich%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210703212048339.png" alt="image-20210703212048339"></p> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
A[application_task]--&gt;B[job_T&amp;A algorithm]
B[job_T&amp;A algorithm]--&gt;F[stage]
B[job_T&amp;A algorithm]--&gt;E[stage]
F[stage]--&gt;C(task1)
F[stage]--&gt;D(task2)
E[stage]
E[stage]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div></li></ol> <h2 id="怎么调试spark"><a href="#怎么调试spark" class="header-anchor">#</a> 怎么调试spark</h2> <ol><li>你的计算机有idea，并且可以远程连接服务器</li> <li>组织一批数据</li> <li>现在local模式，调通代码</li> <li>关掉main方法代码中的local，改成‘yarn-cluster’;尽管其他地方可以调，但是<font color="red">最高优先级</font>是这个</li></ol> <hr> <h2 id="transformer一些算子"><a href="#transformer一些算子" class="header-anchor">#</a> Transformer一些算子</h2> <p>map():</p> <p>flatmap()：只是二维降到1维</p> <p>mappartition():</p> <p>repartition():</p> <p>groupByKey():</p> <p>reducebykey():</p> <p><font color="red">aggregateByKey()</font>：可操作性极强，相当于可以自己定义combiner</p> <ol><li><p>对PairRDD中相同的Key值进行聚合操作，在聚合过程中同样使用了一个中立的初始值。和aggregate函数类似，aggregateByKey返回值的类型不需要和RDD中value的类型一致。因为aggregateByKey是对相同Key中的值进行聚合操作，所以aggregateByKey'函数最终返回的类型还是PairRDD，对应的结果是Key和聚合后的值，而aggregate函数直接返回的是非RDD的结果。</p></li> <li><div class="language-scala line-numbers-mode"><pre class="language-scala"><code><span class="token operator">&lt;</span>span style<span class="token operator">=</span><span class="token string">&quot;font-family:Times New Roman;font-size:18px;&quot;</span><span class="token operator">&gt;</span>
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span></span>SparkConf
<span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span></span>SparkContext

<span class="token keyword">object</span> AggregateByKeyOp <span class="token punctuation">{</span>
  <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span>Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
     <span class="token keyword">val</span> sparkConf<span class="token operator">:</span> SparkConf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">&quot;AggregateByKey&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">&quot;local&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword">val</span> sc<span class="token operator">:</span> SparkContext <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>sparkConf<span class="token punctuation">)</span>
     
     <span class="token keyword">val</span> data<span class="token operator">=</span>List<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
     <span class="token keyword">val</span> rdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>data<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
     
     <span class="token comment">//合并不同partition中的值，a，b得数据类型为zeroValue的数据类型</span>
     <span class="token keyword">def</span> combOp<span class="token punctuation">(</span>a<span class="token operator">:</span><span class="token builtin">String</span><span class="token punctuation">,</span>b<span class="token operator">:</span><span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span><span class="token builtin">String</span><span class="token operator">=</span><span class="token punctuation">{</span>
       println<span class="token punctuation">(</span><span class="token string">&quot;combOp: &quot;</span><span class="token operator">+</span>a<span class="token operator">+</span><span class="token string">&quot;\t&quot;</span><span class="token operator">+</span>b<span class="token punctuation">)</span>
       a<span class="token operator">+</span>b
     <span class="token punctuation">}</span>
     <span class="token comment">//合并在同一个partition中的值，a的数据类型为zeroValue的数据类型，b的数据类型为原value的数据类型</span>
      <span class="token keyword">def</span> seqOp<span class="token punctuation">(</span>a<span class="token operator">:</span><span class="token builtin">String</span><span class="token punctuation">,</span>b<span class="token operator">:</span><span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span><span class="token builtin">String</span><span class="token operator">=</span><span class="token punctuation">{</span>
        println<span class="token punctuation">(</span><span class="token string">&quot;SeqOp:&quot;</span><span class="token operator">+</span>a<span class="token operator">+</span><span class="token string">&quot;\t&quot;</span><span class="token operator">+</span>b<span class="token punctuation">)</span>
        a<span class="token operator">+</span>b
      <span class="token punctuation">}</span>
      rdd<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>
      <span class="token comment">//zeroValue:中立值,定义返回value的类型，并参与运算</span>
      <span class="token comment">//seqOp:用来在同一个partition中合并值</span>
      <span class="token comment">//combOp:用来在不同partiton中合并值</span>
      <span class="token keyword">val</span> aggregateByKeyRDD<span class="token operator">=</span>rdd<span class="token punctuation">.</span>aggregateByKey<span class="token punctuation">(</span><span class="token string">&quot;100&quot;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>seqOp<span class="token punctuation">,</span> combOp<span class="token punctuation">)</span>
      sc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
<span class="token operator">&lt;</span><span class="token operator">/</span>span<span class="token operator">&gt;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br></div></div></li> <li><div class="language-shell line-numbers-mode"><pre class="language-shell"><code>运行结果：

将数据拆分成两个分区

//分区一数据
<span class="token punctuation">(</span><span class="token number">1,3</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token number">1,2</span><span class="token punctuation">)</span>
//分区二数据
<span class="token punctuation">(</span><span class="token number">1,4</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token number">2,3</span><span class="token punctuation">)</span>

//分区一相同key的数据进行合并
seq: <span class="token number">100</span>     <span class="token number">3</span>   //<span class="token punctuation">(</span><span class="token number">1,3</span><span class="token punctuation">)</span>开始和中立值进行合并  合并结果为 <span class="token number">1003</span>
seq: <span class="token number">1003</span>     <span class="token number">2</span>   //<span class="token punctuation">(</span><span class="token number">1,2</span><span class="token punctuation">)</span>再次合并 结果为 <span class="token number">10032</span>

//分区二相同key的数据进行合并
seq: <span class="token number">100</span>     <span class="token number">4</span>  //<span class="token punctuation">(</span><span class="token number">1,4</span><span class="token punctuation">)</span> 开始和中立值进行合并 <span class="token number">1004</span>
seq: <span class="token number">100</span>     <span class="token number">3</span>  //<span class="token punctuation">(</span><span class="token number">2,3</span><span class="token punctuation">)</span> 开始和中立值进行合并 <span class="token number">1003</span>

将两个分区的结果进行合并
//key为2的，只在一个分区存在，不需要合并 <span class="token punctuation">(</span><span class="token number">2,1003</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span><span class="token number">2,1003</span><span class="token punctuation">)</span>

//key为1的, 在两个分区存在，并且数据类型一致，合并
comb: <span class="token number">10032</span>     <span class="token number">1004</span>
<span class="token punctuation">(</span><span class="token number">1,100321004</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div></li></ol> <h2 id="action一些算子"><a href="#action一些算子" class="header-anchor">#</a> Action一些算子</h2> <ol><li><font color="red">aggregate函数</font>:将每个分区里面的元素进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</li> <li></li></ol> <h2 id="spark的宽窄依赖"><a href="#spark的宽窄依赖" class="header-anchor">#</a> spark的宽窄依赖</h2> <p>RDD是个逻辑分区。partition是个物理概念。</p> <ol><li><p>窄依赖是<font color="red">指父RDD的每个分区只被子RDD的一个分区所使用</font>，子RDD分区通常对应常数个父RDD分区(O(1)，与数据规模无关)   ：使用 :<font color="cornflowerblue">map、fliter、flatMap</font></p></li> <li><p>宽依赖是<font color="red">指父RDD的每个分区都可能被多个子RDD分区所使用</font>，子RDD分区通常对应所有的父RDD分区(O(n)，与数据规模有关):<font color="cornflowerblue">Collect、join、groupByKey（shuffle）</font></p></li> <li><p><img src="https://img-blog.csdn.net/20180723140006843?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqZ2l0aHVi/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p></li></ol> <p>1.宽依赖往往对应着shuffle操作(多对一,汇总,多节点)，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输；而窄依赖的每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换。</p> <p>2.当RDD分区丢失时（某个节点故障），spark会对数据进行重算。</p> <p>a.  对于窄依赖，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重算和子RDD分区对应的父RDD分区即可，所以这个重算对数据的利用率是100%的；
b.  对于宽依赖，重算的父RDD分区对应多个子RDD分区，这样实际上父RDD 中只有一部分的数据是被用于恢复这个丢失的子RDD分区的，另一部分对应子RDD的其它未丢失分区，这就造成了多余的计算；更一般的，宽依赖中子RDD分区通常来自多个父RDD分区，极端情况下，所有的父RDD分区都要进行重新计算。
c.  如下图所示，b1分区丢失，则需要重新计算a1,a2和a3，这就产生了冗余计算(a1,a2,a3中对应b2的数据)。</p> <p><img src="https://img-blog.csdn.net/20180723140213409?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lqZ2l0aHVi/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p> <p><strong>为什么进行宽窄依赖</strong>：宽依赖会导致shuffle，会进行不同分区间的分发，shuffle导致数据落地，数据落地导致磁盘IO，磁盘IO导致计算速度下降，计算速度下降导致性能降低。</p> <hr> <p>对多次使用的RDD进行持久化，避免多次进行shuffle的时候出错，需要重复全部计算。</p> <p>cache()：可以把数据加载内存中，获得数据很快</p> <p>un.pesist:释放内存</p> <h2 id="在spark中创建rdd的创建方式"><a href="#在spark中创建rdd的创建方式" class="header-anchor">#</a> 在Spark中创建RDD的创建方式</h2> <p>（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。</p> <p>而从集合中创建RDD，<a href="https://www.iteblog.com/archives/tag/spark/" target="_blank" rel="noopener noreferrer">Spark<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>主要提供了两中函数：parallelize和makeRDD。</p> <h1 id="flink"><a href="#flink" class="header-anchor">#</a> Flink</h1> <h2 id="回顾"><a href="#回顾" class="header-anchor">#</a> 回顾</h2> <h1 id="推荐"><a href="#推荐" class="header-anchor">#</a> 推荐</h1> <h2 id="推荐算法"><a href="#推荐算法" class="header-anchor">#</a> 推荐算法</h2> <blockquote><p>推荐系统：解决信息过载的问题：对信息的反应速度远远大于信息传播的速度（快）；信息量大于受众接受的信息量（大）；大量无关信息（杂）</p></blockquote> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
F[横向流程图]--&gt;C(排序模型)
F[横向流程图]--&gt;D(召回算法)
A[候选集合] --&gt;D(召回算法)
D(召回算法)--&gt;C(排序模型)
E[特征工程]--&gt;C(排序模型)
E[特征工程]--&gt;D(召回算法)

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><h3 id="召回-粗排"><a href="#召回-粗排" class="header-anchor">#</a> 召回（粗排）：</h3> <h4 id="基于内容的推荐contentbase"><a href="#基于内容的推荐contentbase" class="header-anchor">#</a> 基于内容的推荐contentbase</h4> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
A[蓝色的鞋子] --&gt;D(红色的鞋子)
A[蓝色的鞋子] --&gt;E(蓝色的袜子)
A[蓝色的鞋子] --&gt;C(阿迪的鞋子)
D(红色的鞋子)--&gt;G(0.7)
E(蓝色的袜子)--&gt;H(0.2)
C(阿迪的鞋子)--&gt;I(0.5)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>（<font color="red">正排</font>）商家的商品列表：这个过程用<font color="cornflowerblue">TF-IDF</font>打分</p> <p>红色的运动鞋--&gt; 分词--&gt;红色/的/运动鞋(S6)</p> <p>蓝色的跑鞋--&gt;<font color="orange">蓝色</font>（S1）/的/跑鞋(S2)</p> <p>蓝色的天空--&gt;<font color="orange">蓝色</font>(S3)/的/天空(S4)</p> <p>蓝色的运动鞋--&gt;<font color="orange">蓝色</font>(S8)/的/运动鞋(S7)</p> <p><font color="red">倒排</font>这个过程累加每个token在各个物品</p> <p>蓝色(token)--&gt;蓝色的天空(Score2)，蓝色的跑鞋(Score1),蓝色的运动鞋</p> <hr> <p>这时用户搜索：蓝色/的/运动鞋--找<font color="red">倒排表</font></p> <p>蓝色-蓝色的天空(S3)、蓝色的跑鞋(S1),蓝色的运动鞋(S8)</p> <p>运动鞋-红色的运动鞋(S6),蓝色的运动鞋(S7)</p> <p>那么推荐蓝色的运动鞋的分数:S7+S8</p> <p>优点：可解释性好；简单易实现</p> <p>缺点：仅停留在字面，没深层次分析；无个性化</p> <p>可将用户正在浏览的列表替换为历史浏览记录进行个性化调整。</p> <h4 id="协同过滤-collaborative-filtering"><a href="#协同过滤-collaborative-filtering" class="header-anchor">#</a> 协同过滤 Collaborative Filtering</h4> <ol><li><p>User-based CF：用户喜欢和他相似用户喜欢的东西</p></li> <li><p>Item-based CF： 用户喜欢过去喜欢的物品相似的物品</p></li></ol> <h4 id="mf-矩阵分解"><a href="#mf-矩阵分解" class="header-anchor">#</a> MF，矩阵分解</h4> <h4 id="模型融合"><a href="#模型融合" class="header-anchor">#</a> 模型融合</h4> <h3 id="排序"><a href="#排序" class="header-anchor">#</a> <strong>排序</strong>：</h3> <h4 id="lr"><a href="#lr" class="header-anchor">#</a> LR</h4> <h4 id="特征组合"><a href="#特征组合" class="header-anchor">#</a> 特征组合</h4> <h4 id="神经网络"><a href="#神经网络" class="header-anchor">#</a> 神经网络</h4> <h2 id="精排"><a href="#精排" class="header-anchor">#</a> 精排</h2> <h1 id="中文分词"><a href="#中文分词" class="header-anchor">#</a> 中文分词</h1> <h2 id="前缀树"><a href="#前缀树" class="header-anchor">#</a> 前缀树</h2> <h2 id="语言模型"><a href="#语言模型" class="header-anchor">#</a> <font color="orange">语言模型</font></h2> <h3 id="马尔科夫模型"><a href="#马尔科夫模型" class="header-anchor">#</a> 马尔科夫模型</h3> <h3 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="header-anchor">#</a> 隐马尔可夫模型</h3> <p>1元，2元（依赖前面的1个状态,即可视范围），3元（依赖前面的2个状态）模型：有限范围内的依赖</p> <h1 id="nlp神经网络"><a href="#nlp神经网络" class="header-anchor">#</a> NLP神经网络</h1> <h2 id="nlp发展史"><a href="#nlp发展史" class="header-anchor">#</a> NLP发展史</h2> <p>2001 - Neural language models（神经语言模型）
2008 - Multi-task learning（多任务学习）
2013 - Word embeddings（词嵌入）
2013 - Neural networks for NLP（NLP神经网络）
2014 - <font color="red">Sequence-to-sequence models</font>
2015 - <font color="red">Attention（注意力机制）</font>
2015 - Memory-based networks（基于记忆的网络）
2018 - Pretrained language models（预训练语言模型）-Bert,GPT等</p> <h2 id="dnn"><a href="#dnn" class="header-anchor">#</a> DNN</h2> <h2 id="cnn"><a href="#cnn" class="header-anchor">#</a> CNN</h2> <p>图像领域，当然可用于NLP领域，主要是用于特征的抽取。</p> <h2 id="rnn"><a href="#rnn" class="header-anchor">#</a> RNN</h2> <p>适合处理时序问题。</p> <p>用于：<font color="cornflowerblue">写文章、机器翻译、语音识别、输入法、回归分类、文本相似度（特征抽取）</font></p> <h3 id="rnn-vs-dnn"><a href="#rnn-vs-dnn" class="header-anchor">#</a> RNN vs DNN</h3> <p>RNN隐层之间有相互的联系</p> <h3 id="rnn-2"><a href="#rnn-2" class="header-anchor">#</a> RNN</h3> <p>循环神经网络（RNN）：<strong>隐含层之间的节点不再无连接而是有链接的，并且隐含层的输入不仅包含输入层的输出还包含上一时刻隐含层的输出</strong>。</p> <p><img src="https://img-blog.csdnimg.cn/20190719095454627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTA4OTAwNw==,size_16,color_FFFFFF,t_70" alt="img"></p> <p>其中：$x_i$表示为第t,t=1,2,3步的输入;$S_t$是网络的记忆单元。$S_t$根据当前的输入和上一时刻隐含层的输出进行计算，$S_t=f(Ux_t+W_{s_{t-1}})$,其中f一般为非线性的激活函数，如tanh，Relu（后面是LSTM），再计算$S_o$.</p> <p>$O_t$是t步的输出，$O_t=softmax(Vs_t)$</p> <p><img src="https://pic3.zhimg.com/v2-9e50e23bd3dff0d91b0198d0e6b6429a_r.jpg" alt="preview"></p> <p>图中每一步都会有输出，但是每一步都要有输出并不是必须的。比如，我们需要预测一条语句所表达的情绪，我们仅仅需要关系最后一个单词输入后的输出，而不需要知道每个单词输入后的输出。同理，每步都需要输入也不是必须的。<strong>循环神经网络（RNN）的关键之处在于隐含层，隐含层能够捕捉序列的信息。</strong></p> <h2 id="brnn"><a href="#brnn" class="header-anchor">#</a> BRNN</h2> <p>**双向循环网络的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络，而且这两个都连接着一个输出层。**这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一个时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层（w1, w3），隐含层到隐含层自己（w2, w5），向前和向后隐含层到输出层（w4, w6）。值得注意的是：向前和向后隐含层之间没有信息流，这保证了展开图是非循环的。
<img src="https://img-blog.csdnimg.cn/20190719095650582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTA4OTAwNw==,size_16,color_FFFFFF,t_70" alt="img"></p> <p><img src="https://img-blog.csdnimg.cn/20190719095703825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTA4OTAwNw==,size_16,color_FFFFFF,t_70" alt="img"></p> <p>Forward pass:</p> <p>对于BRNN的隐含层，向前推算跟单向的RNN一样，除了输入序列对于两个隐含层是相反方向的，</p> <p>输出层直到两个隐含层处理完所有的全部输入序列才更新：</p> <p><img src="https://img-blog.csdn.net/20160721152522777" alt="img"></p> <p>Backward pass:</p> <p>双向循环神经网络（BRNN）的向后推算与标准的循环神经网络（RNN）通过时间反向传播相似，除了所有的输出层δ项首先被计算，然后返回给两个不同方向的隐含层：</p> <p><img src="https://img-blog.csdn.net/20160721153358055" alt="img"></p> <h2 id="lstm"><a href="#lstm" class="header-anchor">#</a> LSTM</h2> <p>为了解决<font color="cornflowerblue">长序列</font>训练过程中的<font color="red">梯度消失和梯度爆炸</font>问题。</p> <p><img src="https://pic4.zhimg.com/80/v2-e4f9851cad426dfe4ab1c76209546827_720w.jpg" alt="img"></p> <p>相比RNN只有一个传递状态 <img src="https://www.zhihu.com/equation?tex=h%5Et+" alt="[公式]"> ，LSTM有两个传输状态，一个 <img src="https://www.zhihu.com/equation?tex=c%5Et" alt="[公式]"> （cell state），和一个 <img src="https://www.zhihu.com/equation?tex=h%5Et" alt="[公式]"> （hidden state）。（Tips：RNN中的 <img src="https://www.zhihu.com/equation?tex=h%5Et" alt="[公式]"> <font color="red">对于LSTM中的</font> <img src="https://www.zhihu.com/equation?tex=c%5Et" alt="[公式]"> )</p> <p>首先使用LSTM的当前输入 <img src="https://www.zhihu.com/equation?tex=x%5Et" alt="[公式]"> 和上一个状态传递下来的 <img src="https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D" alt="[公式]"> 拼接训练得到四个状态。
$$
z=tanh(w\frac{x^t}{h^{t-1}})，这里不是分数\
z^i=\delta(w^i\frac{x^t}{h^{t-1}})\
z^f=\delta(w^f\frac{x^t}{h^{t-1}})\
z^o=\delta(w^o\frac{x^t}{h^{t-1}})
$$
其中， <img src="https://www.zhihu.com/equation?tex=z%5Ef+" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=z%5Ei" alt="[公式]"> ，<img src="https://www.zhihu.com/equation?tex=z%5Eo" alt="[公式]"> 是由拼接向量乘以权重矩阵之后，再通过一个 <img src="https://www.zhihu.com/equation?tex=sigmoid+" alt="[公式]"> 激活函数转换成0到1之间的数值，来作为一种门控状态。而 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 则是将结果通过一个 <img src="https://www.zhihu.com/equation?tex=tanh" alt="[公式]"> 激活函数将转换成-1到1之间的值（这里使用 <img src="https://www.zhihu.com/equation?tex=tanh" alt="[公式]"> 是因为这里是将其做为输入数据，而不是门控信号)。</p> <p><img src="https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_720w.jpg" alt="img"></p> <p><img src="https://www.zhihu.com/equation?tex=%5Codot" alt="[公式]"> 是哈达玛积(Hadamard Product)，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 <img src="https://www.zhihu.com/equation?tex=%5Coplus" alt="[公式]"> 则代表进行矩阵加法。</p> <p>LSTM内部主要有三个阶段：</p> <ol><li><p>忘记阶段。这个阶段主要是对上一个节点传进来的输入进行<strong>选择性</strong>忘记。简单来说就是会 “忘记不重要的，记住重要的”。具体来说是通过计算得到的 <img src="https://www.zhihu.com/equation?tex=z%5Ef" alt="[公式]"> （f表示forget）来作为忘记门控，来控制上一个状态的 <img src="https://www.zhihu.com/equation?tex=c%5E%7Bt-1%7D" alt="[公式]"> 哪些需要留哪些需要忘。</p></li> <li><p>选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 <img src="https://www.zhihu.com/equation?tex=x%5Et" alt="[公式]"> 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 <img src="https://www.zhihu.com/equation?tex=z+" alt="[公式]"> 表示。而选择的门控信号则是由 <img src="https://www.zhihu.com/equation?tex=z%5Ei" alt="[公式]"> （i代表information)来进行控制。</p></li></ol> <blockquote><p>将上面两步得到的结果相加，即可得到传输给下一个状态的 <img src="https://www.zhihu.com/equation?tex=c%5Et" alt="[公式]"> 。也就是上图中的第一个公式。</p></blockquote> <ol start="3"><li>输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 <img src="https://www.zhihu.com/equation?tex=z%5Eo" alt="[公式]"> 来进行控制的。并且还对上一阶段得到的 <img src="https://www.zhihu.com/equation?tex=c%5Eo" alt="[公式]"> 进行了放缩（通过一个tanh激活函数进行变化)。</li></ol> <p>与普通RNN类似，<strong>输出 <img src="https://www.zhihu.com/equation?tex=y%5Et" alt="[公式]"> 往往最终也是通过 <img src="https://www.zhihu.com/equation?tex=h%5Et" alt="[公式]"> 变化得到</strong>。</p> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/image-20210708170053422.png" alt="image-20210708170053422"></p> <h2 id="双向lstm"><a href="#双向lstm" class="header-anchor">#</a> 双向LSTM</h2> <p>**简单来说双向LSTM就是把BRNN中隐含层的小圆圈换成了长短时记忆的模块。**与其说长短时记忆是一种循环神经网络，倒不如说是一个加强版的组件被放在了循环神经网络中。这个模块的样子如下图所示：</p> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/%E8%9C%82%E8%9C%9C%E6%B5%8F%E8%A7%88%E5%99%A8_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210708174917.jpg" alt="蜂蜜浏览器_微信图片_20210708174917"></p> <p>遗忘门：
$$
f_t=\sigma(W_f(h_{t-1},x_t)+b_f)
$$
输入门：</p> <p>$$
i_t=\sigma(W_i\ast[h_{t-1},x_t]+b_i)\
\hat{C_t}=tanh(W_c\ast[h_{t-1},x_t]+b_c)\
C_t=f_t\circ C_{t-1}+i_t\circ\hat{C_t}
$$</p> <p>输出门：</p> <p>$$
o_t=\sigma(W_o\ast[h_{t-1},x_t]+b_o)\
h_t=o_t\circ tanh(C_t)
$$</p> <h2 id="transformer"><a href="#transformer" class="header-anchor">#</a> Transformer</h2> <p>Transformer是Attention is all you need<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">这篇论文<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>里提出的一个新框架。</p> <p>RNN不能够并行，训练速度慢；CNN易并行（多个卷积核），但是视野局限。计算复杂度高。</p> <p>Transformer逐步取代RNN、CNN成为最主流的特征抽取器</p> <p><font color="blue"><strong>RNN的问题</strong>：</font></p> <ul><li>难以具备并行计算能力</li></ul> <p><font color="blue"><strong>CNN的问题</strong></font>：</p> <ul><li>单层卷积无法捕获远距离特征</li> <li>DilatedCNN容易错失重要特征组合---<a href="https://blog.csdn.net/qq_15111861/article/details/81009636" target="_blank" rel="noopener noreferrer">空洞卷积<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>加深CNN参数设置不方便</li></ul> <h3 id="seq2seq"><a href="#seq2seq" class="header-anchor">#</a> Seq2Seq</h3> <p>Seq2Seq模型，传统的机器翻译基本都是基于Seq2Seq模型来做的，该模型分为encoder层与decoder层，并均为RNN或RNN的变体构成，如下图所示：</p> <p><img src="https://img-blog.csdnimg.cn/20190407193133441.gif" alt="在这里插入图片描述"></p> <p>在encode阶段，第一个节点输入一个词，之后的节点输入的是下一个词与前一个节点的hidden state，最终encoder会输出一个context，这个context又作为decoder的输入，每经过一个decoder的节点就输出一个翻译后的词，并把decoder的hidden state作为下一层的输入。该模型对于短文本的翻译来说效果很好，但是其也存在一定的缺点，如果文本稍长一些，就很容易丢失文本的一些信息，为了解决这个问题，Attention应运而生。</p> <h3 id="attention-一个机制-方法"><a href="#attention-一个机制-方法" class="header-anchor">#</a> attention（一个机制，方法）</h3> <p>attention是一种能让模型对重要信息重点关注并充分学习吸收的技术，<strong>它不算是一个完整的模型</strong>，应当是一种技术，能够作用于任何序列模型中。</p> <p><img src="https://img-blog.csdnimg.cn/20190407193205893.gif" alt="在这里插入图片描述"></p> <p>Attention，正如其名，注意力，该模型在decode阶段，会选择最适合当前节点的context作为输入。<strong>Attention与传统的Seq2Seq模型主要有以下两点不同。</strong></p> <p>1）encoder提供了更多的数据给到decoder，<font color="red">encoder会把所有的节点的hidden state提供给decoder</font>，而不仅仅只是encoder最后一个节点的hidden state。</p> <p>2）decoder并不是直接把所有encoder提供的hidden state作为输入，而是<font color="red">采取一种选择机制，把最符合当前位置的hidden state选出来</font>，具体的步骤如下</p> <ul><li><p>确定哪一个hidden state与当前节点关系最为密切</p></li> <li><p>计算每一个hidden state的分数值（具体怎么计算我们下文讲解）</p></li> <li><p>对每个分数值做一个softmax的计算，这能让相关性高的hidden state的分数值更大，相关性低的hidden state的分数值更低</p></li></ul> <p>例子：</p> <img src="https://img-blog.csdnimg.cn/20190407193223473.gif" alt="在这里插入图片描述"> <p>把每一个encoder节点的hidden states的值与decoder当前节点的上一个节点的hidden state相乘，如上图，h1、h2、h3分别与当前节点的上一节点的hidden state进行相乘(如果是第一个decoder节点，需要随机初始化一个hidden state)，最后会获得三个值，这三个值就是上文提到的hidden state的分数，注意，这个数值对于每一个encoder的节点来说是不一样的，把该分数值进行softmax计算，计算之后的值就是每一个encoder节点的hidden states对于当前节点的权重，把权重与原hidden states相乘并相加，得到的结果即是当前节点的hidden state。可以发现，其实<font color="red">Atttention的关键就是计算这个分值。</font></p> <p>明白每一个节点是怎么获取hidden state之后，接下来就是decoder层的工作原理了，其具体过程如下：</p> <p>**第一个decoder的节点初始化一个向量，并计算当前节点的hidden state，**把该hidden state作为第一个节点的输入，经过RNN节点后得到一个新的hidden state与输出值。注意，这里和Seq2Seq有一个很大的区别，Seq2Seq是直接把输出值作为当前节点的输出，但是Attention会把该值与hidden state做一个连接，并把连接好的值作为context，并送入一个前馈神经网络，最终当前节点的输出内容由该网络决定，重复以上步骤，直到所有decoder的节点都输出相应内容。</p> <img src="https://img-blog.csdnimg.cn/20190407193243788.gif" alt="在这里插入图片描述" style="zoom:150%;"> <p>Attention模型并不只是盲目地将输出的第一个单词与输入的第一个词对齐。实际上，它在训练阶段学习了如何在该语言对中对齐单词(示例中是法语和英语)。Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射。</p> <p><img src="https://img-blog.csdnimg.cn/20190625094348755.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>在计算attention时主要分为三步，第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有<font color="red">点积，拼接，感知机等</font>；然后第二步一般是使用一个softmax函数对这些权重进行归一化；最后将权重和相应的键值value进行加权求和得到最后的attention。目前在NLP研究中，<strong>key和value常常都是同一个，即key=value</strong>。<font color="red">当Query=key=value时就是self-attention</font>。</p> <p><img src="https://img-blog.csdnimg.cn/20190625094424708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>比如上面这个图：key 通过第一阶段的到了分数s1-s4；然后通过归一化softmax得到了a1-a4；然后求$ \sum a_1 \ast Value1$...</p> <p>得到attention。</p> <p>Attention本质是一种对齐的方法。</p> <h3 id="self-attention模型"><a href="#self-attention模型" class="header-anchor">#</a> <strong>Self Attention模型</strong></h3> <h4 id="拓展阅读"><a href="#拓展阅读" class="header-anchor">#</a> 拓展阅读</h4> <p>https://blog.csdn.net/tmb8z9vdm66wh68vx1/article/details/106030130</p> <p>https://daiwk.github.io/posts/nlp-self-attention-models.html#self-attention</p> <p>https://daiwk.github.io/posts/nlp-self-attention-models.html#self-attention</p> <p>通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。</p> <p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。<strong>而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target（<font color="red">Query</font>）=Source（<font color="red">Key</font>）这种特殊情况下的注意力计算机制</strong>。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p> <p>如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。</p> <p><img src="https://pic2.zhimg.com/80/v2-1b7a38bc0bd8a46b52753ece64f665ad_hd.jpg" alt="img"></p> <blockquote><p>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p> <p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p></blockquote> <h3 id="transformer-是一个模型-rnn-lstm"><a href="#transformer-是一个模型-rnn-lstm" class="header-anchor">#</a> Transformer(是一个模型，RNN，LSTM)</h3> <p>和Attention一样，Transformer模型中也采用了 encoer-decoder 架构。但其结构相比于Attention更加复杂，论文中encoder层由6个encoder堆叠在一起，decoder层也一样。</p> <img src="https://img-blog.csdnimg.cn/20190407193306430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;"> <p>每一个encoder和decoder的内部简版结构如下图
<img src="https://img-blog.csdnimg.cn/2019040719332630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。<strong>decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。</strong></p> <p>现在我们知道了模型的主要组件，接下来我们看下模型的内部细节。首先，模型需要对输入的数据进行一个embedding操作，（也可以理解为类似w2v的操作），enmbedding结束之后，输入到encoder层，self-attention处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个encoder。</p> <p><img src="https://img-blog.csdnimg.cn/20190407193344547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <h4 id="self-attention-再说一下"><a href="#self-attention-再说一下" class="header-anchor">#</a> <strong>Self-Attention</strong>(再说一下)</h4> <p>接下来我们详细看一下self-attention，其思想和attention类似，但是self-attention是Transformer用来<font color="red">将其他相关单词的“理解”转换成我们正常理解的单词的一种思路</font>，我们看个例子：
The animal didn't cross the street because it was too tired
这里的it到底代表的是animal还是street呢，对于我们来说能很简单的判断出来，但是对于机器来说，是很难判断的，self-attention就能够让机器把it和animal联系起来</p> <p><img src="https://img-blog.csdnimg.cn/20190625101639261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>接下来我们看下详细的处理过程。</p> <p>1、首先，self-attention会计算出三个新的向量，在论文中，<strong>每个输入向量的维度是1*512维</strong>，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（<font color="red">64，512</font>）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的。</p> <blockquote><p>解读：</p> <ol><li>这里有2个单词，首先将他们表示为2个向量（Embedding）<font color="red">key=value</font></li> <li>分别初始化一些独立的向量Q，K，V--&gt;得到后面的权重矩阵$W^Q,W^K,W^V $,这些都是模型自己学习出来的</li></ol></blockquote> <p><img src="https://img-blog.csdnimg.cn/20190407193410827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>$\sum$<font color="cornflowerblue">Softmax(Query*Key)</font> *Value</p> <p>2、计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2</p> <p><img src="https://img-blog.csdnimg.cn/20190407193428125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>3、接下来，把点成的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的<font color="red">第一个维度（第一步）的开方</font>即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词相关性肯定会会很大</p> <p><img src="https://img-blog.csdnimg.cn/20190407193445308.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>4、下一步就是把Value和softmax得到的值进行相乘，并相加，<font color="red">得到的结果即是self-attetion在当前节点的值Z1。</font></p> <blockquote><p>Z1=0.88<em>V1+0.12</em>V2</p></blockquote> <p><img src="https://img-blog.csdnimg.cn/20190407193506971.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p><em><strong>实现并行</strong></em></p> <p><strong>在实际的应用场景，为了提高计算速度，我们一下计算多个句子，采用的是矩阵的方式，直接计算出Query, Key, Value的矩阵，然后把embedding的值与三个矩阵直接相乘，把得到的新矩阵Q与K相乘，乘以一个常数，做softmax操作，最后乘上V矩阵</strong>，特别是GPU特别适合这样的操作</p> <p><img src="https://img-blog.csdnimg.cn/20190407193524753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaled dot-product attention。其实scaled dot-Product attention就是我们常用的使用点积进行相似度计算的attention，只是**多除了一个（为K的维度）**起到调节作用，使得内积不至于太大。</p> <p><img src="https://img-blog.csdnimg.cn/20190625100244308.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <hr> <p>另一种方式理解</p> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/20210709201311.png" alt="20210709201311"></p> <p>为什么叫self呢，是因为是在<font color="red"><em>同一句中找相关的单词</em></font>。</p> <h4 id="multi-headed-attention"><a href="#multi-headed-attention" class="header-anchor">#</a> Multi-Headed Attention</h4> <p>这篇论文更厉害的地方是给self-attention加入了另外一个机制，被称为“multi-headed” attention，该机制理解起来很简单，就是说不<strong>仅仅只初始化一组Q、K、V的矩阵</strong>，而是<strong>初始化多组</strong>，tranformer是使用了8组，所以最后得到的结果是8个矩阵。<strong>这里的head类似于CNN里面的卷积核。</strong></p> <p><img src="https://img-blog.csdnimg.cn/2019040719354095.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p><img src="https://img-blog.csdnimg.cn/20190407193558607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>这给我们留下了一个小的挑战，前馈神经网络没法输入8个矩阵呀，这该怎么办呢？所以我们需要一种方式，把8个矩阵降为1个，首先，我们把8个矩阵连在一起，这样会得到一个大的矩阵，再随机初始化一个矩阵和这个组合好的矩阵相乘，最后得到一个最终的矩阵。</p> <p><img src="https://img-blog.csdnimg.cn/20190407193641706.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>这就是multi-headed attention的全部流程了，这里其实已经有很多矩阵了，我们把所有的矩阵放到一张图内看一下总体的流程。可以进行<strong>GPU</strong>加速。</p> <p><img src="https://img-blog.csdnimg.cn/20190407193700332.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>多头attention（Multi-head attention）整个过程可以简述为：Query，Key，Value首先进过一个线性变换，然后输入到放缩点积attention（注意这里要做**<font color="red">h</font>**次，其实也就是所谓的多头，每一次算一个头，而且每次Q，K，V进行线性变换的参数W是不一样的），然后将h次的放缩点积attention结果<font color="red">进行拼接</font>，再进行一次线性变换得到的值作为多头attention的结果。可以看到，<strong>google提出来的多头attention的不同之处在于进行了h次计算而不仅仅算一次</strong>，论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息，后面还会根据attention可视化来验证。</p> <p><img src="https://img-blog.csdnimg.cn/20190625100602310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>那么在整个模型中，是如何使用attention的呢？如下图，首先在编码器到解码器的地方使用了多头attention进行连接，K，V，Q分别是编码器的层输出（这里K=V）和解码器中都头attention的输入。其实就和主流的机器翻译模型中的attention一样，利用解码器和编码器attention来进行翻译对齐。然后在编码器和解码器中都使用了多头自注意力self-attention来学习文本的表示。<font color="red">Self-attention即K=V=Q</font>，例如<strong>输入一个句子，那么里面的每个词都要和该句子中的所有词进行attention计算。目的是学习句子内部的词依赖关系，捕获句子的内部结构。</strong></p> <p><img src="https://img-blog.csdnimg.cn/20190625100926500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>使用self-attention的原因：</p> <ul><li><p>每一层的复杂度：</p> <ul><li>如果输入序列n小于表示维度d的话，每一层的时间复杂度Self-Attention是比较有优势的。</li> <li>当n比较大时，作者也给出了一种解决方案Self-Attention(restricted)即每个词不是和所有词计算Attention，而是只与限制的r个词去计算Attention。</li></ul></li> <li><p>是否可以并行: multi-head Attention和CNN一样不依赖于前一时刻的计算，可以很好的并行，优于 RNN。</p></li> <li><p>长距离依赖: 由于Self-Attention是每个词和所有词都要计算Attention，所以不管他们中间有多长距离，最大的路径长度也都只是 1。可以捕获长距离依赖关系。</p> <p><img src="https://img-blog.csdnimg.cn/20190625101001478.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li></ul> <p>现在我们已经接触了attention的header，让我们重新审视我们之前的例子，看看例句中的“it”这个单词在不同的attention header情况下会有怎样不同的关注点（这里不同颜色代表attention不同头的结果，颜色越深attention值越大）。</p> <p><img src="https://img-blog.csdnimg.cn/20190625102044120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>当我们对“it”这个词进行编码时，一个注意力的焦点主要集中在“animal”上，而另一个注意力集中在“tired”（两个heads）
但是，如果我们将所有注意力添加到图片中，可能有点难理解：</p> <p><img src="https://img-blog.csdnimg.cn/2019040719371928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <h4 id="positional-encoding"><a href="#positional-encoding" class="header-anchor">#</a> Positional Encoding</h4> <h5 id="相似度任务"><a href="#相似度任务" class="header-anchor">#</a> 相似度任务</h5> <p>输入两个两个字符串之间的关系。</p> <blockquote><p>我今天很开心</p> <p>开心的不要不要的</p></blockquote> <p>步骤：</p> <ol><li><p>分词</p> <blockquote><p>我/今天/很/开心</p> <p>开心/的/不要/不要/的</p></blockquote></li> <li><p>拼接成一个句子,CLS和SPE是自己打上的特殊标记，CLS表示开头，SPE表示分割或者结尾</p> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
A[CLS]--&gt;B[我 今天 很 开心]--&gt;C[SEP]--&gt;D[开心 的 不要 不要 的]--&gt;E[SEP]

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>判断这个句子是1还是0，是1相关</p></li> <li><p>将每个单词变为embedding（查字典，直接插embedding的dic，开始是堆积初始化，但是随着学习，会变为固定的vector的，每个单词是不一样的），e是embedding</p> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
A[CLS]--&gt;B[我 今天 很 开心]--&gt;C[SEP]--&gt;D[开心 的 不要 不要 的]--&gt;E[SEP]
A1[CLS]--&gt;B1[e ee e ee]--&gt;C1[SEP]--&gt;D1[ee e ee ee e]--&gt;E1[SEP]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div></li> <li><p>然后再加一个标签，明确的表示那个句子是前面的，那个句子是后面的。这四行分别是---<font color="red">句子、 word embedding,  Segment embedding,  position embedding</font></p> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
A[CLS]--&gt;B[我 今天 很 开心]--&gt;C[SEP]--&gt;D[开心 的 不要 不要 的]--&gt;E[SEP]
A1[CLS]--&gt;B1[e ee e ee]--&gt;C1[SEP]--&gt;D1[ee e ee ee e]--&gt;E1[SEP]
A2[1]--&gt;B2[1 11 1 11]--&gt;C2[2]--&gt;D2[11 1 11 11 1]--&gt;E2[2]
A3[1]--&gt;B3[2 3 4 5]--&gt;C3[6]--&gt;D3[7 8 9 10 11]--&gt;E3[12]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div></li> <li><p>这时候有了3个向量：embedding，句子位置vector，每个单词的位置vector</p></li> <li><p>将这3个向量加起来，叫做input embedding，如下图</p> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
A[v]--&gt;B[v v v v]--&gt;C[v]--&gt;D[v v v v v]--&gt;E[SEP]

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div></li> <li><p>将input embedding传给<strong>Bert</strong>。bert会返回是1还是0.</p></li></ol> <h5 id="position-encoding"><a href="#position-encoding" class="header-anchor">#</a> position encoding</h5> <p>到目前为止，transformer模型中还缺少一种<strong>解释输入序列中单词顺序</strong>的方法。为了处理这个问题，transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下
$$
PE=(pos,2i)=sin(pos/10000^{2i}/d_model)\
PE=(pos,2i+1)=cos(pos/10000^{2i}/d_model)
$$</p> <p>其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。</p> <p><img src="https://img-blog.csdnimg.cn/20190407193742137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>为了让模型捕捉到单词的顺序信息，我们添加位置编码向量信息（POSITIONAL ENCODING），位置编码向量不需要训练，它有一个规则的产生方式（上图公式）。</p> <p>如果我们的嵌入维度为4，那么实际上的位置编码就如下图所示：</p> <p><img src="https://img-blog.csdnimg.cn/20190407193756162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>那么生成位置向量需要遵循怎样的规则呢？</p> <p>观察下面的图形，每一行都代表着对一个矢量的位置编码。因此第一行就是我们输入序列中第一个字的嵌入向量，每行都包含512个值，每个值介于1和-1之间。我们用颜色来表示1，-1之间的值，这样方便可视化的方式表现出来：</p> <img src="https://img-blog.csdnimg.cn/20190407193805560.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;"> <p>这是一个20个字（行）的（512）列位置编码示例。你会发现它咋中心位置被分为了2半，这是因为左半部分的值是一由一个正弦函数生成的，而右半部分是由另一个函数（余弦）生成。然后将它们连接起来形成每个位置编码矢量。</p> <h5 id="layer-normalization"><a href="#layer-normalization" class="header-anchor">#</a> Layer normalization</h5> <p>在transformer中，每一个子层（self-attetion，ffnn）之后都会接一个残差模块，并且有一个Layer normalization</p> <blockquote><p>Feed Forward: FC全连接线性变换</p> <p>shot-cut:解决深度学习中的退化问题</p></blockquote> <p><img src="https://img-blog.csdnimg.cn/20190407193820725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>在进一步探索其内部计算方式，我们可以将上面图层可视化为下图：</p> <p><img src="https://img-blog.csdnimg.cn/20190407193828541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>残差模块相信大家都很清楚了，这里不再讲解，主要讲解下Layer normalization。Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。</p> <p>说到 normalization，那就肯定得提到 Batch Normalization。BN的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。</p> <p>BN的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示：</p> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/20210709200749.png" alt="20210709200749"></p> <p>可以看到，右半边求均值是沿着数据 batch_size的方向进行的，其计算公式如下：</p> <p><img src="https://img-blog.csdnimg.cn/20190407194002495.png" alt="在这里插入图片描述"></p> <p>那么什么是 Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是在<strong>每一个样本上计算均值和方差</strong>，而不是BN那种在批方向计算均值和方差！</p> <p><font color="red">BN适合固定深度的网络，而LN则适合变长深度的网络；LN用于RNN比较好，而BN用于CNN比较好</font></p> <p><img src="https://img-blog.csdnimg.cn/20190407193953348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>下面看一下 LN 的公式：</p> <p><img src="https://img-blog.csdnimg.cn/2019040719394179.png" alt="在这里插入图片描述"></p> <p>到这里为止就是全部encoders的内容了，如果把两个encoders叠加在一起就是这样的结构，在self-attention需要强调的最后一点是其采用了残差网络中的short-cut结构，目的是解决深度学习中的退化问题。</p> <p><img src="https://img-blog.csdnimg.cn/20190407194033648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <h5 id="decoder层"><a href="#decoder层" class="header-anchor">#</a> Decoder层</h5> <p><img src="https://img-blog.csdnimg.cn/20190407194054634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>上图是transformer的一个详细结构，相比本文一开始结束的结构图会更详细些，接下来，我们会按照这个结构图讲解下decoder部分。</p> <p>可以看到decoder部分其实和encoder部分大同小异，不过在最下面额外多了一个masked mutil-head attetion，这里的mask也是transformer一个很关键的技术，我们一起来看一下。</p> <h5 id="mask"><a href="#mask" class="header-anchor">#</a> Mask</h5> <p>mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p> <p><font color="red"><strong>其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。</strong></font></p> <h5 id="padding-mask"><a href="#padding-mask" class="header-anchor">#</a> Padding Mask</h5> <p>什么是 padding mask 呢？因为<strong>每个批次输入序列长度是不一样的</strong>也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p> <p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！</p> <p>而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。</p> <h5 id="sequence-mask"><a href="#sequence-mask" class="header-anchor">#</a> Sequence mask</h5> <p>文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p> <p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p> <p>对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。
其他情况，attn_mask 一律等于 padding mask。</p> <hr> <p>编码器通过处理输入序列启动。然后将顶部编码器的输出转换为一组注意向量k和v。每个解码器将在其“encoder-decoder attention”层中使用这些注意向量，这有助于解码器将注意力集中在输入序列中的适当位置：</p> <p><img src="https://img-blog.csdnimg.cn/img_convert/3fad10cd1045205ba6ca8ed84a2153e6.png" alt="在这里插入图片描述"></p> <p>完成编码阶段后，我们开始解码阶段。解码阶段的每个步骤从输出序列（本例中为英语翻译句）输出一个元素。
以下步骤重复此过程，一直到达到表示解码器已完成输出的符号。每一步的输出在下一个时间步被送入底部解码器，解码器像就像我们对编码器输入所做操作那样，我们将<strong>位置编码</strong>嵌入并添加到这些解码器输入中，以表示每个字的位置。</p> <p><img src="https://img-blog.csdnimg.cn/img_convert/be8b8331026d84185cbd9a3e1358cd2d.png" alt="在这里插入图片描述"></p> <h5 id="输出层"><a href="#输出层" class="header-anchor">#</a> 输出层</h5> <p>当decoder层全部执行完毕后，怎么把得到的向量映射为我们需要的词呢，很简单，只需要在结尾再添加一个全连接层和softmax层，假如我们的词典是1w个词，那最终softmax会输入1w个词的概率，概率值最大的对应的词就是我们最终的结果。</p> <p><img src="https://img-blog.csdnimg.cn/20190407194120538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <h5 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h5> <ol><li>Self attention</li> <li>Multi-Head</li> <li>多册堆叠（embedding+position embedding=Input embedding），位置编码</li> <li>并行加速训练</li></ol> <h2 id="bert-bert就是从transformer中衍生出来的预训练语言模型"><a href="#bert-bert就是从transformer中衍生出来的预训练语言模型" class="header-anchor">#</a> Bert （<strong>BERT就是从transformer中衍生出来的预训练语言模型</strong>）</h2> <p>bert (Transformer是bert里面的一个结构)：类似长江后浪推前浪，是改进了的RNN</p> <p>Bidirectional Encoder Representations from Transformers</p> <p><strong>是一种表示学习：（w2v，graphembeding）</strong></p> <blockquote><p><strong>RNN缺点：</strong></p></blockquote> <ol><li><p>串行的状态，不能乱，必须依次执行，导致了训练速度慢</p></li> <li><p>RNN长期记忆能力有限。不能记忆很久之前的信息，所以使用LSTM，但是LSTM不是万能的，最近的消息权重较大</p></li></ol> <p>Bert就是基于Transformer构建的，这个模型广泛应用于NLP领域，例如<strong>机器翻译，问答系统，文本摘要和语音识别</strong>等等方向。</p> <hr> <blockquote><p>GPT是“Generative Pre-Training”的简称，是指的生成式的预训练。</p></blockquote> <p>从创新的角度来看，bert其实并没有过多的结构方面的创新点，其和GPT一样均是采用的transformer的结构，相对于GPT来说，其是双向结构的，而GPT是单向的，如下图所示</p> <p><img src="https://img-blog.csdnimg.cn/20190407194131428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>elmo：将上下文当作特征，但是无监督的语料和我们真实的语料还是有区别的，不一定的符合我们特定的任务，是一种双向的特征提取。</p> <p>openai gpt就做了一个改进，也是通过transformer学习出来一个语言模型，不是固定的，通过任务 finetuning,用transfomer代替elmo的lstm。
openai gpt其实就是缺少了encoder的transformer。当然也没了encoder与decoder之间的attention。</p> <p>openAI gpt虽然可以进行fine-tuning,但是有些特殊任务与pretraining输入有出入，单个句子与两个句子不一致的情况，很难解决，还有就是decoder只能看到前面的信息。
其次bert在多方面的nlp任务变现来看效果都较好，具备较强的泛化能力，对于特定的任务只需要添加一个输出层来进行fine-tuning即可。</p> <h4 id="结构"><a href="#结构" class="header-anchor">#</a> 结构</h4> <blockquote><p>Bert包括两个阶段:<font color="red">预训练阶段(Pre-train)、微调阶段(Fine Tuning)</font></p> <p>预训练：一个寻网络权值初值的过程，将pre-train的结果作为BP算法的权值的初值，能够解决深度网络在非凸目标函数上陷入局部最优的问题（没有目标，不考虑结果，无监督的学习：如，nlp，学习到单词与单词之间的关系，单词的表示）</p> <p>Fine Turning：用<strong>训练好的参数</strong>（可以从已训练好的模型中获得）初始化自己的网络，然后用自己的数据接着训练（专业目的性学习，有监督学习）</p> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
aa[Bert]--&gt;A[Pre-Train]
aa[Bert]--&gt;bb[Fine Tune]
A[Pre-Train]--&gt;B[语言模型-Masked LM]
A[Pre-Train]--&gt;C[下一个句子的预测-Next Seq Predict]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>**语言模型：**首先修改原文章中的句子。</p> <ol><li><p>80%的概率真的用[MASK]取代被选中的词：--解决完形填空
my dog is hairy-&gt; my dog is [MASK]</p></li> <li><p>10%的概率用一个随机词取代它：--解决纠错问题
my dog is hairy-&gt; my dog is apple</p></li> <li><p>10%的概率保持不变：
my dog is hairy-&gt; my dog is hairy</p></li></ol> <p><strong>Next Seq Predict</strong>：</p> <p>问答，出了上句，给出下句；一问一答。如果两句相关，训练给出yes；否则为no。</p> <p><strong>微调</strong>：<font color="red">前面的预训练就得到了向量化的表示,用向量化表示进行文本的初始化，然后做一个有监督的学习，就可以完成Fine tune了</font></p> <p>•句子对的分类任务
•单个句子的分类任务
•问答任务
•名命名实体识别</p></blockquote> <p>先看下bert的内部结构，官网最开始提供了两个版本，L表示的是transformer的层数，H表示输出的维度，A表示mutil-head attention的个数
<img src="https://img-blog.csdnimg.cn/20190407194141481.png" alt="在这里插入图片描述"></p> <p>如今已经增加了多个模型，中文是其中唯一一个非英语的模型。
<img src="https://img-blog.csdnimg.cn/20190407194149597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">
从模型的层数来说其实已经很大了，但是由于transformer的残差（residual）模块，层数并不会引起梯度消失等问题，但是并不代表层数越多效果越好，有论点认为低层偏向于语法特征学习，高层偏向于语义特征学习。</p> <h4 id="预训练模型"><a href="#预训练模型" class="header-anchor">#</a> 预训练模型</h4> <p>首先我们要了解一下什么是预训练模型，举个例子，假设我们有大量的维基百科数据，那么我们可以用这部分巨大的数据来训练一个泛化能力很强的模型，当我们需要在特定场景使用时，例如做文本相似度计算，那么，<strong>只需要简单的修改一些输出层，再用我们自己的数据进行一个增量训练，对权重进行一个轻微的调整。</strong></p> <p>预训练的好处在于在特定场景使用时不需要用大量的语料来进行训练，节约时间效率高效，bert就是这样的一个泛化能力较强的预训练模型。</p> <h4 id="bert的预训练过程"><a href="#bert的预训练过程" class="header-anchor">#</a> BERT的预训练过程</h4> <p>接下来我们看看BERT的预训练过程，BERT的预训练阶段包括两个任务，一个是<font color="red">Masked Language Model</font>，还有一个是<font color="red">Next Sentence Prediction</font></p> <h4 id="masked-language-model"><a href="#masked-language-model" class="header-anchor">#</a> Masked Language Model</h4> <p>MLM可以理解为完形填空，作者会<strong>随机mask每一个句子中15%的词</strong>，用其上下文来做预测，例如：<code>my dog is hairy → my dog is [MASK]</code></p> <p>此处将hairy进行了mask处理，然后采用非监督学习的方法预测mask位置的词是什么，但是该方法有一个问题，<strong>因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过</strong>，为了解决这个问题，作者做了如下的处理：</p> <ul><li>80%的<strong>时间</strong>是采用[mask]，my dog is hairy → my dog is [MASK]</li> <li>10%的<strong>时间</strong>是随机取一个词来代替mask的词，my dog is hairy -&gt; my dog is apple</li> <li>10%的时间保持不变，my dog is hairy -&gt; my dog is hairy</li></ul> <p>那么为啥要以一定的概率使用随机词呢？这是因为<strong>transformer要保持对每个输入token分布式的表征</strong>，否则Transformer很可能会记住这个[MASK]就是&quot;hairy&quot;。至于使用随机词带来的负面影响，文章中解释说,所有其他的token(即非&quot;hairy&quot;的token)共享15%*10% = 1.5%的概率，其影响是可以忽略不计的。Transformer全局的可视，又增加了信息的获取，但是不让模型获取全量信息。
注意：</p> <ul><li>有参数dupe_factor决定数据duplicate的次数。</li> <li>其中，create_instance_from_document函数，是构造了一个sentence-pair的样本。对每一句，先生成[CLS]+A+[SEP]+B+[SEP]，有长（0.9）有短（0.1），再加上mask，然后做成样本类object。</li> <li>create_masked_lm_predictions函数返回的tokens是已经被遮挡词替换之后的tokens</li> <li>masked_lm_labels则是遮挡词对应位置真实的label。</li></ul> <h4 id="next-sentence-prediction"><a href="#next-sentence-prediction" class="header-anchor">#</a> Next Sentence Prediction</h4> <p>选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA（<strong>问答</strong>）和NLI(<strong>自然语言推理</strong>)都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。
个人理解：</p> <ul><li><p>Bert先是用Mask来提高视野范围的信息获取量，增加duplicate再随机Mask，这样跟RNN类方法依次训练预测没什么区别了除了mask不同位置外；</p></li> <li><p>全局视野极大地降低了学习的难度，然后再用A+B/C来作为样本，这样每条样本都有50%的概率看到一半左右的噪声；</p></li> <li><p>但直接学习Mask A+B/C是没法学习的，因为不知道哪些是噪声，所以又加上next_sentence预测任务，与MLM <strong>(屏蔽语言模型)</strong> 同时进行训练，这样用next来辅助模型对噪声/非噪声的辨识，用MLM来完成语义的大部分的学习。<img src="https://img-blog.csdnimg.cn/20190625090614245.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <h4 id="输入"><a href="#输入" class="header-anchor">#</a> 输入</h4> <p>bert的输入可以是单一的一个句子或者是句子对，实际的输入值是segment embedding与position embedding相加，具体的操作流程可参考上面的transformer讲解。</p> <p>BERT的输入词向量是三个向量之和：</p> <p><strong>Token Embedding</strong>：WordPiece tokenization subword词向量。
<strong>Segment Embedding</strong>：表明这个词属于哪个句子（NSP需要两个句子）。
<strong>Position Embedding</strong>：学习出来的embedding向量。这与Transformer不同，Transformer中是预先设定好的值。</p> <p><img src="https://img-blog.csdnimg.cn/20190407194245766.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <h4 id="总结-2"><a href="#总结-2" class="header-anchor">#</a> 总结</h4> <p><img src="https://img-blog.csdnimg.cn/20190407194312495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">
BERT的去除实验表明，双向LM和NSP带了的提升最大。</p> <p><img src="https://img-blog.csdnimg.cn/20190623234641714.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">
另一个结论是，增加模型参数数量可以提升模型效果。</p> <p><img src="https://img-blog.csdnimg.cn/2019062323474479.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"> <font color="blue">BERT预训练模型的输出结果，无非就是一个或多个向量。</font>下游任务可以通过精调（改变预训练模型参数）或者特征抽取（不改变预训练模型参数，只是<strong>把预训练模型的输出作为特征输入</strong>到下游任务）两种方式进行使用。BERT原论文使用了<strong>精调方式</strong>，但也尝试了<strong>特征抽取</strong>方式的效果，比如在NER(<strong>命名实体识别</strong>)任务上，最好的特征抽取方式只比精调差一点点。<strong>但特征抽取方式的好处可以预先计算好所需的向量，存下来就可重复使用，极大提升下游任务模型训练的速度。</strong> <img src="https://img-blog.csdnimg.cn/20190623234803769.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">
后来也有其他人针对ELMo和BERT比较了这两种使用方式的精度差异。下面列出基本结论：</p> <p><img src="https://img-blog.csdnimg.cn/2019062323481629.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"> <img src="https://img-blog.csdnimg.cn/20190623234836973.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">
总结下BERT的主要贡献：</p> <ul><li>引入了<strong>Masked LM，使用双向LM做模型预训练</strong>。</li> <li>为预训练引入了新目标Next Sentence Prediction (<em>NSP</em>)，它可以学习句子与句子间的关系。</li> <li>进一步验证了更大的模型效果更好： 12 --&gt; 24 层。</li> <li>为下游任务引入了很通用的求解框架，不再为任务做模型定制。</li> <li>刷新了多项NLP任务的记录，引爆了NLP无监督预训练技术。</li></ul> <p>BERT是谷歌团队糅合目前已有的NLP知识集大成者，刷新11条赛道彰显了无与伦比的实力，且极容易被用于多种NLP任务。宛若一束烟花点亮在所有NLP从业者心中。更为可贵的是谷歌选择了开源这些，让所有从业者看到了在各行各业落地的更多可能性。</p> <p><strong>BERT优点</strong></p> <ul><li><em><strong>Transformer Encoder因为有Self-attention机制，因此BERT自带双向功能</strong></em></li> <li>因为双向功能以及多层Self-attention机制的影响，使得BERT必须使用Cloze版的语言模型Masked-LM来完成token级别的预训练</li> <li>为了获取比词更高级别的句子级别的语义表征，BERT加入了Next Sentence Prediction来和Masked-LM一起做联合训练</li> <li>为了适配多任务下的迁移学习，BERT设计了更通用的输入层和输出层</li> <li>微调成本小</li></ul> <p><strong>BERT缺点</strong></p> <ul><li>task1的随机遮挡策略略显粗犷，推荐阅读《Data Nosing As Smoothing In Neural Network Language Models》</li> <li>[MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现;</li> <li>每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token）</li> <li>BERT对硬件资源的消耗巨大（大模型需要16个tpu，历时四天；更大的模型需要64个tpu，历时四天。</li></ul> <p>关于BERT最新的各领域应用推荐张俊林的<a href="https://zhuanlan.zhihu.com/p/68446772" target="_blank" rel="noopener noreferrer">Bert时代的创新（应用篇）<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>思考</strong></p> <ul><li>个人并不认为文章是模型的改进，更认可为任务的设计改进。</li> <li>论文作者只比较了有没有task1的影响，并没有针对task2对比试验。提升是否来自好的预训练任务设计没有明说。</li> <li>bert对nlp领域目前已有知识的有效“整合”，在硬件配置足够的情况下能提高nlp多领域性能</li></ul> <p><strong>BERT适用场景</strong></p> <p>第一，如果NLP任务偏向在<strong>语言本身中就包含答案，而不特别依赖文本外的其它特征</strong>，往往应用Bert能够极大提升应用效果。典型的任务比如QA和阅读理解，正确答案更偏向对语言的理解程度，理解能力越强，解决得越好，不太依赖语言之外的一些判断因素，所以效果提升就特别明显。反过来说，对于某些任务，除了文本类特征外，其它特征也很关键，比如搜索的用户行为／链接分析／内容质量等也非常重要，所以Bert的优势可能就不太容易发挥出来。再比如，推荐系统也是类似的道理，Bert可能只能对于文本内容编码有帮助，其它的用户行为类特征，不太容易融入Bert中。</p> <p>第二，Bert特别适合解决<strong>句子或者段落的匹配类任务</strong>。就是说，Bert特别适合用来解决判断句子关系类问题，这是相对单文本分类任务和序列标注等其它典型NLP任务来说的，很多实验结果表明了这一点。而其中的原因，我觉得很可能主要有两个，一个原因是：很可能是因为Bert在预训练阶段增加了Next Sentence Prediction任务，所以能够在预训练阶段学会一些句间关系的知识，而如果下游任务正好涉及到句间关系判断，就特别吻合Bert本身的长处，于是效果就特别明显。第二个可能的原因是：因为Self Attention机制自带句子A中单词和句子B中任意单词的Attention效果，而这种细粒度的匹配对于句子匹配类的任务尤其重要，所以Transformer的本质特性也决定了它特别适合解决这类任务。</p> <p>从上面这个<strong>Bert的擅长处理句间关系类任务的特性</strong>，我们可以继续推理出以下观点：</p> <p>既然预训练阶段增加了Next Sentence Prediction任务，就能对下游类似性质任务有较好促进作用，那么是否可以继续在预训练阶段加入其它的新的辅助任务？而这个辅助任务如果具备一定通用性，可能会对一类的下游任务效果有直接促进作用。这也是一个很有意思的探索方向，当然，这种方向因为要动Bert的第一个预训练阶段，所以属于NLP届土豪们的工作范畴，穷人们还是散退、旁观、鼓掌、叫好为妙。</p> <p>第三，Bert的适用场景，与NLP任务对<strong>深层语义特征</strong>的需求程度有关。感觉越是需要深层语义特征的任务，越适合利用Bert来解决；而对有些NLP任务来说，浅层的特征即可解决问题，典型的浅层特征性任务比如分词，POS词性标注，NER，文本分类等任务，这种类型的任务，只需要较短的上下文，以及浅层的非语义的特征，貌似就可以较好地解决问题，所以Bert能够发挥作用的余地就不太大，有点杀鸡用牛刀，有力使不出来的感觉。</p> <p>这很可能是因为Transformer层深比较深，所以可以逐层捕获不同层级不同深度的特征。于是，对于需要语义特征的问题和任务，Bert这种深度捕获各种特征的能力越容易发挥出来，而浅层的任务，比如分词／文本分类这种任务，也许传统方法就能解决得比较好，因为任务特性决定了，要解决好它，不太需要深层特征。</p> <p>第四，Bert比较<strong>适合解决输入长度不太长的NLP任务</strong>，而输入比较长的任务，典型的比如文档级别的任务，Bert解决起来可能就不太好。主要原因在于：Transformer的self attention机制因为要对任意两个单词做attention计算，所以时间复杂度是n平方，n是输入的长度。如果输入长度比较长，Transformer的训练和推理速度掉得比较厉害，于是，这点约束了Bert的输入长度不能太长。所以对于输入长一些的文档级别的任务，Bert就不容易解决好。结论是：Bert更适合解决句子级别或者段落级别的NLP任务。</p></li></ul> <hr></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">2 years ago</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#hadoop" class="sidebar-link reco-side-hadoop" data-v-70334359>hadoop</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#spark-需要先启动hadoop" class="sidebar-link reco-side-spark-需要先启动hadoop" data-v-70334359>Spark(需要先启动hadoop)</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#spark3种模式" class="sidebar-link reco-side-spark3种模式" data-v-70334359>Spark3种模式</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#本地local" class="sidebar-link reco-side-本地local" data-v-70334359>本地local</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#standalone" class="sidebar-link reco-side-standalone" data-v-70334359>Standalone</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#yarn" class="sidebar-link reco-side-yarn" data-v-70334359>Yarn</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#回顾-任务运行中的一些基本概念" class="sidebar-link reco-side-回顾-任务运行中的一些基本概念" data-v-70334359>回顾：任务运行中的一些基本概念</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#怎么调试spark" class="sidebar-link reco-side-怎么调试spark" data-v-70334359>怎么调试spark</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#transformer一些算子" class="sidebar-link reco-side-transformer一些算子" data-v-70334359>Transformer一些算子</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#action一些算子" class="sidebar-link reco-side-action一些算子" data-v-70334359>Action一些算子</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#spark的宽窄依赖" class="sidebar-link reco-side-spark的宽窄依赖" data-v-70334359>spark的宽窄依赖</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#在spark中创建rdd的创建方式" class="sidebar-link reco-side-在spark中创建rdd的创建方式" data-v-70334359>在Spark中创建RDD的创建方式</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#回顾" class="sidebar-link reco-side-回顾" data-v-70334359>回顾</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#推荐算法" class="sidebar-link reco-side-推荐算法" data-v-70334359>推荐算法</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#召回-粗排" class="sidebar-link reco-side-召回-粗排" data-v-70334359>召回（粗排）：</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#排序" class="sidebar-link reco-side-排序" data-v-70334359>排序：</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#精排" class="sidebar-link reco-side-精排" data-v-70334359>精排</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#前缀树" class="sidebar-link reco-side-前缀树" data-v-70334359>前缀树</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#语言模型" class="sidebar-link reco-side-语言模型" data-v-70334359></a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#马尔科夫模型" class="sidebar-link reco-side-马尔科夫模型" data-v-70334359>马尔科夫模型</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#隐马尔可夫模型" class="sidebar-link reco-side-隐马尔可夫模型" data-v-70334359>隐马尔可夫模型</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#nlp发展史" class="sidebar-link reco-side-nlp发展史" data-v-70334359>NLP发展史</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#dnn" class="sidebar-link reco-side-dnn" data-v-70334359>DNN</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#cnn" class="sidebar-link reco-side-cnn" data-v-70334359>CNN</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#rnn" class="sidebar-link reco-side-rnn" data-v-70334359>RNN</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#rnn-vs-dnn" class="sidebar-link reco-side-rnn-vs-dnn" data-v-70334359>RNN vs DNN</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#rnn-2" class="sidebar-link reco-side-rnn-2" data-v-70334359>RNN</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#brnn" class="sidebar-link reco-side-brnn" data-v-70334359>BRNN</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#lstm" class="sidebar-link reco-side-lstm" data-v-70334359>LSTM</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#双向lstm" class="sidebar-link reco-side-双向lstm" data-v-70334359>双向LSTM</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#transformer" class="sidebar-link reco-side-transformer" data-v-70334359>Transformer</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#seq2seq" class="sidebar-link reco-side-seq2seq" data-v-70334359>Seq2Seq</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#attention-一个机制-方法" class="sidebar-link reco-side-attention-一个机制-方法" data-v-70334359>attention（一个机制，方法）</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#self-attention模型" class="sidebar-link reco-side-self-attention模型" data-v-70334359>Self Attention模型</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#transformer-是一个模型-rnn-lstm" class="sidebar-link reco-side-transformer-是一个模型-rnn-lstm" data-v-70334359>Transformer(是一个模型，RNN，LSTM)</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/DisLearn.html#bert-bert就是从transformer中衍生出来的预训练语言模型" class="sidebar-link reco-side-bert-bert就是从transformer中衍生出来的预训练语言模型" data-v-70334359>Bert （BERT就是从transformer中衍生出来的预训练语言模型）</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><canvas id="vuepress-canvas-cursor"></canvas><!----><div class="vuepress-canvas-nest-element"></div><div class="kanbanniang" data-v-27e9bfa4><div class="banniang-container" style="display:;" data-v-27e9bfa4><div class="messageBox" style="position:fixed;right:75px;bottom:235px;opacity:0.75;height:max-content;width:200px;fon-szie:16px;display:none;" data-v-27e9bfa4></div> <div class="operation" style="display:;" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660425629" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6044" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M577.5584 307.848533l-21.345067-18.699733c-0.062933-0.061867-0.186667-0.123733-0.280533-0.186667l-44.305067-38.689067-53.752533 47.5648L127.009067 587.424l0 177.1392 0 155.0048c0 45.886933 37.454933 83.0976 83.610667 83.0976l183.966933 0L394.586667 735.8688c0-27.512533 22.448-49.8336 50.162133-49.8336l133.7728 0c27.714133 0 50.178133 22.321067 50.178133 49.8336L628.699733 1002.666667l183.966933 0c46.170667 0 83.610667-37.211733 83.610667-83.0976L896.277333 763.9424 896.277333 586.7712 578.5216 308.688 577.5584 307.848533z" p-id="6045" data-v-27e9bfa4></path> <path d="M990.637867 418.164267l-94.360533-82.600533 0-181.290667c0-36.714667-29.952-66.482133-66.894933-66.482133-36.941867 0-66.893867 29.767467-66.893867 66.482133l0 64.197333L556.213333 37.911467c-25.291733-22.103467-63.165867-22.103467-88.4256 0L33.348267 418.164267c-27.730133 24.247467-30.402133 66.264533-5.9808 93.808 24.437333 27.544533 66.692267 30.219733 94.407467 5.938133L512 176.376533l390.2432 341.533867c12.7072 11.130667 28.4608 16.600533 44.181333 16.600533 18.549333 0 37.0048-7.617067 50.209067-22.538667C1021.054933 484.4128 1018.382933 442.4128 990.637867 418.164267z" p-id="6046" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660394444" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5299" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M0 202.7V631c0 83.3 68.3 150.7 152.6 150.7h228.9l8 190.3 224.9-190.3h257c84.3 0 152.6-67.4 152.6-150.7V202.7C1024 119.4 955.7 52 871.4 52H152.6C68.3 52 0 119.4 0 202.7z m658.6 237.9c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S771 512 730.9 512c-40.2 0-72.3-31.7-72.3-71.4z m-220.9 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S550.1 512 510 512c-40.2 0-72.3-31.7-72.3-71.4z m-216.8 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S333.3 512 293.1 512c-40.1 0-72.2-31.7-72.2-71.4z" p-id="5300" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660570409" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2153" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 393.846154c-86.646154 0-157.538462 70.892308-157.538462 157.538461s70.892308 157.538462 157.538462 157.538462 157.538462-70.892308 157.538462-157.538462-70.892308-157.538462-157.538462-157.538461z m393.846154-118.153846h-102.4c-27.569231 0-51.2-13.784615-66.953846-35.446154l-45.292308-68.923077C677.415385 137.846154 643.938462 118.153846 608.492308 118.153846h-192.984616c-35.446154 0-68.923077 19.692308-84.676923 53.169231l-45.292307 68.923077c-13.784615 21.661538-39.384615 35.446154-66.953847 35.446154H118.153846c-43.323077 0-78.769231 35.446154-78.769231 78.76923v472.615385c0 43.323077 35.446154 78.769231 78.769231 78.769231h787.692308c43.323077 0 78.769231-35.446154 78.769231-78.769231V354.461538c0-43.323077-35.446154-78.769231-78.769231-78.76923zM512 787.692308c-129.969231 0-236.307692-106.338462-236.307692-236.307693s106.338462-236.307692 236.307692-236.307692 236.307692 106.338462 236.307692 236.307692-106.338462 236.307692-236.307692 236.307693z" p-id="2154" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660469241" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6553" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M706.544835 64.021106h-6.500041a159.889547 159.889547 0 0 0-83.558068 23.557532c-54.583153 33.422204-86.949304 40.439014-104.486726 40.439014-17.538445 0-49.903573-7.016811-104.494912-40.445154a159.88136 159.88136 0 0 0-83.550905-23.551392h-6.507204a127.823224 127.823224 0 0 0-97.182366 44.702108l-172.836417 201.635323c-19.522636 22.774703-20.600177 56.050574-2.609431 80.047104l95.995331 127.994116a63.99757 63.99757 0 0 0 83.198887 17.024745v328.558038c0 52.93256 43.060725 95.995331 95.995331 95.995331h415.98011c52.934606 0 95.995331-43.062771 95.995331-95.995331V535.424502a64.028269 64.028269 0 0 0 42.240033 7.749498 64.013943 64.013943 0 0 0 46.990221-34.528398l63.996546-127.856993c11.522428-23.027459 8.125051-50.721195-8.632611-70.278623L803.743574 108.74675c-24.335245-28.421306-59.770292-44.725644-97.198739-44.725644z" p-id="6554" data-v-27e9bfa4></path></svg></i>
      <a target="_blank" href="https://github.com/kii-chan-iine" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660325062" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3809" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 3.413333c280.849067 0 508.586667 199.273813 508.586667 444.94848 0 140.427947-74.519893 265.53344-190.65856 347.11552V1020.586667l-222.839467-135.168c-30.859947 5.147307-62.552747 8.021333-95.085227 8.021333-280.845653 0-508.586667-199.28064-508.586666-445.078187C3.413333 202.687147 231.150933 3.413333 512 3.413333z m-158.96576 603.921067h317.805227c17.578667 0 31.812267-14.2336 31.812266-31.819093a31.798613 31.798613 0 0 0-31.812266-31.80544h-317.805227c-17.578667 0-31.812267 14.2336-31.812267 31.80544 0.116053 17.585493 14.349653 31.819093 31.812267 31.819093z m-63.511893-190.665387h444.951893c17.578667 0 31.812267-14.2336 31.812267-31.812266a31.802027 31.802027 0 0 0-31.812267-31.81568H289.522347a31.802027 31.802027 0 0 0-31.81568 31.81568c0 17.578667 14.2336 31.812267 31.81568 31.812266z" p-id="3810" data-v-27e9bfa4></path></svg></i></a> <i data-v-27e9bfa4><svg t="1572660347392" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4543" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 34.133333a486.4 486.4 0 1 0 486.4 486.4A486.4 486.4 0 0 0 512 34.133333z m209.4848 632.8064l-55.6032 55.466667-151.517867-151.125333-151.517866 151.1168-55.6032-55.466667 151.517866-151.108267L307.242667 364.714667l55.6032-55.466667 151.517866 151.125333 151.517867-151.1168 55.6032 55.466667-151.517867 151.099733z m0 0" p-id="4544" data-v-27e9bfa4></path></svg></i></div> <canvas id="banniang" width="216" height="281.6" class="live2d" style="position:fixed;right:90px;bottom:-20px;opacity:1;" data-v-27e9bfa4></canvas></div> <div class="showBanNiang" style="display:none;" data-v-27e9bfa4>
    看板娘
  </div></div><!----><div></div></div></div>
    <script src="/assets/js/app.91cde711.js" defer></script><script src="/assets/js/3.a67abeb3.js" defer></script><script src="/assets/js/1.c373aa88.js" defer></script><script src="/assets/js/24.b604a735.js" defer></script><script src="/assets/js/11.747f0d2b.js" defer></script>
  </body>
</html>
