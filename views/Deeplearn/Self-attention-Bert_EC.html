<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer &amp; Bert | KII IINE</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="manifest" href="/manifest.json">
    <link rel="apple-touch-icon" href="/icons/apple-touch-icon-152x152.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2c2c2c">
    <meta name="description" content="明早一起去看海 望向未来">
    <meta name="theme-color" content="#22979b">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#22979b">
    <meta name="msapplication-TileImage" content="/icons/msapplication-icon-144x144.png">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="google-site-verification" content="XCppppl60fPQTlwxDodwZIhMarkybEgwVpcEz85KTuQ">
    
    <link rel="preload" href="/assets/css/0.styles.c67cb200.css" as="style"><link rel="preload" href="/assets/js/app.c5936fd9.js" as="script"><link rel="preload" href="/assets/js/3.6c2c0b5f.js" as="script"><link rel="preload" href="/assets/js/1.06133022.js" as="script"><link rel="preload" href="/assets/js/22.e5025c86.js" as="script"><link rel="preload" href="/assets/js/11.6a0e9e65.js" as="script"><link rel="prefetch" href="/assets/js/10.0c65cdf0.js"><link rel="prefetch" href="/assets/js/12.4be1983b.js"><link rel="prefetch" href="/assets/js/13.7d5668c8.js"><link rel="prefetch" href="/assets/js/14.dbeeea81.js"><link rel="prefetch" href="/assets/js/15.091fc643.js"><link rel="prefetch" href="/assets/js/16.b5f05fbd.js"><link rel="prefetch" href="/assets/js/17.f64eb964.js"><link rel="prefetch" href="/assets/js/18.4b5864ac.js"><link rel="prefetch" href="/assets/js/19.37992306.js"><link rel="prefetch" href="/assets/js/20.0334c785.js"><link rel="prefetch" href="/assets/js/21.b2e767b9.js"><link rel="prefetch" href="/assets/js/23.422f052b.js"><link rel="prefetch" href="/assets/js/24.ace6c420.js"><link rel="prefetch" href="/assets/js/25.fc5f867b.js"><link rel="prefetch" href="/assets/js/26.92af3e0b.js"><link rel="prefetch" href="/assets/js/27.a3859aa5.js"><link rel="prefetch" href="/assets/js/28.c97e6d38.js"><link rel="prefetch" href="/assets/js/29.783c6d22.js"><link rel="prefetch" href="/assets/js/30.49abcc5f.js"><link rel="prefetch" href="/assets/js/31.7379248d.js"><link rel="prefetch" href="/assets/js/32.b19ad03d.js"><link rel="prefetch" href="/assets/js/33.db85dbe1.js"><link rel="prefetch" href="/assets/js/34.71e32573.js"><link rel="prefetch" href="/assets/js/35.a5e2bc82.js"><link rel="prefetch" href="/assets/js/4.7b5c4b79.js"><link rel="prefetch" href="/assets/js/5.2bef9b18.js"><link rel="prefetch" href="/assets/js/6.45c24d02.js"><link rel="prefetch" href="/assets/js/7.51de8bdd.js"><link rel="prefetch" href="/assets/js/8.1d292254.js"><link rel="prefetch" href="/assets/js/9.49750083.js">
    <link rel="stylesheet" href="/assets/css/0.styles.c67cb200.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>KII IINE</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>明早一起去看海 望向未来</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/favicon.ico" alt="KII IINE" class="logo"> <span class="site-name">KII IINE</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/avatar.jpeg" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    KII IINE
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>18</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>11</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/深度学习/" class="nav-link"><i class="undefined"></i>
  深度学习
</a></li><li class="dropdown-item"><!----> <a href="/categories/CV/" class="nav-link"><i class="undefined"></i>
  CV
</a></li><li class="dropdown-item"><!----> <a href="/categories/Exp/" class="nav-link"><i class="undefined"></i>
  Exp
</a></li><li class="dropdown-item"><!----> <a href="/categories/闲言碎语/" class="nav-link"><i class="undefined"></i>
  闲言碎语
</a></li><li class="dropdown-item"><!----> <a href="/categories/Hadoop/" class="nav-link"><i class="undefined"></i>
  Hadoop
</a></li><li class="dropdown-item"><!----> <a href="/categories/thinks/" class="nav-link"><i class="undefined"></i>
  thinks
</a></li><li class="dropdown-item"><!----> <a href="/categories/Music/" class="nav-link"><i class="undefined"></i>
  Music
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/project/" class="nav-link"><i class="iconfont icon-project"></i>
  Project
</a></div><div class="nav-item"><a href="/timeLine/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/kii-chan-iine" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li><li class="dropdown-item"><!----> <a href="mailto:kaichen1993@hotmail.com" class="nav-link external"><i class="iconfont icon-Gmail"></i>
  Mail
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Transformer &amp; Bert</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>KII IINE</span>
            
          <span data-v-4e82dffc>2021 - </span>
          2022
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Transformer &amp; Bert</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>kii</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>7/20/2021</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>NLP</span><span class="tag-item" data-v-1ff7123e>deeplearn</span></i></div></div> <div class="theme-reco-content content__default"><div id="boxx" data-v-f4ca0dac><div data-v-f4ca0dac><p v-if="true" class="custom-block-title" data-v-f4ca0dac></p> <p v-if="true" data-v-f4ca0dac></p></div></div> <div class="custom-block tip"><p class="title">前言</p><p>学习札记。</p></div> <h1 id="transformer-是一个模型-rnn-lstm"><a href="#transformer-是一个模型-rnn-lstm" class="header-anchor">#</a> Transformer(是一个模型，RNN，LSTM)</h1> <p>可用于机器翻译，转写等</p> <blockquote><ul><li>输入：我爱中国</li> <li>输出： I Love China</li></ul> <p>Input Embedding ：我，爱，中国 这三个词的词向量(word2vector等词嵌入方法获得)</p> <p>Positional Encoding: 利用词的位置和长度对位置进行编码，代码实现可在Google开源的算法中<code>get_timing_signal_1d()</code>函数找到。</p> <p>词向量与位置编码相加，就能给每个词赋上位置信息。</p> <p><strong>模型执行</strong></p> <p><strong>Encoder步骤：</strong></p> <p>每次的输入都是我，爱，中国 这三个词的词向量和其位置编码的和。</p> <p><strong>Decoder步骤(<strong>每一步都预测一个词</strong>):</strong></p> <p><strong>Step 1</strong></p> <ul><li><ul><li>Outputs： 起始符 + Positional Encoding（位置编码）</li> <li>输出最大概率的词：“I”</li></ul></li></ul> <p><strong>Step 2</strong></p> <ul><li><ul><li>初始输入：起始符 + “I”+ Positonal Encoding</li> <li>输出最大概率的词：“Love”</li></ul></li></ul> <p><strong>Step 3</strong></p> <ul><li><ul><li>初始输入：起始符 + “I”+ “Love”+ Positonal Encoding</li> <li>最终输出：产生预测“China”</li></ul></li></ul></blockquote> <h2 id="整体框架"><a href="#整体框架" class="header-anchor">#</a> 整体框架</h2> <p>和Attention一样，Transformer模型中也采用了 encoer-decoder 架构。但其结构相比于Attention更加复杂，论文中encoder层由6个encoder堆叠在一起，decoder层也一样。</p> <img src="https://img-blog.csdnimg.cn/20190407193306430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;"> <p>每一个encoder和decoder的内部简版结构如下图</p> <img src="https://img-blog.csdnimg.cn/2019040719332630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:80%;"> <p>对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。<strong>decoder也包含encoder提到的两层网络，但是在这两层中间还有一层<font color="red">Encoder-Decoder attention层</font>，帮助当前节点获取到当前需要关注的重点内容。</strong></p> <p>现在我们知道了模型的主要组件，接下来我们看下模型的内部细节。首先，<font color="green">模型需要对输入的数据进行一个<strong>Positional Embedding</strong>操作，（也可以理解为类似w2v的操作，考虑词在句子中的位置顺序关系）</font>，enmbedding结束之后，输入到encoder层，self-attention处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个encoder。</p> <p><img src="https://img-blog.csdnimg.cn/20190407193828541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>再宏观缩放一下</p> <p><img src="https://img-blog.csdnimg.cn/20190407194033648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p>换个方式，Multi-head attention(一次初始化多个KVQ)</p> <p><img src="https://img-blog.csdnimg.cn/20190407194054634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <h2 id="positional-embedding"><a href="#positional-embedding" class="header-anchor">#</a> <strong>Positional Embedding</strong></h2> <p>类似加入位置信息</p> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/image-20210719223938607.png" alt="image-20210719223938607"></p> <h3 id="具体"><a href="#具体" class="header-anchor">#</a> 具体</h3> <p>到目前为止，transformer模型中还缺少一种<strong>解释输入序列中单词顺序</strong>的方法。为了处理这个问题，transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下
$$
PE=(pos,2i)=sin(pos/10000^{2i}/d_model)\
PE=(pos,2i+1)=cos(pos/10000^{2i}/d_model)
$$</p> <p>其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。</p> <h2 id="self-attention"><a href="#self-attention" class="header-anchor">#</a> Self-Attention</h2> <p>例子分为以下步骤：</p> <ol><li>准备输入</li> <li>初始化权重</li> <li>导出key, query and value的表示</li> <li>计算输入1 的注意力得分(attention scores)</li> <li>计算softmax</li> <li>将attention scores乘以value</li> <li>对加权后的value求和以得到输出1</li> <li>对输入2重复步骤4–7</li></ol> <p><strong>Note:</strong></p> <p>实际上，数学运算是向量化的，即所有输入都一起进行数学运算。我们稍后会在“代码”部分中看到此信息。</p> <h3 id="_1-准备输入"><a href="#_1-准备输入" class="header-anchor">#</a> <strong>1 准备输入</strong></h3> <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzNG82WjZkdmxEVmtaa0M0ZEZLOWRBVGc5OVRLSk5aWHNtNGJrWGcyV0xzOW1uR1M3WTk3YmFRLzY0MA?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.1: Prepare inputs</p> <p>在本教程中，我们从3个输入开始，每个输入的尺寸为4。</p> <div class="language-css line-numbers-mode"><pre class="language-css"><code>    Input 1<span class="token punctuation">:</span> [1<span class="token punctuation">,</span> 0<span class="token punctuation">,</span> 1<span class="token punctuation">,</span> 0]     Input 2<span class="token punctuation">:</span> [0<span class="token punctuation">,</span> 2<span class="token punctuation">,</span> 0<span class="token punctuation">,</span> 2]    Input 3<span class="token punctuation">:</span> [1<span class="token punctuation">,</span> 1<span class="token punctuation">,</span> 1<span class="token punctuation">,</span> 1]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="_2-初始化权重"><a href="#_2-初始化权重" class="header-anchor">#</a> <strong>2 初始化权重</strong></h3> <p>每个输入必须具有三个表示形式（请参见下图）。这些表示称为key（橙色），`query（红色）和value（紫色）。在此示例中，假设我们希望这些表示的尺寸为3。由于每个输入的尺寸均为4，这意味着每组权重的形状都必须为4×3。</p> <p><strong>Note:</strong></p> <p>稍后我们将看到value的维度也就是输出的维度。</p> <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzWW04N3VueHRiVVh6cHpndnN3UnBUSFFLeXNxY2liSWt6N0ZCZzM1ZXI3QVh2VTBFckJSclIwUS82NDA?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.2: Deriving key, query and value representations from each input</p> <p>为了获得这些表示，将每个输入（绿色）乘以一组用于key的权重，另一组用于query的权重和一组value的权重。在我们的示例中，我们如下初始化三组权重。</p> <p><strong>key的权重</strong></p> <div class="language-json line-numbers-mode"><pre class="language-json"><code>    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><strong>query的权重</strong></p> <div class="language-json line-numbers-mode"><pre class="language-json"><code>    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><strong>value的权重</strong></p> <div class="language-json line-numbers-mode"><pre class="language-json"><code>    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>     <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><strong>Note:</strong></p> <p>在神经网络的设置中，这些权重通常是很小的数，使用适当的随机分布（如高斯，Xavie 和 Kaiming 分布）随机初始化。初始化在训练之前完成一次。</p> <h3 id="_3-从每个输入中导出key-query-and-value的表示"><a href="#_3-从每个输入中导出key-query-and-value的表示" class="header-anchor">#</a> <strong>3 从每个输入中导出key, query and value的表示</strong></h3> <p>现在我们有了三组值的权重，让我们实际查看每个输入的键，查询和值表示形式。</p> <p><strong>输入 1 的key的表示形式</strong></p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> x <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>



                   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>使用相同的权重集获得输入 2 的key的表示形式：</p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> x <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>



                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>



                   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>使用相同的权重集获得输入 3 的key的表示形式：</p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> x <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>



                   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>一种更快的方法是对上述操作进行矩阵运算：</p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> x <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzbDJwYVUydndPUEJpY1hscHdhdXdBdXRsM21rNkJ5SktUS3l3eGgzWXB6RHRvaDRYVnV5aWE5ZncvNjQw?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.3a: Derive key representations from each input</p> <p>让我们做同样的事情以获得每个输入的value表示形式：</p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>                   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> 



    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> x <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzY0pCUWliNWljVE5rTkpBUHhuWjFkdmNsZEpGUmp2TUVyNXE0QXFpYnhkZVNZR1o3aWM3Rkh3dTEzZy82NDA?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.3b: Derive value representations from each input</p> <p>以及query的表示形式:</p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>                   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> x <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>   <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzcjN6QU5nRmRpYkNpYWxkM1Z4cnFTajBlak1SQm92U3dWT0lSeEY1WTRiTnBpYmZiTmRuUEhMZkJRLzY0MA?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.3c: Derive query representations from each input</p> <p><strong>Notes:</strong></p> <p>实际上，可以将偏差向量b添加到矩阵乘法的乘积中。</p> <p>译者注:y=w·x+b</p> <h3 id="_4-计算输入的注意力得分-attention-scores"><a href="#_4-计算输入的注意力得分-attention-scores" class="header-anchor">#</a> <strong>4 计算输入的注意力得分(attention scores)</strong></h3> <p>为了获得注意力分数，我们首先在输入1的query（红色）与所有key（橙色）（包括其自身）之间取点积。由于有3个key表示（因为我们有3个输入），因此我们获得3个注意力得分（蓝色）。</p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>                <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>



    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> x <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>



                <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzY0JQOVVNVTE0eWljUTVrMVRUT3F1WXdzdmVGeEt0ak00ZEN6YjlsS3VXZEpDalExdEM4OFZDQS82NDA?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.4: Calculating attention scores (blue) from query 1</p> <p>请注意，在这里我们仅使用输入1的query。稍后，我们将对其他查询重复相同的步骤。</p> <p><strong>Note:</strong></p> <p>上面的操作被称为&quot;点积注意力&quot;，是几种sorce之一。其他评分功能包括缩放的点积和拼接。</p> <p>更多：</p> <p>sorce:https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3</p> <h3 id="_5-计算softmax"><a href="#_5-计算softmax" class="header-anchor">#</a> <strong>5 计算softmax</strong></h3> <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzZTFnU0JlQmFjcVZXeFdvaWExS3A5clUyUjNLaWM0YmQ2WFRVNTh6WWliZWlheER3UjVEQnA4NGpaQS82NDA?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.5: Softmax the attention scores (blue)</p> <p>将attention scores通过 softmax 函数(蓝色)得到概率</p> <div class="language-apache line-numbers-mode"><pre class="language-text"><code>    softmax([2, 4, 4]) = [0.0, 0.5, 0.5]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="_6-将attention-scores乘以value"><a href="#_6-将attention-scores乘以value" class="header-anchor">#</a> <strong>6 将attention scores乘以value</strong></h3> <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzb2lheGI2RUM3QjlPVDBsSFY2dGw3S1FyRWdlaWFXZlZHMmxSMUhjYmJSVGVBTktBUXIxaE5zNUEvNjQw?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.6: Derive weighted value representation (yellow) from multiply value(purple) and score (blue)</p> <p>每个输入的softmax注意力得分（蓝色）乘以其相应的value（紫色）。这将得到3个对齐的向量（黄色）。在本教程中，我们将它们称为&quot;加权值&quot;。</p> <div class="language-http line-numbers-mode"><pre class="language-http"><code>    1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]



    2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]



    3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h3 id="_7-对加权后的value求和以得到输出1"><a href="#_7-对加权后的value求和以得到输出1" class="header-anchor">#</a> <strong>7 对加权后的value求和以得到输出1</strong></h3> <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzb2lheGI2RUM3QjlPVDBsSFY2dGw3S1FyRWdlaWFXZlZHMmxSMUhjYmJSVGVBTktBUXIxaE5zNUEvNjQw?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.7: Sum all weighted values (yellow) to get Output 1 (dark green)</p> <p>对所有加权值(黄色)按元素求和：</p> <div class="language-cs line-numbers-mode"><pre class="language-cs"><code>      <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span>



    <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span>



    <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">1.5</span><span class="token punctuation">]</span>



    <span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">--</span><span class="token operator">-</span>



    <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">7.0</span><span class="token punctuation">,</span> <span class="token number">1.5</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>得到的向量[2.0, 7.0, 1.5] (深绿)是输出 1 , 它是基于“输入1”的“query表示的形式” 与所有其他key(包括其自身）进行的交互。</p> <h3 id="_8-对输入2重复步骤4-7"><a href="#_8-对输入2重复步骤4-7" class="header-anchor">#</a> <strong>8 对输入2重复步骤4–7</strong></h3> <p>现在我们已经完成了输出1，我们将对输出2和输出3重复步骤4至7。我相信我可以让您自己进行操作????????。</p> <p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2dpZi9pYVRhOHV0NkhpYXdEMVU3TkF4eGVCdzhWRURyT0paaDYzQ2dhdmN2Y3JNcVFLWnZCOGdBSnFBdGxsZmNHTnBMaWNLQWtsaWM1NmhReGZyWXZsb1hqSHpqMlEvNjQw?x-oss-process=image/format,png" alt="img"></p> <p>Fig. 1.8: Repeat previous steps for Input 2 &amp; Input 3</p> <p>Notes:</p> <p>因为点积得分函数 query和key的维度必须始终相同.但是value的维数可能与query和key的维数不同。因此输出结果将遵循value的维度。</p> <h3 id="_9-来个总结"><a href="#_9-来个总结" class="header-anchor">#</a> 9 来个总结</h3> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/image-20210719225018921.png" alt="image-20210719225018921"></p> <h3 id="代码"><a href="#代码" class="header-anchor">#</a> <strong>代码</strong></h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># %% 准备输入</span>
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">import</span> softmax
<span class="token keyword">import</span> torch
x <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Input 1</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># Input 2</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># Input 3</span>
<span class="token punctuation">]</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment"># %% 初始化权重</span>

w_key <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span>
w_query <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span>
w_value <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span>
w_key <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>w_key<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
w_query <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>w_query<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
w_value <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>w_value<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>


<span class="token comment"># %% 导出KQV的表示</span>
keys <span class="token operator">=</span> x @ w_key
querys <span class="token operator">=</span> x @ w_query
values <span class="token operator">=</span> x @ w_value
<span class="token keyword">print</span><span class="token punctuation">(</span>keys<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>querys<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>values<span class="token punctuation">)</span>
<span class="token comment"># %%  计算输入的注意力得分(attention scores)</span>
attn_scores <span class="token operator">=</span> querys @ keys<span class="token punctuation">.</span>T
<span class="token comment"># tensor([[ 2.,  4.,  4.],  # attention scores from Query 1</span>
<span class="token comment">#         [ 4., 16., 12.],  # attention scores from Query 2</span>
<span class="token comment">#         [ 4., 12., 10.]]) # attention scores from Query 3</span>
<span class="token comment"># %%  计算softmax</span>
attn_scores_softmax <span class="token operator">=</span> softmax<span class="token punctuation">(</span>attn_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>attn_scores_softmax<span class="token punctuation">)</span>
<span class="token comment"># tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],</span>
<span class="token comment">#         [6.0337e-06, 9.8201e-01, 1.7986e-02],</span>
<span class="token comment">#         [2.9539e-04, 8.8054e-01, 1.1917e-01]])</span>

<span class="token comment"># For readability, approximate the above as follows</span>
<span class="token comment">#以下为近似值</span>
attn_scores_softmax <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span>
attn_scores_softmax <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>attn_scores_softmax<span class="token punctuation">)</span>
<span class="token comment"># %% 将attention scores乘以value</span>
weighted_values <span class="token operator">=</span> values<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> attn_scores_softmax<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>weighted_values<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># %%</span>
outputs <span class="token operator">=</span> weighted_values<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>

<span class="token comment">#%%</span>


</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br></div></div><p><strong>Note：</strong></p> <p>PyTorch has provided an API for this called* <em>nn.MultiheadAttention</em>. However, this API requires that you feed in key, query and value PyTorch tensors. Moreover, the outputs of this module undergo a linear transformation.</p> <h2 id="encoder-decoder"><a href="#encoder-decoder" class="header-anchor">#</a> Encoder-Decoder</h2> <h3 id="mask"><a href="#mask" class="header-anchor">#</a> Mask</h3> <p>mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p> <p><font color="red"><strong>其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。</strong></font></p> <h4 id="padding-mask"><a href="#padding-mask" class="header-anchor">#</a> Padding Mask</h4> <p>什么是 padding mask 呢？因为<strong>每个批次输入序列长度是不一样的</strong>也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p> <p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！</p> <p>而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。</p> <h4 id="sequence-mask"><a href="#sequence-mask" class="header-anchor">#</a> Sequence mask</h4> <p>文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p> <p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p> <p>对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。
其他情况，attn_mask 一律等于 padding mask。</p> <h3 id="具体事项"><a href="#具体事项" class="header-anchor">#</a> 具体事项</h3> <p>*<strong>注意encoder的输出并没直接作为decoder的直接输入。*</strong></p> <p>训练的时候，</p> <ol><li><p>初始decoder的time step为1时(也就是第一次接收输入)，其输入为一个特殊的token，可能是目标序列开始的token(如<BOS>)，也可能是源序列结尾的token(如<EOS>)，也可能是其它视任务而定的输入等等，不同源码中可能有微小的差异，其目标则是**预测翻译后的第1个单词(token)**是什么；</EOS></BOS></p></li> <li><p>然后<BOS>和预测出来的第1个单词一起，再次作为decoder的输入，得到第2个预测单词；3后续依此类推；</BOS></p></li></ol> <p>具体的例子如下：</p> <p>样本：“我/爱/机器/学习”和 &quot;i/ love /machine/ learning&quot;</p> <p><strong>训练：</strong></p> <ol><li><p>把“我/爱/机器/学习”embedding后输入到encoder里去，最后一层的encoder最终输出的outputs [10, 512]（假设我们采用的embedding长度为512，而且batch size = 1),此outputs 乘以新的参数矩阵，可以作为decoder里每一层用到的K和V；</p></li> <li><p>将<bos>作为decoder的初始输入，将decoder的最大概率输出词 A1和‘i’做cross entropy计算error。</bos></p></li> <li><p>将<bos>，&quot;i&quot; 作为decoder的输入，将decoder的最大概率输出词 A2 和‘love’做cross entropy计算error。</bos></p></li> <li><p>将<bos>，&quot;i&quot;，&quot;love&quot; 作为decoder的输入，将decoder的最大概率输出词A3和'machine' 做cross entropy计算error。</bos></p></li> <li><p>将<bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot; 作为decoder的输入，将decoder最大概率输出词A4和‘learning’做cross entropy计算error。</bos></p></li> <li><p>将<bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot; 作为decoder的输入，将decoder最大概率输出词A5和终止符做cross entropy计算error。</bos></p></li></ol> <p><strong>Sequence Mask</strong></p> <p>上述训练过程是<strong>挨个单词串行进行的</strong>，那么能不能并行进行呢，当然可以。可以看到上述单个句子训练时候，输入到 decoder的分别是</p> <bos><p><bos>，&quot;i&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot;</bos></p> <p>那么为何不将这些输入组成矩阵，进行输入呢？这些输入组成矩阵形式如下：</p> <p>【<bos></bos></p> <p><bos>，&quot;i&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot; 】</bos></p> <p>怎么操作得到这个矩阵呢？</p> <p>将decoder在上述2-6步次的输入补全为一个完整的句子</p> <p>【<bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot;
<bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot;
<bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot;
<bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot;
<bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot;】</bos></bos></bos></bos></bos></p> <p>然后将上述矩阵矩阵乘以一个 mask矩阵</p> <p>【1 0 0 0 0</p> <p>1 1 0 0 0</p> <p>1 1 1 0 0</p> <p>1 1 1 1 0</p> <p>1 1 1 1 1 】</p> <p>这样是不是就得到了</p> <p>【<bos></bos></p> <p><bos>，&quot;i&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;</bos></p> <p><bos>，&quot;i&quot;，&quot;love &quot;，&quot;machine&quot;，&quot;learning&quot; 】</bos></p> <p>这样的矩阵了 。着就是我们需要输入矩阵。这个mask矩阵就是 sequence mask，其实它和encoder中的padding mask 异曲同工。</p> <p>这样将这个矩阵输入到decoder（其实你可以想一下，此时这个矩阵是不是类似于批处理，矩阵的每行是一个样本，只是每行的样本长度不一样，每行输入后最终得到一个输出概率分布，作为矩阵输入的话一下可以得到5个输出概率分布）。</p> <p>这样我们就可以进行并行计算进行训练了。</p> <p><strong>测试</strong></p> <p>训练好模型， 测试的时候，比如用 '机器学习很有趣'当作测试样本，得到其英语翻译。</p> <p>这一句经过encoder后得到输出tensor，送入到decoder(并不是当作decoder的直接输入)：</p> <ol><li><p>然后用起始符<bos>当作decoder的 输入，得到输出 machine</bos></p></li> <li><p>用<bos> + machine 当作输入得到输出 learning</bos></p></li> <li><p>用 <bos> + machine + learning 当作输入得到is</bos></p></li> <li><p>用<bos> + machine + learning + is 当作输入得到interesting</bos></p></li> <li><p>用<bos> + machine + learning + is + interesting 当作输入得到 结束符号<eos></eos></bos></p></li></ol> <p>我们就得到了完整的翻译 'machine learning is interesting'</p> <p>可以看到，在测试过程中，只能一个单词一个单词的进行输出，是串行进行的。</p> <h1 id="bert-预训练模型-先放放"><a href="#bert-预训练模型-先放放" class="header-anchor">#</a> Bert（预训练模型，先放放）</h1> <p><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>https://zhuanlan.zhihu.com/p/364966458</p> <p>BERT提供了简单和复杂两个模型，对应的超参数分别如下：</p> <ul><li><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BBERT%7D_%7B%5Cmathbf%7BBASE%7D%7D" alt="[公式]"> : L=12，H=768，A=12，参数总量110M；</li> <li><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BBERT%7D_%7B%5Cmathbf%7BLARGE%7D%7D" alt="[公式]"> : L=24，H=1024，A=16，参数总量340M；</li></ul> <p>在上面的超参数中，L表示网络的层数（即<strong>Transformer blocks的数量</strong>），A表示Multi-Head Attention中self-Attention的数量，filter的尺寸是4H。隐含层大小H。</p> <p>Bert包括两个阶段:<font color="red">预训练阶段(Pre-train)、微调阶段(Fine Tuning)</font></p> <blockquote><p>预训练：一个寻网络权值初值的过程，将pre-train的结果作为BP算法的权值的初值，能够解决深度网络在非凸目标函数上陷入局部最优的问题（没有目标，不考虑结果，无监督的学习：如，nlp，学习到单词与单词之间的关系，单词的表示）</p> <p>Fine Turning：用<strong>训练好的参数</strong>（可以从已训练好的模型中获得）初始化自己的网络，然后用自己的数据接着训练（专业目的性学习，有监督学习）</p> <div class="language-mermaid line-numbers-mode"><pre class="language-text"><code>graph LR
aa[Bert]--&gt;A[Pre-Train]
aa[Bert]--&gt;bb[Fine Tune]
A[Pre-Train]--&gt;B[语言模型-Masked LM]
A[Pre-Train]--&gt;C[下一个句子的预测-Next Seq Predict]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>**语言模型：**首先修改原文章中的句子。</p> <ol><li><p>80%的概率真的用[MASK]取代被选中的词：--解决完形填空
my dog is hairy-&gt; my dog is [MASK]</p></li> <li><p>10%的概率用一个随机词取代它：--解决纠错问题
my dog is hairy-&gt; my dog is apple</p></li> <li><p>10%的概率保持不变：
my dog is hairy-&gt; my dog is hairy</p></li></ol> <p><strong>Next Seq Predict</strong>：</p> <p>问答，出了上句，给出下句；一问一答。如果两句相关，训练给出yes；否则为no。</p> <p><strong>微调</strong>：<font color="red">前面的预训练就得到了向量化的表示,用向量化表示进行文本的初始化，然后做一个有监督的学习，就可以完成Fine tune了</font></p> <p>•句子对的分类任务
•单个句子的分类任务
•问答任务
•名命名实体识别</p></blockquote> <p>BERT模型具有以下两个特点：</p> <p>第一，是这个模型非常的深，12层transformer，并不宽(wide），中间层只有1024，而之前的Transformer模型中间层有2048。这似乎又印证了计算机图像处理的一个观点——深而窄 比 浅而宽 的模型更好。</p> <p><img src="https://img-blog.csdnimg.cn/20190407194131428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> <p><strong>图1中的左侧部分是一个Transformer Block，对应到图2中的一个“Trm”。</strong></p> <p>第二，MLM（Masked Language Model），同时利用左侧和右侧的词语，这个在ELMo上已经出现了，绝对不是原创。其次，对于Mask（遮挡）在语言模型上的应用，已经被Ziang Xie提出了（我很有幸的也参与到了这篇论文中）：[1703.02573] Data Noising as Smoothing in Neural Network Language Models。这也是篇巨星云集的论文：Sida Wang，Jiwei Li（香侬科技的创始人兼CEO兼史上发文最多的NLP学者），Andrew Ng，Dan Jurafsky都是Coauthor。但很可惜的是他们没有关注到这篇论文。用这篇论文的方法去做Masking，相信BRET的能力说不定还会有提升。</p> <h2 id="embedding"><a href="#embedding" class="header-anchor">#</a> <strong>Embedding</strong></h2> <p>这里的Embedding由三种Embedding求和而成：</p> <p><img src="https://imagerk.oss-cn-beijing.aliyuncs.com/img/v2-11505b394299037e999d12997e9d1789_720w.jpg" alt="img"></p> <p>其中：</p> <ul><li>Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务</li> <li>Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务</li> <li><font color="red">Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的</font></li></ul> <h2 id="pre-training-task-1-masked-lm"><a href="#pre-training-task-1-masked-lm" class="header-anchor">#</a> <strong>Pre-training Task 1#: Masked LM</strong></h2> <p>第一步预训练的目标就是做语言模型，从上文模型结构中看到了这个模型的不同，即bidirectional。<strong>关于为什么要如此的bidirectional</strong>，作者在<a href="https://link.zhihu.com/?target=http%3A//www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/" target="_blank" rel="noopener noreferrer">reddit<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>上做了解释，意思就是如果使用预训练模型处理其他任务，那人们想要的肯定不止某个词左边的信息，而是左右两边的信息。而考虑到这点的模型ELMo只是将left-to-right和right-to-left分别训练拼接起来。直觉上来讲我们其实想要一个deeply bidirectional的模型，但是普通的LM又无法做到，因为在训练时可能会“穿越”（<strong>关于这点我不是很认同，之后会发文章讲一下如何做bidirectional LM</strong>）。所以作者用了一个加mask的trick。</p> <p>在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。<strong>最终的损失函数只计算被mask掉那个token。</strong></p> <p>Mask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。具体为什么这么分配，作者没有说。。。要注意的是Masked LM预训练阶段模型是不知道真正被mask的是哪个词，所以模型每个词都要关注。</p> <p>因为序列长度太大（512）会影响训练速度，所以90%的steps都用seq_len=128训练，余下的10%步数训练512长度的输入。</p> <h2 id="pre-training-task-2-next-sentence-prediction"><a href="#pre-training-task-2-next-sentence-prediction" class="header-anchor">#</a> <strong>Pre-training Task 2#: Next Sentence Prediction</strong></h2> <p>因为涉及到QA和NLI之类的任务，增加了第二个预训练任务，目的是让模型理解两个句子之间的联系。训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，模型预测B是不是A的下一句。预训练的时候可以达到97-98%的准确度。</p> <p><strong>注意：作者特意说了语料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。</strong></p> <h2 id="fine-tunning"><a href="#fine-tunning" class="header-anchor">#</a> <strong>Fine-tunning</strong></h2> <p>分类：对于sequence-level的分类任务，BERT直接取第一个[CLS]token的final hidden state <img src="https://www.zhihu.com/equation?tex=C%5Cin%5CRe%5EH" alt="[公式]"> ，加一层权重 <img src="https://www.zhihu.com/equation?tex=W%5Cin%5CRe%5E%7BK%5Ctimes+H%7D" alt="[公式]"> 后softmax预测label proba： <img src="https://www.zhihu.com/equation?tex=P%3Dsoftmax%28CW%5ET%29+%5C%5C" alt="[公式]"></p> <p>其他预测任务需要进行一些调整，如图：</p> <p><img src="https://pic2.zhimg.com/80/v2-b054e303cdafa0ce41ad761d5d0314e1_720w.jpg" alt="img"></p> <p>可以调整的参数和取值范围有：</p> <ul><li>Batch size: 16, 32</li> <li>Learning rate (Adam): 5e-5, 3e-5, 2e-5</li> <li>Number of epochs: 3, 4</li></ul> <p>因为大部分参数都和预训练时一样，精调会快一些，所以作者推荐多试一些参数。</p></bos></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">6 months ago</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#整体框架" class="sidebar-link reco-side-整体框架" data-v-70334359>整体框架</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#positional-embedding" class="sidebar-link reco-side-positional-embedding" data-v-70334359>Positional Embedding</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#具体" class="sidebar-link reco-side-具体" data-v-70334359>具体</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#self-attention" class="sidebar-link reco-side-self-attention" data-v-70334359>Self-Attention</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_1-准备输入" class="sidebar-link reco-side-_1-准备输入" data-v-70334359>1 准备输入</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_2-初始化权重" class="sidebar-link reco-side-_2-初始化权重" data-v-70334359>2 初始化权重</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_3-从每个输入中导出key-query-and-value的表示" class="sidebar-link reco-side-_3-从每个输入中导出key-query-and-value的表示" data-v-70334359>3 从每个输入中导出key, query and value的表示</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_4-计算输入的注意力得分-attention-scores" class="sidebar-link reco-side-_4-计算输入的注意力得分-attention-scores" data-v-70334359>4 计算输入的注意力得分(attention scores)</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_5-计算softmax" class="sidebar-link reco-side-_5-计算softmax" data-v-70334359>5 计算softmax</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_6-将attention-scores乘以value" class="sidebar-link reco-side-_6-将attention-scores乘以value" data-v-70334359>6 将attention scores乘以value</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_7-对加权后的value求和以得到输出1" class="sidebar-link reco-side-_7-对加权后的value求和以得到输出1" data-v-70334359>7 对加权后的value求和以得到输出1</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_8-对输入2重复步骤4-7" class="sidebar-link reco-side-_8-对输入2重复步骤4-7" data-v-70334359>8 对输入2重复步骤4–7</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#_9-来个总结" class="sidebar-link reco-side-_9-来个总结" data-v-70334359>9 来个总结</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#代码" class="sidebar-link reco-side-代码" data-v-70334359>代码</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#encoder-decoder" class="sidebar-link reco-side-encoder-decoder" data-v-70334359>Encoder-Decoder</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#mask" class="sidebar-link reco-side-mask" data-v-70334359>Mask</a></li><li class="level-3" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#具体事项" class="sidebar-link reco-side-具体事项" data-v-70334359>具体事项</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#embedding" class="sidebar-link reco-side-embedding" data-v-70334359>Embedding</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#pre-training-task-1-masked-lm" class="sidebar-link reco-side-pre-training-task-1-masked-lm" data-v-70334359>Pre-training Task 1#: Masked LM</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#pre-training-task-2-next-sentence-prediction" class="sidebar-link reco-side-pre-training-task-2-next-sentence-prediction" data-v-70334359>Pre-training Task 2#: Next Sentence Prediction</a></li><li class="level-2" data-v-70334359><a href="/views/Deeplearn/Self-attention-Bert_EC.html#fine-tunning" class="sidebar-link reco-side-fine-tunning" data-v-70334359>Fine-tunning</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><canvas id="vuepress-canvas-cursor"></canvas><!----><div class="vuepress-canvas-nest-element"></div><div class="kanbanniang" data-v-27e9bfa4><div class="banniang-container" style="display:;" data-v-27e9bfa4><div class="messageBox" style="position:fixed;right:75px;bottom:235px;opacity:0.75;height:max-content;width:200px;fon-szie:16px;display:none;" data-v-27e9bfa4></div> <div class="operation" style="display:;" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660425629" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6044" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M577.5584 307.848533l-21.345067-18.699733c-0.062933-0.061867-0.186667-0.123733-0.280533-0.186667l-44.305067-38.689067-53.752533 47.5648L127.009067 587.424l0 177.1392 0 155.0048c0 45.886933 37.454933 83.0976 83.610667 83.0976l183.966933 0L394.586667 735.8688c0-27.512533 22.448-49.8336 50.162133-49.8336l133.7728 0c27.714133 0 50.178133 22.321067 50.178133 49.8336L628.699733 1002.666667l183.966933 0c46.170667 0 83.610667-37.211733 83.610667-83.0976L896.277333 763.9424 896.277333 586.7712 578.5216 308.688 577.5584 307.848533z" p-id="6045" data-v-27e9bfa4></path> <path d="M990.637867 418.164267l-94.360533-82.600533 0-181.290667c0-36.714667-29.952-66.482133-66.894933-66.482133-36.941867 0-66.893867 29.767467-66.893867 66.482133l0 64.197333L556.213333 37.911467c-25.291733-22.103467-63.165867-22.103467-88.4256 0L33.348267 418.164267c-27.730133 24.247467-30.402133 66.264533-5.9808 93.808 24.437333 27.544533 66.692267 30.219733 94.407467 5.938133L512 176.376533l390.2432 341.533867c12.7072 11.130667 28.4608 16.600533 44.181333 16.600533 18.549333 0 37.0048-7.617067 50.209067-22.538667C1021.054933 484.4128 1018.382933 442.4128 990.637867 418.164267z" p-id="6046" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660394444" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5299" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M0 202.7V631c0 83.3 68.3 150.7 152.6 150.7h228.9l8 190.3 224.9-190.3h257c84.3 0 152.6-67.4 152.6-150.7V202.7C1024 119.4 955.7 52 871.4 52H152.6C68.3 52 0 119.4 0 202.7z m658.6 237.9c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S771 512 730.9 512c-40.2 0-72.3-31.7-72.3-71.4z m-220.9 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S550.1 512 510 512c-40.2 0-72.3-31.7-72.3-71.4z m-216.8 0c0-39.7 32.1-71.4 72.3-71.4 40.2 0 72.3 31.7 72.3 71.4S333.3 512 293.1 512c-40.1 0-72.2-31.7-72.2-71.4z" p-id="5300" data-v-27e9bfa4></path></svg></i> <i data-v-27e9bfa4><svg t="1572660570409" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2153" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 393.846154c-86.646154 0-157.538462 70.892308-157.538462 157.538461s70.892308 157.538462 157.538462 157.538462 157.538462-70.892308 157.538462-157.538462-70.892308-157.538462-157.538462-157.538461z m393.846154-118.153846h-102.4c-27.569231 0-51.2-13.784615-66.953846-35.446154l-45.292308-68.923077C677.415385 137.846154 643.938462 118.153846 608.492308 118.153846h-192.984616c-35.446154 0-68.923077 19.692308-84.676923 53.169231l-45.292307 68.923077c-13.784615 21.661538-39.384615 35.446154-66.953847 35.446154H118.153846c-43.323077 0-78.769231 35.446154-78.769231 78.76923v472.615385c0 43.323077 35.446154 78.769231 78.769231 78.769231h787.692308c43.323077 0 78.769231-35.446154 78.769231-78.769231V354.461538c0-43.323077-35.446154-78.769231-78.769231-78.76923zM512 787.692308c-129.969231 0-236.307692-106.338462-236.307692-236.307693s106.338462-236.307692 236.307692-236.307692 236.307692 106.338462 236.307692 236.307692-106.338462 236.307692-236.307692 236.307693z" p-id="2154" data-v-27e9bfa4></path></svg></i> <!---->
      <a target="_blank" href="https://github.com/kii-chan-iine" data-v-27e9bfa4><i data-v-27e9bfa4><svg t="1572660325062" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3809" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 3.413333c280.849067 0 508.586667 199.273813 508.586667 444.94848 0 140.427947-74.519893 265.53344-190.65856 347.11552V1020.586667l-222.839467-135.168c-30.859947 5.147307-62.552747 8.021333-95.085227 8.021333-280.845653 0-508.586667-199.28064-508.586666-445.078187C3.413333 202.687147 231.150933 3.413333 512 3.413333z m-158.96576 603.921067h317.805227c17.578667 0 31.812267-14.2336 31.812266-31.819093a31.798613 31.798613 0 0 0-31.812266-31.80544h-317.805227c-17.578667 0-31.812267 14.2336-31.812267 31.80544 0.116053 17.585493 14.349653 31.819093 31.812267 31.819093z m-63.511893-190.665387h444.951893c17.578667 0 31.812267-14.2336 31.812267-31.812266a31.802027 31.802027 0 0 0-31.812267-31.81568H289.522347a31.802027 31.802027 0 0 0-31.81568 31.81568c0 17.578667 14.2336 31.812267 31.81568 31.812266z" p-id="3810" data-v-27e9bfa4></path></svg></i></a> <i data-v-27e9bfa4><svg t="1572660347392" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4543" width="16" height="16" class="icon" data-v-27e9bfa4><path d="M512 34.133333a486.4 486.4 0 1 0 486.4 486.4A486.4 486.4 0 0 0 512 34.133333z m209.4848 632.8064l-55.6032 55.466667-151.517867-151.125333-151.517866 151.1168-55.6032-55.466667 151.517866-151.108267L307.242667 364.714667l55.6032-55.466667 151.517866 151.125333 151.517867-151.1168 55.6032 55.466667-151.517867 151.099733z m0 0" p-id="4544" data-v-27e9bfa4></path></svg></i></div> <canvas id="banniang" width="216" height="281.6" class="live2d" style="position:fixed;right:90px;bottom:-20px;opacity:1;" data-v-27e9bfa4></canvas></div> <div class="showBanNiang" style="display:none;" data-v-27e9bfa4>
    看板娘
  </div></div><!----><div></div></div></div>
    <script src="/assets/js/app.c5936fd9.js" defer></script><script src="/assets/js/3.6c2c0b5f.js" defer></script><script src="/assets/js/1.06133022.js" defer></script><script src="/assets/js/22.e5025c86.js" defer></script><script src="/assets/js/11.6a0e9e65.js" defer></script>
  </body>
</html>
