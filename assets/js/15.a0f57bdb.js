(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{631:function(_,v,a){"use strict";a.r(v);var e=a(3),c=Object(e.a)({},(function(){var _=this,v=_.$createElement,a=_._self._c||v;return a("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[a("Boxx",{attrs:{changeTime:"10000"}}),_._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"title"},[_._v("前言")]),a("p",[_._v("本文作者提出了一种新的损失函数，来通过图像质量强调不同的困难样本的重要性。主要采用自适应边缘函数，实现了使用feature norms来近似图像质量的方法。该方法在多个数据集上提高了现有（SOTA）的人脸识别性能。")]),_._v(" "),a("p",[_._v("一直以来，低质量图像的人脸识别都具有挑战性，因为人脸属性是模糊和退化的。"),a("code",[_._v("margin-based loss functions")]),_._v("的进步提高了嵌入空间中人脸的可辨别性。此外，以往的研究研究了适应性损失的影响，使错误分类（Head）的样本更加重要。")]),_._v(" "),a("p",[_._v("在这项工作中，在损失函数中引入了另一个因素，即"),a("code",[_._v("图像质量")]),_._v("。作者认为，强调错误分类样本的策略应根据其图像质量进行调整。具体来说，"),a("strong",[_._v("简单或困难样本的相对重要性应该基于样本的图像质量来给定")]),_._v("。据此作者提出了一种新的损失函数来通过图像质量强调不同的困难样本的重要性。")]),_._v(" "),a("p",[_._v("本文的方法通过用"),a("code",[_._v("feature norms")]),_._v("来近似图像质量，这里是以自适应边缘函数的形式来实现这一点。")]),_._v(" "),a("p",[_._v("大量的实验表明，"),a("code",[_._v("AdaFace")]),_._v("在4个数据集("),a("code",[_._v("IJB-B")]),_._v("、"),a("code",[_._v("IJB-C")]),_._v("、"),a("code",[_._v("IJB-S")]),_._v("和"),a("code",[_._v("IJBTinyFace")]),_._v(")上提高了现有的(SoTA)的人脸识别性能。")])]),_._v(" "),a("h4",{attrs:{id:"说在前面"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#说在前面"}},[_._v("#")]),_._v(" 说在前面")]),_._v(" "),a("blockquote",[a("p",[_._v("对于人脸识别，大家可能觉得已经内卷的差不多了，没什么可以挖掘的了，但是实际上我们还是在有意无意的在回避一些实际落地的问题，"),a("code",[_._v("AdaFace")]),_._v("则是一个直面落地问题的经典工作，作为CVPR2022的Oral工作当之无愧。")]),_._v(" "),a("p",[_._v("其直面低质量人脸图像的识别问题，同时作者通过使用"),a("code",[_._v("特征范数")]),_._v("来"),a("code",[_._v("近似图像质量")]),_._v("提出一个概括性的损失函数，可以随意在"),a("code",[_._v("ArcFace")]),_._v("和"),a("code",[_._v("CosFace")]),_._v("之间随意游走，在提升低质量图像的识别精度的同时，也没有损失高质量图像的精度，可以说是一个很不错和经典的工作。")]),_._v(" "),a("p",[_._v("希望这里的解读可以帮助到大家！！！")]),_._v(" "),a("p",[_._v("这里文中给出最富有总结性的一个表格：")]),_._v(" "),a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/90c9136ca1793d08588a066d8cd3f9bd.webp",title:"",alt:"","data-align":"center"}})]),_._v(" "),a("h2",{attrs:{id:"_1简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1简介"}},[_._v("#")]),_._v(" 1简介")]),_._v(" "),a("p",[_._v("图像质量是一个属性的组合，表明一个图像如何如实地捕获原始场景。影响图像质量的因素包括亮度、对比度、锐度、噪声、色彩一致性、分辨率、色调再现等。")]),_._v(" "),a("p",[_._v("这里人脸图像是本文的重点，可以在各种灯光、姿势和面部表情的设置下捕捉到的图像，有时也可以在极端的视觉变化下捕捉，如对象的年龄或妆容。这些参数的设置使得学习过的人脸识别模型很难完成识别任务。尽管如此，这项任务还是可以完成的，因为人类或模型通常可以在这些困难的环境下识别人脸。")]),_._v(" "),a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/734ae625489a0514c7480c39c99967e0.webp",title:"",alt:"","data-align":"center"}}),_._v(" "),a("p",[_._v("图1")]),_._v(" "),a("p",[_._v("然而，当人脸图像质量较低时，根据质量程度的不同，识别任务变得不可行。图1显示了高质量和低质量的人脸图像的例子。不可能识别出图1最后1列中的对象。")]),_._v(" "),a("p",[_._v("像图1最下面一行这样的低质量图像正越来越成为人脸识别数据集的重要组成部分，因为它们会在监控视频和无人机镜头中遇到。鉴于SoTA FR方法能够在相对较高质量的数据集，如"),a("code",[_._v("LFW")]),_._v("或"),a("code",[_._v("CFP-FP")]),_._v("中获得超过98%的验证精度，最近的FR挑战已经转向了较低质量的数据集，如"),a("code",[_._v("IJB-B")]),_._v("、"),a("code",[_._v("IJB-C")]),_._v("和"),a("code",[_._v("IJB-S")]),_._v("。虽然挑战是在低质量的数据集上获得较高的准确性，但大多数流行的训练数据集仍然由高质量的图像组成。由于只有一小部分训练数据质量较低，因此在训练期间适当地利用它是很重要的。")]),_._v(" "),a("p",[_._v("低质量的人脸图像的一个问题是，它们往往无法辨认。当图像退化过大时，相关的身份信息从图像中消失，导致图像无法识别。")]),_._v(" "),a("p",[_._v("这些无法识别的图像对训练过程有害的，因为模型将试图利用图像中的其他视觉特征，如服装颜色或图像分辨率，进而会影响训练损失。如果这些图像在低质量图像的分布中占主导地位，那么该模型在测试期间很可能在低质量的数据集上表现不佳。")]),_._v(" "),a("p",[_._v("由于无法识别的面部图像的存在，于是作者便想设计一个损失函数，根据图像质量对不同困难的样本赋予不同的重要性。作者的目标是强调高质量图像的困难样本和低质量图像的简单样本。通常，对样本的不同困难是通过观察训练进展（课程学习）来分配不同的重要性的。然而，作者实验表明，样本的重要性应该通过观察难度和图像质量来调整。")]),_._v(" "),a("p",[_._v("应该根据图像质量不同地设置重要性的原因是，直接强调困难样本总是强烈强调不可识别的图像。这是因为人们只能对无法识别的图像进行随机猜测，因此，它们总是在困难样本中。在将图像质量引入到目标中方面存在着一些挑战。这是因为图像质量是难以量化的，因为它的广泛定义和基于困难的缩放样本经常引入本质上是启发式。")]),_._v(" "),a("p",[_._v("在本工作中，作者提出了一个损失函数，以无缝的方式实现上述目标。作者还发现，")]),_._v(" "),a("ol",[a("li",[a("p",[_._v("特征范数可以很好地代表图像质量；")])]),_._v(" "),a("li",[a("p",[_._v("不同的裕度函数对不同的样本困难具有不同的重要性。")])])]),_._v(" "),a("p",[_._v("这2个发现结合在一个统一的损失函数"),a("code",[_._v("AdaFace")]),_._v("中，该函数根据图像质量自适应地改变边缘函数，对不同的样本困难赋予不同的重要性。")]),_._v(" "),a("h4",{attrs:{id:"主要贡献"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#主要贡献"}},[_._v("#")]),_._v(" 主要贡献")]),_._v(" "),a("ol",[a("li",[a("p",[_._v("提出了一个损失函数，"),a("code",[_._v("AdaFace")]),_._v("，它根据样本的图像质量对不同的困难样本赋予不同的权重。通过"),a("strong",[_._v("结合图像质量，避免强调难以识别的图像，专注于困难但可识别的样本")]),_._v("；")])]),_._v(" "),a("li",[a("p",[_._v("通过实验表明，角边缘尺度的学习梯度与训练样本的难度相关。这一观察结果促使作者通过自适应地改变边缘函数来强调困难样本，如果图像质量较低，则忽略非常困难的样本（无法识别的图像）。")])]),_._v(" "),a("li",[a("p",[_._v("证明了"),a("code",[_._v("feature norms")]),_._v("可以作为图像质量的代理。它绕过了需要一个额外的模块来估计图像质量。因此，自适应边际函数不需要额外的复杂度。")])]),_._v(" "),a("li",[a("p",[_._v("通过对9个不同质量的数据集(LFW、CFP-FP、CPLFW、AgeDB、CALFW、IJB-B、IJB-C、IJB-S和TinyFace)的广泛评估，验证了该方法的有效性。实验表明，"),a("code",[_._v("AdaFace")]),_._v("在低质量数据集上的识别性能可以大大提高，同时保持在高质量数据集上的性能。")])])]),_._v(" "),a("h2",{attrs:{id:"_2相关工作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2相关工作"}},[_._v("#")]),_._v(" 2相关工作")]),_._v(" "),a("h3",{attrs:{id:"_2-1-margin-based-loss-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-margin-based-loss-function"}},[_._v("#")]),_._v(" 2.1 Margin Based Loss Function")]),_._v(" "),a("p",[_._v("基于Margin的softmax损失函数被广泛应用于人脸识别训练中(FR)。在Softmax损失中加入了Margin，是因为加入Margin后模型可以学习到更好的类间表征和类内表征，特征也就更具有可判别性。典型的形式有："),a("code",[_._v("SphereFace")]),_._v("、"),a("code",[_._v("CosFace")]),_._v("和"),a("code",[_._v("ArcFace")]),_._v("引入了不同形式的"),a("code",[_._v("Margin函数")]),_._v("。具体来说，它可以t同意写成：")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/cb21a2a0b945a4d4c23df04b55392a1d.webp",alt:""}})]),_._v(" "),a("p",[_._v("式中，θ为特征向量与第个分类器权值向量之间的夹角，为Ground Truth(GT)的索引，m为"),a("code",[_._v("Margin")]),_._v("是一个标量超参数。是一个边际函数，其中,"),a("code",[_._v("SphereFace")]),_._v("、"),a("code",[_._v("CosFace")]),_._v("和"),a("code",[_._v("ArcFace")]),_._v("可以用一下3中不同的"),a("code",[_._v("Margin函数")]),_._v("表达:")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/68f20859fc7e4661a6e329902d0b912b.webp",alt:""}})]),_._v(" "),a("p",[_._v("有时，"),a("code",[_._v("ArcFace")]),_._v("被称为"),a("code",[_._v("angular margin")]),_._v("，而CosFace被称为"),a("code",[_._v("additive margin")]),_._v("。这里，是一个用于缩放的超参数。"),a("code",[_._v("P2SGrad")]),_._v("中注意到"),a("code",[_._v("m")]),_._v("和"),a("code",[_._v("s")]),_._v("是敏感的超参数，并建议直接修改梯度，使其没有"),a("code",[_._v("m")]),_._v("和"),a("code",[_._v("s")]),_._v("超参数。")]),_._v(" "),a("p",[a("code",[_._v("AdaFace")]),_._v("旨在将"),a("code",[_._v("Margin")]),_._v("建模为图像质量的函数，因为影响在训练过程中哪些样本贡献了更多的梯度(即学习信号)。")]),_._v(" "),a("h3",{attrs:{id:"_2-2-adaptive-loss-functions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-adaptive-loss-functions"}},[_._v("#")]),_._v(" 2.2 Adaptive Loss Functions")]),_._v(" "),a("p",[_._v("许多研究在训练目标中引入了适应性元素，用于"),a("code",[_._v("hard sample mining")]),_._v("、训练期间的调度困难或寻找最优超参数。例如，"),a("code",[_._v("CurricularFace")]),_._v("将"),a("code",[_._v("课程学习")]),_._v("的思想引入到损失函数中。在训练的最初阶段，(负余弦相似度)的"),a("code",[_._v("Margin")]),_._v("被设置为很小，以便容易样本的学习，在后期阶段，"),a("code",[_._v("Margin")]),_._v("被增加，以便"),a("code",[_._v("Hard样本")]),_._v("可以学习。具体来说，它被写成:")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/f0b280c68d863c7745640a2dbe0f1048.webp",alt:""}})]),_._v(" "),a("p",[_._v("其中，")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/239de7392f6c9514fe60f1db59b18c33.webp",alt:""}})]),_._v(" "),a("p",[_._v("而是一个随着训练的进展而增加的参数。因此，在"),a("code",[_._v("CurricularFace")]),_._v("中，"),a("code",[_._v("Margin")]),_._v("的适应性是基于训练的进展（Curricular）。")]),_._v(" "),a("p",[_._v("相反，作者认为"),a("code",[_._v("Margin")]),_._v("的适应性应该基于图像质量。在高质量的图像，如果样本是很困难的(对模型)，网络应该学会利用图像中的信息；但在低质量的图像，如果样本是很困难的，它更有可能是缺乏适当的身份的线索，那么网络不应该去学习相关的特征。")]),_._v(" "),a("p",[a("code",[_._v("MagFace")]),_._v("探索了基于可识别性应用不同"),a("code",[_._v("Margin")]),_._v("的想法。它在"),a("code",[_._v("high norm features")]),_._v("易于识别的前提下，对"),a("code",[_._v("high norm features")]),_._v("应用大角度"),a("code",[_._v("Margin")]),_._v("。大"),a("code",[_._v("Margin")]),_._v("推动"),a("code",[_._v("high norm features")]),_._v("更接近"),a("code",[_._v("class中心")]),_._v("。然而，它并没有强调困难的训练样本，但是这些困难样本对学习鉴别特征也很重要。")]),_._v(" "),a("p",[_._v("同样值得一提的是，"),a("code",[_._v("DDL")]),_._v("使用蒸馏损失来最小化简单和困难样本特征之间的差距。")]),_._v(" "),a("h3",{attrs:{id:"_2-3-低质量图像的人脸识别"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-低质量图像的人脸识别"}},[_._v("#")]),_._v(" 2.3 低质量图像的人脸识别")]),_._v(" "),a("p",[_._v("最近的"),a("code",[_._v("FR模型")]),_._v("在人脸属性可识别的数据集上取得了较高性能，例如"),a("code",[_._v("LFW")]),_._v("、"),a("code",[_._v("CFP-FP")]),_._v("、"),a("code",[_._v("CPLFW")]),_._v("、"),a("code",[_._v("AgeDB")]),_._v("和"),a("code",[_._v("CALFW")]),_._v("。当"),a("code",[_._v("FR模型")]),_._v("学习不受"),a("code",[_._v("光照")]),_._v("、"),a("code",[_._v("年龄")]),_._v("或"),a("code",[_._v("姿态变化")]),_._v("影响的鉴别特征时，可以在这些数据集上获得良好的性能。")]),_._v(" "),a("p",[_._v("然而，在不受约束的情况下，如监控或低质量的视频，FR便会带来很多的问题。这种配置下的数据集包括"),a("code",[_._v("IJB-B")]),_._v("、"),a("code",[_._v("IJB-C")]),_._v("和"),a("code",[_._v("IJB-S")]),_._v("，其中大多数图像质量很低，有些甚至不包含足够的身份信息，即使是对人工检查人员来说。良好表现的关键包括:")]),_._v(" "),a("blockquote",[a("ol",[a("li",[a("p",[_._v("学习低质量图像的可鉴别特征;")])]),_._v(" "),a("li",[a("p",[_._v("学习丢弃包含少量识别线索的图像(质量感知融合)。")])])])]),_._v(" "),a("p",[_._v("为了进行质量感知融合，人们提出了概率方法来预测FR表示中的不确定性。假设特征是分布，其中方差可以用来计算预测的确定性。然而，由于训练目标的不稳定性，概率方法会分别采用学习"),a("code",[_._v("均值")]),_._v("和"),a("code",[_._v("方差")]),_._v("，这在训练过程中并不简单，因为"),a("code",[_._v("方差")]),_._v("是用一个固定的"),a("code",[_._v("均值")]),_._v("来优化的。然而，"),a("code",[_._v("AdaFace")]),_._v("是对传统的"),a("code",[_._v("softmax损失")]),_._v("的一个修改，使框架易于使用。此外，"),a("code",[_._v("AdaFace")]),_._v("使用"),a("code",[_._v("feature norms")]),_._v("作为"),a("code",[_._v("质量感知融合")]),_._v("过程中预测质量的代理。")]),_._v(" "),a("p",[a("code",[_._v("合成数据")]),_._v("或"),a("code",[_._v("数据扩充")]),_._v("可以用来模拟低质量的数据。有方法通过训练人脸属性标记器生成训练数据的伪标签。这些辅助步骤只会使训练过程复杂化，并使其难以推广到其他数据集或领域。"),a("code",[_._v("AdaFace")]),_._v("方法只涉及简单的"),a("code",[_._v("裁剪")]),_._v("、"),a("code",[_._v("模糊")]),_._v("和"),a("code",[_._v("光照增强")]),_._v("，这也适用于其他数据集和域。")]),_._v(" "),a("h2",{attrs:{id:"_3本文方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3本文方法"}},[_._v("#")]),_._v(" 3本文方法")]),_._v(" "),a("p",[_._v("样本的"),a("code",[_._v("Cross entropy softmax loss")]),_._v("可以表述为：")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/aa7d5ffa4d9e391e1d8bd2bc51261d8c.webp",alt:""}})]),_._v(" "),a("p",[_._v("其中是的特征嵌入，属于第类。为最后一个FC层权值矩阵的第列，，为对应的偏置项。C表示类的数量。")]),_._v(" "),a("p",[_._v("在测试时间内，对于任意一对图像，和，使用余弦相似度度量来寻找最接近的匹配恒等式。为了使训练目标直接优化余弦距离，使用"),a("code",[_._v("normalized softmax")]),_._v("，其中偏差项设置为零，然后特征通过归一化和缩放参数进行了转换:")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/ed64d5952c57a444487275d78fdccd52.webp",alt:""}})]),_._v(" "),a("p",[_._v("其中，对应于和之间的夹角。并引入了"),a("code",[_._v("Margin")]),_._v("来减少类内的变化。通常，它可以被写成"),a("code",[_._v("ArcFace")]),_._v("等的统一表达式，其中"),a("code",[_._v("Margin")]),_._v("函数在方程式中定义。")]),_._v(" "),a("h3",{attrs:{id:"_3-1-margin-form-and-the-gradient"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-margin-form-and-the-gradient"}},[_._v("#")]),_._v(" 3.1 Margin Form and the Gradient")]),_._v(" "),a("p",[_._v("先前关于基于"),a("code",[_._v("Margin")]),_._v("的"),a("code",[_._v("Softmax")]),_._v("的工作主要集中在"),a("code",[_._v("Margin")]),_._v("如何改变决策边界以及它们的几何解释。在本节中，作者展示了在反向传播过程中，由于"),a("code",[_._v("Margin")]),_._v("而引起的梯度变化会影响到一个样本相对于其他样本的重要性的影响。换句话说，"),a("code",[_._v("angular margin")]),_._v("可以在梯度方程中引入一个附加项，根据样本的难度对信号进行缩放。为了证明这一点，作者将研究"),a("code",[_._v("梯度方程")]),_._v("如何随"),a("code",[_._v("Margin")]),_._v("函数而变化。")]),_._v(" "),a("p",[_._v("设为对输入进行"),a("code",[_._v("softmax")]),_._v("后在第类上的概率。通过推导的梯度方程和，可以得到如下：")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/67aef3eca2e05ee3cb510e6c0c87abc6.webp",alt:""}})]),_._v(" "),a("p",[_._v("在等式中,和是标量。此外，这两个项是唯一受参数到影响的项。没有m，这里可以把前2个标量项看作是一个"),a("code",[_._v("梯度尺度项")]),_._v("("),a("code",[_._v("GST")]),_._v(")，并表示为：")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/d43d44f47d7c1ce452f6171fe6d0448a.webp",alt:""}})]),_._v(" "),a("p",[_._v("为了"),a("code",[_._v("GST")]),_._v("分析的目的，将考虑"),a("code",[_._v("类指数")]),_._v("，因为所有负类指数在方程中没有"),a("code",[_._v("Margin")]),_._v("。于是"),a("code",[_._v("normalized softmax loss")]),_._v("的GST为：")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/475ee34cc4b71e400c67ffb0e352d128.webp",alt:""}})]),_._v(" "),a("p",[_._v("因为和=s。所以"),a("code",[_._v("CosFace")]),_._v("的"),a("code",[_._v("GST")]),_._v("同样也是：")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/7fac0bbbef3dce56739aa2a2ffff1fa1.webp",alt:""}})]),_._v(" "),a("p",[_._v("通过定义和=s。所以"),a("code",[_._v("ArcFace")]),_._v("的"),a("code",[_._v("GST")]),_._v("如下：")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/6ac237d52e80eefd25fd03b8bafae078.webp",alt:""}})]),_._v(" "),a("p",[_._v("因为"),a("code",[_._v("GST")]),_._v("是和"),a("code",[_._v("m")]),_._v("的函数，就像在等式中一样、可以用它根据样本的困难成都来控制对样本的强调，即训练期间的。")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/5fe661511c9a96be72825ca63dfda422.webp",alt:""}})]),_._v(" "),a("p",[_._v("图2")]),_._v(" "),a("p",[_._v("为了了解"),a("code",[_._v("GST")]),_._v("的效果，图3为"),a("code",[_._v("GST")]),_._v("在特征空间中的颜色。需要注意的是，对于"),a("code",[_._v("angular margin")]),_._v("，"),a("code",[_._v("GST")]),_._v("在"),a("code",[_._v("决策边界")]),_._v("达到峰值，但随着它向移动而逐渐减小，而较困难的样本得到的强调较少。如果改变"),a("code",[_._v("angular margin")]),_._v("的符号，会看到相反的效果。")]),_._v(" "),a("p",[_._v("请注意，在第6列中，"),a("code",[_._v("MagFace")]),_._v("是"),a("code",[_._v("ArcFace")]),_._v("的扩展，具有更大的"),a("code",[_._v("Margin")]),_._v("分配给高范数特征。"),a("code",[_._v("ArcFace")]),_._v("和"),a("code",[_._v("MagFace")]),_._v("都没有高度重视困难样本(附近的绿色区域)。结合所有的"),a("code",[_._v("Margin")]),_._v("函数，以在必要时强调困难样本。")]),_._v(" "),a("blockquote",[a("p",[_._v("请注意，这种适应性也不同于使用训练阶段来改变样本中不同困难的相对重要性的方法。图3显示了"),a("code",[_._v("CurricularFace")]),_._v("，其中决策边界和"),a("code",[_._v("GST g")]),_._v("随训练阶段的不同而变化。")])]),_._v(" "),a("h3",{attrs:{id:"_3-2-norm-and-image-quality"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-norm-and-image-quality"}},[_._v("#")]),_._v(" 3.2 Norm and Image quality")]),_._v(" "),a("p",[a("code",[_._v("图像质量")]),_._v("是一个综合性的术语，它涵盖了诸如"),a("code",[_._v("亮度")]),_._v("、"),a("code",[_._v("对比度")]),_._v("和"),a("code",[_._v("锐度")]),_._v("等特征。"),a("code",[_._v("图像质量评估")]),_._v("(IQA)在计算机视觉中得到了广泛的研究。"),a("code",[_._v("SER-FIQ")]),_._v("是一种用于"),a("code",[_._v("人脸IQA")]),_._v("的无监督"),a("code",[_._v("DL")]),_._v("方法。"),a("code",[_._v("Brisque")]),_._v("是一种流行的"),a("code",[_._v("blind/no-reference IQA")]),_._v("算法。")]),_._v(" "),a("p",[_._v("然而，这些方法在训练过程中使用的计算成本很高。在这项工作中避免引入一个额外的模块来计算图像质量。相反，使用特征规范作为图像质量的代理。作者观察到，在使用基于"),a("code",[_._v("Margin")]),_._v("的"),a("code",[_._v("Softmax Loss")]),_._v("训练的模型中，"),a("code",[_._v("特征范数表现出与图像质量相关的趋势")]),_._v("。")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/a1f9fdfa90cca918ee880d634cde706d.webp",alt:""}})]),_._v(" "),a("p",[_._v("在图4(a)中显示了特征范数与图像质量("),a("code",[_._v("1-brisque")]),_._v(")作为绿色曲线计算的图像质量(IQ)得分之间的相关图。从训练数据集随机抽取1534张图像(MS1MV2)并使用预先训练好的模型计算"),a("code",[_._v("特征范数")]),_._v("。在最后一个阶段，"),a("code",[_._v("特征规范")]),_._v("与"),a("code",[_._v("IQ score")]),_._v("之间的相关性得分达到0.5235（超过−1和1）。对应的散点图如图4(b)所示,"),a("code",[_._v("特征范数")]),_._v("和"),a("code",[_._v("IQ score")]),_._v("之间的高相关性支持了使用"),a("code",[_._v("特征范数")]),_._v("作为"),a("code",[_._v("图像质量")]),_._v("的代理。")]),_._v(" "),a("p",[_._v("在图4(a)中还展示了概率输出与"),a("code",[_._v("IQ score")]),_._v("之间的相关图，其曲线为"),a("code",[_._v("橙色曲线")]),_._v("。注意，特征范数的相关性总是比高。此外，特征范数与"),a("code",[_._v("IQ score")]),_._v("之间的相关性在训练的早期阶段是可见的。这对于使用"),a("code",[_._v("特征范数")]),_._v("作为"),a("code",[_._v("图像质量的代理")]),_._v("是一个有用的属性，因为可以依赖于训练的早期阶段的代理。")]),_._v(" "),a("p",[_._v("此外，在图4(c)中展示了与"),a("code",[_._v("IQ score")]),_._v("之间的散点图。注意，和"),a("code",[_._v("图像质量")]),_._v("之间存在非线性关系。描述样本难度的一种方法是使用，图中显示了样本难度的分布随图像质量的不同而不同。因此，"),a("code",[_._v("根据难度调整样本重要性")]),_._v("时考虑图像质量是有意义的。")]),_._v(" "),a("h3",{attrs:{id:"_3-3-adaface-adaptive-margin-based-on-norm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-adaface-adaptive-margin-based-on-norm"}},[_._v("#")]),_._v(" 3.3 AdaFace: Adaptive Margin based on Norm")]),_._v(" "),a("p",[_._v("为了解决不可识别图像引起的问题，作者提出基于特征范数来适应"),a("code",[_._v("Margin")]),_._v("函数。在第二节中已经证明，使用不同的"),a("code",[_._v("Margin")]),_._v("函数可以强调样本的不同困难成都。另外，观察到特征规范是寻找低质量图像的好方法。")]),_._v(" "),a("h4",{attrs:{id:"_1、image-quality-indicator"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、image-quality-indicator"}},[_._v("#")]),_._v(" 1、Image Quality Indicator")]),_._v(" "),a("p",[_._v("作为特征范数，是一个模型依赖的量，使用batch统计和对其进行归一化。具体来说:")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/d11c8b1d6bc45acad6b011bf464c950e.webp",alt:""}})]),_._v(" "),a("p",[_._v("其中，和为一个batch内所有的平均值和标准差。[]是指在−1和1之间裁剪值，阻止梯度流动。")]),_._v(" "),a("p",[_._v("由于将的"),a("code",[_._v("Batch分布")]),_._v("近似为单位"),a("code",[_._v("高斯分布")]),_._v("，因此将该值剪辑在−1和−1范围内，以便更好地处理。")]),_._v(" "),a("p",[_._v("已知，大约68%的单位高斯分布落在−1和1之间，因此引入项"),a("code",[_._v("h")]),_._v("来控制"),a("code",[_._v("concentration")]),_._v("。设置h，使大多数值落在−1和1之间。实现这一点的将是"),a("code",[_._v("h=0.33")]),_._v("。")]),_._v(" "),a("p",[_._v("如果"),a("code",[_._v("Batch size")]),_._v("较小，则Batch统计信息和可能不稳定。因此，使用和跨多个步骤的指数移动平均数(EMA)来稳定Batch统计数据。具体来说，设和是的第k步批统计。然后:")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/d40320d4df47073b20cd81bb603be956.webp",alt:""}})]),_._v(" "),a("p",[_._v("α的动量设定为0.99。对于也是如此。")]),_._v(" "),a("h4",{attrs:{id:"_2、adaptive-margin-function"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、adaptive-margin-function"}},[_._v("#")]),_._v(" 2、Adaptive Margin Function")]),_._v(" "),a("p",[_._v("作者设计了一个"),a("code",[_._v("Margin")]),_._v("函数：")]),_._v(" "),a("blockquote",[a("ol",[a("li",[a("p",[a("strong",[_._v("如果图像质量高，强调困难样本")])])]),_._v(" "),a("li",[a("p",[a("strong",[_._v("如果图像质量低，不强调困难样本")])])])])]),_._v(" "),a("p",[_._v("用2个自适应和来实现这一点，分别指"),a("code",[_._v("angular margin")]),_._v("和"),a("code",[_._v("additive margins")]),_._v("。具体来说:")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/0aa2b1baa5b02dc00d5da0634d070910.webp",alt:""}})]),_._v(" "),a("p",[_._v("其中和是的函数:")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/6b689a485a111134d9ca4572fa557ca9.webp",alt:""}})]),_._v(" "),a("p",[_._v("请注意，当=−1时，建议的函数变成了"),a("code",[_._v("ArcFace")]),_._v("。当=0时，它就变成了"),a("code",[_._v("CosFace")]),_._v("。当=1时，它变成了"),a("code",[_._v("negative angular margin")]),_._v("。")]),_._v(" "),a("blockquote",[a("p",[a("strong",[_._v("图3显示了自适应函数对梯度的影响。高范数特征在远离决策边界的情况下得到较高的梯度尺度，而低范数特征在决策边界附近得到较高的梯度尺度。对于低范数特征，远离边界的较难样本被弱化。")])])]),_._v(" "),a("h2",{attrs:{id:"_4实验"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4实验"}},[_._v("#")]),_._v(" 4实验")]),_._v(" "),a("h3",{attrs:{id:"_4-1-消融实验"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-消融实验"}},[_._v("#")]),_._v(" 4.1 消融实验")]),_._v(" "),a("h4",{attrs:{id:"_1、图像质量指标h的影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、图像质量指标h的影响"}},[_._v("#")]),_._v(" 1、图像质量指标h的影响")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/f77c430015412e99b81d1e9100bbe04c.webp",alt:""}})]),_._v(" "),a("p",[_._v("如表1所示。当h=0.33时，模型表现最佳。当h=0.22或h=0.66时，成绩仍然高于curriculum face。只要把h设置成类似的情况，就仅仅只是一些变化，h不是很敏感。这里设h=0.33。")]),_._v(" "),a("h4",{attrs:{id:"_2、超参数m的影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、超参数m的影响"}},[_._v("#")]),_._v(" 2、超参数m的影响")]),_._v(" "),a("p",[a("code",[_._v("Margin")]),_._v(" m既对应于"),a("code",[_._v("angular margin")]),_._v("的最大范围，也对应于"),a("code",[_._v("additive margins")]),_._v("的大小。从表1可以看出：")]),_._v(" "),a("ul",[a("li",[a("p",[_._v("对于HQ数据集，m=0.4时性能最好，")])]),_._v(" "),a("li",[a("p",[_._v("对于LQ数据集，m=0.75时性能最好。")])])]),_._v(" "),a("p",[_._v("m越大，基于图像质量的"),a("code",[_._v("angular margin")]),_._v("变化也越大，自适应能力越强。在后续的实验中，选择m=0.4，因为它在LQ数据集上有很好的性能，而在HQ数据集上又不牺牲性能。")]),_._v(" "),a("h4",{attrs:{id:"_3、代理选择的影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、代理选择的影响"}},[_._v("#")]),_._v(" 3、代理选择的影响")]),_._v(" "),a("p",[_._v("在表1中，为了显示使用"),a("code",[_._v("特征范数")]),_._v("作为"),a("code",[_._v("图像质量代理")]),_._v("的有效性，将"),a("code",[_._v("特征范数")]),_._v("与其他数量进行了切换，例如(1-BRISQUE)或。使用特征规范的性能优于使用其他范数。对于训练数据集，"),a("code",[_._v("BRISQUE评分")]),_._v("是预先计算的，因此当使用增强训练时，它不能有效地捕捉图像质量。作者引入来说明特征范数的适应性不同于难度的适应性。")]),_._v(" "),a("h4",{attrs:{id:"_4、数据增强的影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、数据增强的影响"}},[_._v("#")]),_._v(" 4、数据增强的影响")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/5d687c6308ba7b727ed72da14f54871c.webp",alt:""}})]),_._v(" "),a("p",[_._v("表2显示，数据增强确实为"),a("code",[_._v("AdaFace")]),_._v("带来了性能提升。HQ数据集的性能保持不变，而LQ数据集的性能显著提高。需要注意的是，数据增强会影响"),a("code",[_._v("CurricularFace")]),_._v("的性能，这与假设是一致的，即"),a("code",[_._v("数据增强")]),_._v("是"),a("code",[_._v("获得更多数据的积极效果")]),_._v("和"),a("code",[_._v("无法识别的图像的消极效果")]),_._v("之间的"),a("code",[_._v("权衡")]),_._v("。基于"),a("code",[_._v("Margin")]),_._v("的softmax之前的工作不包括动态增强，因为性能可能会更差。"),a("code",[_._v("AdaFace")]),_._v("避免了对不可识别图像的过拟合，可以更好地利用增强效果。")]),_._v(" "),a("h4",{attrs:{id:"分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分析"}},[_._v("#")]),_._v(" 分析")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/2c4cf19c27e0148c61d47e60344e25cc.webp",alt:""}})]),_._v(" "),a("p",[_._v("图6")]),_._v(" "),a("p",[_._v("为了显示特征范数以及训练样本的难度在训练过程中的变化情况，在图6中绘制了样本轨迹。从训练数据中随机抽取共计1536个样本。热力图中的每一列代表一个样本，x轴是根据上一个Epoch的范数排序的。")]),_._v(" "),a("p",[_._v("样本#600大约是低范数样本向高范数样本过渡的中间点。底部的图显示，许多低范数样本的概率轨迹直到最后才得到高概率。这与假设是一致的，低规范特征更可能是无法识别的图像。这证明了不太重视这些案例的动机，尽管它们是很难的案例。")]),_._v(" "),a("p",[_._v("低范数特征比高范数特征具有增强的样本百分比更高。对于编号为#0到#600的样本，大约62.0%的样本至少有一种类型的增强。对于#600或更高的样本，该百分比约为38.5%。")]),_._v(" "),a("h3",{attrs:{id:"_4-2-sota方法对比"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-sota方法对比"}},[_._v("#")]),_._v(" 4.2 SOTA方法对比")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/5b041375a6943fbcc68bff67c281908f.webp",alt:""}})]),_._v(" "),a("p",[_._v("表3a")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/01cfd5dff90d2219298889a651402f5c.webp",alt:""}})]),_._v(" "),a("p",[_._v("表3b")]),_._v(" "),a("h3",{attrs:{id:"_4-3-局限性与影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-局限性与影响"}},[_._v("#")]),_._v(" 4.3 局限性与影响")]),_._v(" "),a("h4",{attrs:{id:"_1、局限性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、局限性"}},[_._v("#")]),_._v(" 1、局限性")]),_._v(" "),a("p",[_._v("这项工作解决了训练数据中存在的无法识别的图像。然而，噪声标签也是大规模人脸训练数据集的突出特征之一。"),a("code",[_._v("AdaFace")]),_._v("损失函数对贴错标签的样品没有特殊处理。由于自适应损失赋予高质量的困难样本很大的重要性，高质量的错误标记图像可能会被错误地强调。未来可以同时适应"),a("code",[_._v("不可识别性")]),_._v("和"),a("code",[_._v("标签噪声")]),_._v("。")]),_._v(" "),a("h4",{attrs:{id:"_2、潜在的社会影响"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、潜在的社会影响"}},[_._v("#")]),_._v(" 2、潜在的社会影响")]),_._v(" "),a("p",[_._v("作者认为，计算机视觉社区作为一个整体，应该努力尽量减少负面的社会影响。论文的实验使用了训练数据集MS1MV*，这是MS-Celeb的副产品，一个由其创建者撤回的数据集。")]),_._v(" "),a("p",[_._v("使用MS1MV*是必要的，以比较本文的结果与SoTA方法的公平对比。然而，作者认为社区应该转向新的数据集，所以作者还在最新发布的WebFace4M进行了，以促进未来的研究。")]),_._v(" "),a("p",[_._v("在科学界，收集人类数据需要获得伦理委员会的批准，以确保知情同意。虽然IRB状态通常不是由数据集创建者提供的，但由于收集过程的性质，假设大多数FR数据集(除了IJB-S)没有IRB。FR社区的一个方向是在知情同意的情况下收集大型数据集，促进没有社会关注的研发。")]),_._v(" "),a("h3",{attrs:{id:"检索展示"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#检索展示"}},[_._v("#")]),_._v(" 检索展示")]),_._v(" "),a("p",[a("img",{attrs:{src:"https://imagerk.oss-cn-beijing.aliyuncs.com/img/809b0578560770db7143b6602f456757.webp",alt:""}})]),_._v(" "),a("p",[_._v("可以看到使用"),a("code",[_._v("AdaFace")]),_._v("得到的gallery结果的置信度都比"),a("code",[_._v("ArcFace")]),_._v("要高。")])],1)}),[],!1,null,null,null);v.default=c.exports}}]);