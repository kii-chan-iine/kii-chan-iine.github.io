(window.webpackJsonp=window.webpackJsonp||[]).push([[17],{629:function(t,s,a){"use strict";a.r(s);var n=a(3),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("Boxx",{attrs:{changeTime:"10000"}}),t._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"title"},[t._v("前言")]),a("p",[t._v("记录pytorch学习的心得。")])]),t._v(" "),a("h1",{attrs:{id:"dataloader"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dataloader"}},[t._v("#")]),t._v(" Dataloader")]),t._v(" "),a("h1",{attrs:{id:"backbone"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#backbone"}},[t._v("#")]),t._v(" Backbone")]),t._v(" "),a("h1",{attrs:{id:"loss"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#loss"}},[t._v("#")]),t._v(" Loss")]),t._v(" "),a("h3",{attrs:{id:"优化器和损失函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#优化器和损失函数"}},[t._v("#")]),t._v(" 优化器和损失函数")]),t._v(" "),a("h4",{attrs:{id:"loss-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#loss-2"}},[t._v("#")]),t._v(" Loss")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("fc"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_features"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncriterion"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BCEWithLogitsLoss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loss ")]),t._v("\noptimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# optimizer ")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" step "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_steps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# optimizer ")]),t._v("\n        loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loss  ")]),t._v("\n        optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# optimizer ")]),t._v("\n\n    pred"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("fc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("criterion"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("train_end"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("train_end"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br")])]),a("p",[t._v("下面：当我们在使用torch.tensor()对 自变量x进行构建的时候，曾经对参数"),a("code",[t._v("requires_grad=True")]),t._v("进行设置为True，意思就相当于说，这个Tensor类保留了一个盛放自己导数值的单位tensor。即，这个x有两个单位tensor：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("一个是主体的单位tensor")]),t._v("：用于存放的是实实在在的这个自变量x的数值")]),t._v(" "),a("li",[a("strong",[t._v("另外一个是辅助的单位tensor")]),t._v("：用于存放的是这个自变量x被求导之后的导函数值--x.gard\n当我们通过一些正常手段（例如使用Tensor类的函数）进行操作的时候都是操作在主体那个单位tensor上，剩下那个辅助的单位tensor，我们可以通过"),a("code",[t._v("x.grad")]),t._v("来访问那个辅助的单位tensor。")])]),t._v(" "),a("p",[t._v("如果自变量x不经过导数计算，"),a("code",[t._v("x.grad")]),t._v("是不会有数据的，你访问得到是None。"),a("br"),t._v("\n所以"),a("code",[t._v("f.backward()")]),t._v("所做的就是把导数值存放在"),a("code",[t._v("x.grad")]),t._v("里面。")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("func")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n\nx"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("requires_grad"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建Tensor类")]),t._v("\nf"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("func"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#把 Tensor 类作为参数传入,类比loss函数")]),t._v("\nf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#求导数")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br")])]),a("p",[t._v("需要注意的是"),a("code",[t._v("x.grad")]),t._v("不会自动清零的，他只会不断把新得到的数值累加到旧的数值上面，这就需要我们利用"),a("code",[t._v("optimizer.zero_grad()")]),t._v("来给他清零。")]),t._v(" "),a("h4",{attrs:{id:"优化器"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#优化器"}},[t._v("#")]),t._v(" 优化器")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("func")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#x的5次方")]),t._v("\n\nx"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("requires_grad"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noptimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nf"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("func"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\noptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 梯度清零")]),t._v("\nf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("retain_graph"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第一次反向传播，链式求导，retain_graph 为了不让计算图释放")]),t._v("\noptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 迭代更新")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\noptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 梯度清零==》   x.grad=None")]),t._v("\nf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第二次反向传播")]),t._v("\noptimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#迭代更新")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grad"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("\ntensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("80")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("79.8401")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br")])]),a("h4",{attrs:{id:"_1-zero-grad-到底干了什么"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-zero-grad-到底干了什么"}},[t._v("#")]),t._v(" （1）zero_grad() 到底干了什么？")]),t._v(" "),a("p",[t._v("我们前文提到过Tensor类的grad数值只会在原有基础上增加，自己是不会覆盖的，所以我们每计算一次就应该清零。实际上，"),a("code",[t._v("zero_grad()")]),t._v("做的就是一个清理工作。"),a("br"),t._v("\n如果我们把"),a("code",[t._v("optim.zero_grad()")]),t._v("换成"),a("code",[t._v("x.grad=None")]),t._v("，会发现得到一样的输出结果。")]),t._v(" "),a("h4",{attrs:{id:"_2-step-函数到底干了什么"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-step-函数到底干了什么"}},[t._v("#")]),t._v(" （2）step()函数到底干了什么？")]),t._v(" "),a("p",[t._v("首先，我们必须明白优化器是干什么的？现在我们有一个函数，我们现在需要到达一个谷点，也就是我们需要知道当函数在谷点的时候，自变量x到底等于多少，但是我们只能一个个点去尝试，所以我们必须要让梯度下降，也就是不断迭代x，每一次更换一次x的值，来促使梯度的值不断变小。")]),t._v(" "),a("blockquote",[a("p",[t._v("这个决定了迭代的方向")])]),t._v(" "),a("p",[t._v("我们在运算step()的时是参考了"),a("code",[t._v("x.grad")]),t._v("，也就是"),a("code",[t._v("f.backward()")]),t._v("的结果，这也就是为啥backward会先执行，再执行step。")]),t._v(" "),a("p",[t._v("我们在最开始初始化的时候就已经把自变量x托管给optim类了，无论是我们自己编写的函数，还是已经封装好的损失函数。也就是说我们的optim类只需要损失函数的自变量即可。"),a("br"),t._v("\n我们自己的函数使用优化器：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("func")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#x的5次方")]),t._v("\n\nx"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("requires_grad"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \ncriterion"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("func"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#我们自己写的loss???我的理解，是否正确？")]),t._v("\noptimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[t._v("封装的损失函数使用优化器：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("fc"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_features"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncriterion"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BCEWithLogitsLoss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loss 类")]),t._v("\noptimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# optimizer 类")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("p",[t._v("我们已经知道"),a("code",[t._v("weights,bias=fc.parameters()")]),t._v("，对于损失函数而言，weights和bias就是他的自变量，所以这里用weights和bias来初始化优化器就能说得通了。")]),t._v(" "),a("p",[t._v("所有的优化都是围绕损失函数来转的，我们想要损失降到最小，我们想要损失函数最小的时候的那个自变量的值，就是我们需要的权值。整个训练的过程就是在求权值的过程。")]),t._v(" "),a("img",{attrs:{title:"",src:"https://img-blog.csdnimg.cn/20200205210102349.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9qaW50dXpoZW5nLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70",alt:"在这里插入图片描述",width:"528","data-align":"center"}}),t._v(" "),a("h2",{attrs:{id:"optimizer-step-和scheduler-step-的区别"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#optimizer-step-和scheduler-step-的区别"}},[t._v("#")]),t._v(" optimizer.step()和"),a("a",{attrs:{href:"https://so.csdn.net/so/search?q=scheduler&spm=1001.2101.3001.7020",target:"_blank",rel:"noopener noreferrer"}},[t._v("scheduler"),a("OutboundLink")],1),t._v(".step()的区别")]),t._v(" "),a("p",[t._v("optimizer.step()通常用在每个mini-batch之中，而scheduler.step()通常用在epoch里面,但是不绝对，可以根据具体的需求来做。只有用了optimizer.step()，模型才会更新，而scheduler.step()是对lr进行调整。通常我们有")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("optimizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" optim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SGD"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" momentum "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nscheduler "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lr_scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("StepLR"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" step_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" gamma "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" net"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss_function"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" scheduler"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_epochs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("p",[t._v("在scheduler的step_size表示scheduler.step()每调用step_size次，对应的学习率就会按照策略调整一次。所以如果scheduler.step()是放在mini-batch里面，那么step_size指的是经过这么多次迭代，学习率改变一次。")])],1)}),[],!1,null,null,null);s.default=e.exports}}]);